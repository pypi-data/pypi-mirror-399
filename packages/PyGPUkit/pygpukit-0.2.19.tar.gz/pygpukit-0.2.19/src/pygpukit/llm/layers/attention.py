"""Attention layer implementation for PyGPUkit LLM.

Provides:
- Attention: Multi-head attention with RoPE, GQA, QK-Norm, KV cache
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np

from pygpukit.core.array import GPUArray
from pygpukit.core.dtypes import bfloat16 as dt_bfloat16
from pygpukit.core.dtypes import float16 as dt_float16
from pygpukit.core.factory import from_numpy
from pygpukit.ops.basic import (
    bias_add_inplace,
    concat_axis0,
    copy_to,
    kv_cache_prefill_gqa,
    kv_cache_update_gqa,
    repeat_interleave_axis1,
    reshape_copy,
    rmsnorm,
    rope_inplace,
    sdpa_causal,
    sdpa_causal_fixed_cache,
    slice_rows_range_ptr,
    split_qkv_batch,
    transpose_3d_021,
)

from .linear import LinearBF16, LinearFP8
from .norm import Norm
from .rope import precompute_freqs_cis

if TYPE_CHECKING:
    from pygpukit.llm.buffers import DecodeBuffers
    from pygpukit.llm.config import TransformerConfig


class Attention:
    """Unified attention with Hybrid CPU/GPU execution.

    Supports:
    - Multi-Head Attention (MHA): num_kv_heads == num_heads
    - Grouped Query Attention (GQA): num_kv_heads < num_heads
    - RoPE: enabled via config.use_rope
    - QK Norm: optional normalization of Q and K (Qwen3 style)
    - Hybrid execution: CPU for seq_len=1, GPU for longer sequences
    - FP8 quantized weights via LinearFP8
    """

    def __init__(
        self,
        q_proj: GPUArray | LinearBF16 | LinearFP8,
        k_proj: GPUArray | LinearBF16 | LinearFP8,
        v_proj: GPUArray | LinearBF16 | LinearFP8,
        o_proj: GPUArray | LinearBF16 | LinearFP8,
        config: TransformerConfig,
        q_bias: GPUArray | None = None,
        k_bias: GPUArray | None = None,
        v_bias: GPUArray | None = None,
        o_bias: GPUArray | None = None,
        q_norm: Norm | None = None,
        k_norm: Norm | None = None,
    ):
        # Accept either GPUArray (wrapped in LinearBF16) or pre-built LinearBF16/LinearFP8
        def wrap_linear(
            proj: GPUArray | LinearBF16 | LinearFP8, bias: GPUArray | None
        ) -> LinearBF16 | LinearFP8:
            if isinstance(proj, (LinearBF16, LinearFP8)):
                return proj
            return LinearBF16(proj, bias)

        self.q_proj = wrap_linear(q_proj, q_bias)
        self.k_proj = wrap_linear(k_proj, k_bias)
        self.v_proj = wrap_linear(v_proj, v_bias)
        self.o_proj = wrap_linear(o_proj, o_bias)

        # QK Norm (Qwen3 style)
        self.q_norm = q_norm
        self.k_norm = k_norm

        self.config = config
        self.head_dim = config.head_dim
        self.num_heads = config.num_heads
        assert config.num_kv_heads is not None  # Set in __post_init__
        self.num_kv_heads: int = config.num_kv_heads
        self.num_kv_groups = config.num_kv_groups

        # Store dimensions for QKV split
        self.q_dim = self.num_heads * self.head_dim
        self.k_dim = self.num_kv_heads * self.head_dim
        self.v_dim = self.num_kv_heads * self.head_dim

        # Create fused QKV projection (reduces 3 matmuls to 1)
        # Skip fusion for FP8 (LinearFP8 can't be concatenated)
        self.qkv_proj: LinearBF16 | None = None
        if not isinstance(self.q_proj, LinearFP8):
            # Extract weights from LinearBF16 for concatenation
            q_weight = self.q_proj.weight if isinstance(self.q_proj, LinearBF16) else q_proj
            k_weight = self.k_proj.weight if isinstance(self.k_proj, LinearBF16) else k_proj
            v_weight = self.v_proj.weight if isinstance(self.v_proj, LinearBF16) else v_proj
            qkv_weight = concat_axis0(concat_axis0(q_weight, k_weight), v_weight)
            self.qkv_proj = LinearBF16(qkv_weight, None)

        # Precompute RoPE if enabled
        self._cos: np.ndarray | None
        self._sin: np.ndarray | None
        if config.use_rope:
            self._cos, self._sin = precompute_freqs_cis(
                self.head_dim, config.max_position_embeddings, config.rope_theta
            )
        else:
            self._cos, self._sin = None, None

        # Fixed-length KV cache for CUDA Graph (initialized on first use)
        self._k_cache: GPUArray | None = None
        self._v_cache: GPUArray | None = None
        self._max_cache_len: int = 0

        # Lookahead KV tracking for Jacobi decoding
        self._confirmed_pos: int = 0
        self._logical_pos: int = 0

    def init_fixed_cache(self, max_seq_len: int, dtype: str = "float16") -> None:
        """Initialize fixed-length KV cache for CUDA Graph capture.

        Args:
            max_seq_len: Maximum sequence length to support.
            dtype: Data type for cache (float16/bfloat16/float32).
        """
        cache_shape = (self.num_heads, max_seq_len, self.head_dim)
        if dtype == "float16":
            np_dtype = np.float16
        elif dtype == "bfloat16":
            np_dtype = np.uint16  # bf16 stored as uint16
        else:
            np_dtype = np.float32
        self._k_cache = from_numpy(np.zeros(cache_shape, dtype=np_dtype))
        self._v_cache = from_numpy(np.zeros(cache_shape, dtype=np_dtype))
        self._max_cache_len = max_seq_len
        self._confirmed_pos = 0
        self._logical_pos = 0

    # =========================================================================
    # Lookahead KV Cache Management (for Jacobi Decoding)
    # =========================================================================

    def set_confirmed_pos(self, pos: int) -> None:
        """Set the confirmed position (e.g., after prefill)."""
        assert 0 <= pos <= self._max_cache_len, f"Invalid pos {pos}"
        self._confirmed_pos = pos
        self._logical_pos = pos

    def reset_lookahead(self) -> None:
        """Reset lookahead pointer to confirmed position."""
        self._logical_pos = self._confirmed_pos

    def commit_lookahead(self, n_accepted: int) -> None:
        """Commit accepted tokens by advancing confirmed_pos."""
        new_pos = self._confirmed_pos + n_accepted
        assert new_pos <= self._max_cache_len, f"Commit exceeds cache: {new_pos}"
        self._confirmed_pos = new_pos
        self._logical_pos = new_pos

    def get_confirmed_pos(self) -> int:
        """Get current confirmed position."""
        return self._confirmed_pos

    def __call__(
        self,
        x: GPUArray,
        position_ids: list[int] | None = None,
        past_kv: tuple | None = None,
        use_cache: bool = False,
    ) -> tuple[GPUArray, tuple | None]:
        """Forward pass with hybrid CPU/GPU attention.

        Args:
            x: Input tensor [seq_len, hidden_size]
            position_ids: Position IDs for RoPE (auto-generated if None)
            past_kv: Tuple of (past_k, past_v) numpy arrays
            use_cache: Whether to return KV cache

        Returns:
            Tuple of (output, present_kv)
        """
        seq_len = x.shape[0]

        if position_ids is None:
            position_ids = list(range(seq_len))

        return self._forward_gpu(x, position_ids, past_kv, use_cache)

    def _forward_gpu(
        self,
        x: GPUArray,
        position_ids: list[int],
        past_kv: tuple | None,
        use_cache: bool,
    ) -> tuple[GPUArray, tuple | None]:
        """GPU path for long sequences (prefill)."""
        seq_len = x.shape[0]

        # Project Q, K, V
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # Reshape for multi-head
        q = reshape_copy(q, (seq_len, self.num_heads, self.head_dim))
        k = reshape_copy(k, (seq_len, self.num_kv_heads, self.head_dim))
        v = reshape_copy(v, (seq_len, self.num_kv_heads, self.head_dim))

        # QK Norm (Qwen3 style)
        if self.q_norm is not None:
            q_shape = (seq_len, self.num_heads, self.head_dim)
            q_2d = reshape_copy(q, (seq_len * self.num_heads, self.head_dim))
            q_2d = self.q_norm(q_2d)
            q = reshape_copy(q_2d, q_shape)
        if self.k_norm is not None:
            k_shape = (seq_len, self.num_kv_heads, self.head_dim)
            k_2d = reshape_copy(k, (seq_len * self.num_kv_heads, self.head_dim))
            k_2d = self.k_norm(k_2d)
            k = reshape_copy(k_2d, k_shape)

        # Apply RoPE on GPU
        if self.config.use_rope:
            assert self._cos is not None and self._sin is not None
            from pygpukit.ops.basic import rope_inplace_f32table

            q_dtype = q.dtype
            cos_f32 = from_numpy(self._cos[position_ids].astype(np.float32))
            sin_f32 = from_numpy(self._sin[position_ids].astype(np.float32))
            if q_dtype in (dt_float16, dt_bfloat16):
                # Use f32 tables directly for higher precision (no intermediate alloc)
                rope_inplace_f32table(q, k, cos_f32, sin_f32)
            else:
                rope_inplace(q, k, cos_f32, sin_f32)

        # GPU KV Cache
        if past_kv is not None:
            past_k, past_v = past_kv
            if isinstance(past_k, GPUArray):
                k = concat_axis0(past_k, k)
                v = concat_axis0(past_v, v)
            else:
                k_np = k.to_numpy()
                v_np = v.to_numpy()
                k_np = np.concatenate([past_k, k_np], axis=0)
                v_np = np.concatenate([past_v, v_np], axis=0)
                k = from_numpy(k_np)
                v = from_numpy(v_np)

        present_kv = (k, v) if use_cache else None

        # Expand for GQA on GPU
        if self.num_kv_groups > 1:
            k_expanded = repeat_interleave_axis1(k, self.num_kv_groups)
            v_expanded = repeat_interleave_axis1(v, self.num_kv_groups)
        else:
            k_expanded = k
            v_expanded = v

        # GPU SDPA
        q_t = transpose_3d_021(q)
        k_t = transpose_3d_021(k_expanded)
        v_t = transpose_3d_021(v_expanded)

        attn_output = sdpa_causal(q_t, k_t, v_t)
        attn_output = transpose_3d_021(attn_output)
        attn_output = reshape_copy(attn_output, (seq_len, self.num_heads * self.head_dim))

        return self.o_proj(attn_output), present_kv

    def forward_fixed_cache(
        self,
        x: GPUArray,
        position: int,
        context_len: int,
        *,
        out: GPUArray | None = None,
    ) -> GPUArray:
        """Forward pass using fixed-length KV cache (for CUDA Graph decode).

        Args:
            x: Input tensor [1, hidden_size] - single token
            position: Current position in sequence (for RoPE and cache update)
            context_len: Total context length (prefill + decoded so far)
            out: Optional pre-allocated output buffer

        Returns:
            Output tensor [1, hidden_size]
        """
        assert self._k_cache is not None, "Call init_fixed_cache first"
        assert x.shape[0] == 1, "forward_fixed_cache expects single token"

        if self.qkv_proj is not None:
            # Fused QKV projection (faster for non-FP8)
            qkv = self.qkv_proj(x)
            q_2d = qkv.narrow(0, self.q_dim)
            k_2d = qkv.narrow(self.q_dim, self.k_dim)
            v_2d = qkv.narrow(self.q_dim + self.k_dim, self.v_dim)

            # Apply biases separately
            if self.q_proj.bias is not None:
                bias_add_inplace(q_2d, self.q_proj.bias)
            if self.k_proj.bias is not None:
                bias_add_inplace(k_2d, self.k_proj.bias)
            if self.v_proj.bias is not None:
                bias_add_inplace(v_2d, self.v_proj.bias)
        else:
            # Separate projections (for FP8)
            q_2d = self.q_proj(x)
            k_2d = self.k_proj(x)
            v_2d = self.v_proj(x)

        # Zero-copy reshape
        q = q_2d.view((1, self.num_heads, self.head_dim))
        k = k_2d.view((1, self.num_kv_heads, self.head_dim))
        v = v_2d.view((1, self.num_kv_heads, self.head_dim))

        # QK Norm
        if self.q_norm is not None:
            q_flat = q.view((self.num_heads, self.head_dim))
            q_normed = self.q_norm(q_flat)
            q = q_normed.view((1, self.num_heads, self.head_dim))
        if self.k_norm is not None:
            k_flat = k.view((self.num_kv_heads, self.head_dim))
            k_normed = self.k_norm(k_flat)
            k = k_normed.view((1, self.num_kv_heads, self.head_dim))

        q_dtype = q.dtype

        # Apply RoPE
        if self.config.use_rope and self._cos is not None and self._sin is not None:
            from pygpukit.ops.basic import rope_inplace_f32table

            cos_f32 = from_numpy(self._cos[position : position + 1].astype(np.float32))
            sin_f32 = from_numpy(self._sin[position : position + 1].astype(np.float32))
            if q_dtype in (dt_float16, dt_bfloat16):
                rope_inplace_f32table(q, k, cos_f32, sin_f32)
            else:
                rope_inplace(q, k, cos_f32, sin_f32)

        # Update KV cache
        kv_cache_update_gqa(k, self._k_cache, self.num_heads, position)
        kv_cache_update_gqa(v, self._v_cache, self.num_heads, position)

        q_t = q.view((self.num_heads, 1, self.head_dim))

        # Allocate output buffer if needed
        if out is None:
            if q_dtype == dt_float16:
                out_np_dtype = np.float16
            elif q_dtype == dt_bfloat16:
                out_np_dtype = np.uint16
            else:
                out_np_dtype = np.float32
            attn_out = from_numpy(np.zeros((self.num_heads, 1, self.head_dim), dtype=out_np_dtype))
        else:
            attn_out = out

        sdpa_causal_fixed_cache(q_t, self._k_cache, self._v_cache, attn_out, context_len)

        attn_output = attn_out.view((1, self.num_heads * self.head_dim))
        return self.o_proj(attn_output)

    def forward_fixed_cache_batch(
        self,
        x: GPUArray,
        start_position: int,
        context_len: int,
    ) -> GPUArray:
        """Forward pass for batch decode using fixed-length KV cache.

        Processes multiple tokens at once for speculative decoding verification.
        """
        assert self._k_cache is not None, "Call init_fixed_cache first"
        seq_len = x.shape[0]

        if seq_len == 1:
            return self.forward_fixed_cache(x, start_position, context_len)

        if self.qkv_proj is not None:
            # Fused QKV projection (faster for non-FP8)
            qkv = self.qkv_proj(x)
            qkv_np = qkv.to_numpy()
            q_np = qkv_np[:, : self.q_dim]
            k_np = qkv_np[:, self.q_dim : self.q_dim + self.k_dim]
            v_np = qkv_np[:, self.q_dim + self.k_dim :]

            # Apply biases
            if self.q_proj.bias is not None:
                q_np = q_np + self.q_proj.bias.to_numpy()
            if self.k_proj.bias is not None:
                k_np = k_np + self.k_proj.bias.to_numpy()
            if self.v_proj.bias is not None:
                v_np = v_np + self.v_proj.bias.to_numpy()

            q_2d = from_numpy(q_np.astype(qkv_np.dtype))
            k_2d = from_numpy(k_np.astype(qkv_np.dtype))
            v_2d = from_numpy(v_np.astype(qkv_np.dtype))
        else:
            # Separate projections (for FP8)
            q_2d = self.q_proj(x)
            k_2d = self.k_proj(x)
            v_2d = self.v_proj(x)

        q = reshape_copy(q_2d, (seq_len, self.num_heads, self.head_dim))
        k = reshape_copy(k_2d, (seq_len, self.num_kv_heads, self.head_dim))
        v = reshape_copy(v_2d, (seq_len, self.num_kv_heads, self.head_dim))

        # QK Norm
        if self.q_norm is not None:
            q_flat = reshape_copy(q, (seq_len * self.num_heads, self.head_dim))
            q_normed = self.q_norm(q_flat)
            q = reshape_copy(q_normed, (seq_len, self.num_heads, self.head_dim))
        if self.k_norm is not None:
            k_flat = reshape_copy(k, (seq_len * self.num_kv_heads, self.head_dim))
            k_normed = self.k_norm(k_flat)
            k = reshape_copy(k_normed, (seq_len, self.num_kv_heads, self.head_dim))

        q_dtype = q.dtype

        # RoPE
        if self.config.use_rope and self._cos is not None and self._sin is not None:
            from pygpukit.ops.basic import rope_inplace_f32table

            end_pos = start_position + seq_len
            cos_f32 = from_numpy(self._cos[start_position:end_pos].astype(np.float32))
            sin_f32 = from_numpy(self._sin[start_position:end_pos].astype(np.float32))
            if q_dtype in (dt_float16, dt_bfloat16):
                rope_inplace_f32table(q, k, cos_f32, sin_f32)
            else:
                rope_inplace(q, k, cos_f32, sin_f32)

        # Update KV cache
        kv_cache_prefill_gqa(k, self._k_cache, self.num_heads, start_position)
        kv_cache_prefill_gqa(v, self._v_cache, self.num_heads, start_position)

        q_t = transpose_3d_021(q)
        # Allocate attn_out with matching dtype
        if q_dtype == dt_float16:
            out_np_dtype = np.float16
        elif q_dtype == dt_bfloat16:
            out_np_dtype = np.uint16  # bfloat16 stored as uint16
        else:
            out_np_dtype = np.float32
        attn_out = from_numpy(
            np.zeros((self.num_heads, seq_len, self.head_dim), dtype=out_np_dtype)
        )

        sdpa_causal_fixed_cache(q_t, self._k_cache, self._v_cache, attn_out, context_len)

        attn_output = transpose_3d_021(attn_out)
        attn_output = reshape_copy(attn_output, (seq_len, self.num_heads * self.head_dim))
        return self.o_proj(attn_output)

    def forward_fixed_cache_batch_zero_alloc(
        self,
        x: GPUArray,
        start_position: int,
        context_len: int,
        buffers: DecodeBuffers,
        rope_cos_gpu: GPUArray | None,
        rope_sin_gpu: GPUArray | None,
        start_pos_buf: GPUArray,
    ) -> GPUArray:
        """Zero-allocation forward pass for batch decode using fixed-length KV cache.

        This version uses pre-allocated buffers for all operations, making it
        compatible with CUDA Graph capture. No memory allocations occur.
        """
        assert self._k_cache is not None, "Call init_fixed_cache first"
        seq_len = x.shape[0]

        q_out = buffers.q_batch.view((seq_len, self.num_heads, self.head_dim))
        k_out = buffers.k_batch.view((seq_len, self.num_kv_heads, self.head_dim))
        v_out = buffers.v_batch.view((seq_len, self.num_kv_heads, self.head_dim))

        if self.qkv_proj is not None:
            # Fused QKV projection into pre-allocated buffer
            qkv_out = buffers.qkv_proj_out_batch.slice_rows(seq_len)
            self.qkv_proj(x, out=qkv_out)

            # Split QKV
            split_qkv_batch(qkv_out, q_out, k_out, v_out, self.q_dim, self.k_dim, self.v_dim)

            # Apply biases
            if self.q_proj.bias is not None:
                q_out_2d = q_out.view((seq_len, self.q_dim))
                bias_add_inplace(q_out_2d, self.q_proj.bias)
            if self.k_proj.bias is not None:
                k_out_2d = k_out.view((seq_len, self.k_dim))
                bias_add_inplace(k_out_2d, self.k_proj.bias)
            if self.v_proj.bias is not None:
                v_out_2d = v_out.view((seq_len, self.v_dim))
                bias_add_inplace(v_out_2d, self.v_proj.bias)
        else:
            # Separate projections (for FP8 - allocates, not zero-alloc)
            q_2d = self.q_proj(x)
            k_2d = self.k_proj(x)
            v_2d = self.v_proj(x)
            copy_to(reshape_copy(q_2d, (seq_len, self.num_heads, self.head_dim)), q_out)
            copy_to(reshape_copy(k_2d, (seq_len, self.num_kv_heads, self.head_dim)), k_out)
            copy_to(reshape_copy(v_2d, (seq_len, self.num_kv_heads, self.head_dim)), v_out)

        # QK Norm
        if self.q_norm is not None and buffers.q_flat_batch is not None:
            q_flat = buffers.q_flat_batch.slice_rows(seq_len * self.num_heads)
            copy_to(q_out.view((seq_len * self.num_heads, self.head_dim)), q_flat)
            rmsnorm(q_flat, self.q_norm.weight, self.q_norm.eps, out=q_flat)
            copy_to(q_flat.view((seq_len, self.num_heads, self.head_dim)), q_out)

        if self.k_norm is not None and buffers.k_flat_batch is not None:
            k_flat = buffers.k_flat_batch.slice_rows(seq_len * self.num_kv_heads)
            copy_to(k_out.view((seq_len * self.num_kv_heads, self.head_dim)), k_flat)
            rmsnorm(k_flat, self.k_norm.weight, self.k_norm.eps, out=k_flat)
            copy_to(k_flat.view((seq_len, self.num_kv_heads, self.head_dim)), k_out)

        # RoPE
        if self.config.use_rope and rope_cos_gpu is not None and rope_sin_gpu is not None:
            cos_out = buffers.cos_batch.slice_rows(seq_len)
            sin_out = buffers.sin_batch.slice_rows(seq_len)
            slice_rows_range_ptr(rope_cos_gpu, cos_out, start_pos_buf, seq_len)
            slice_rows_range_ptr(rope_sin_gpu, sin_out, start_pos_buf, seq_len)
            rope_inplace(q_out, k_out, cos_out, sin_out)

        # Update KV cache
        kv_cache_prefill_gqa(k_out, self._k_cache, self.num_heads, start_position)
        kv_cache_prefill_gqa(v_out, self._v_cache, self.num_heads, start_position)

        # Transpose Q for SDPA
        q_t_out = buffers.q_t_batch.view((self.num_heads, seq_len, self.head_dim))
        transpose_3d_021(q_out, out=q_t_out)

        # SDPA
        attn_out = buffers.attn_out_batch.view((self.num_heads, seq_len, self.head_dim))
        sdpa_causal_fixed_cache(q_t_out, self._k_cache, self._v_cache, attn_out, context_len)

        # Transpose output
        attn_out_t = buffers.attn_out_t_batch.view((seq_len, self.num_heads, self.head_dim))
        transpose_3d_021(attn_out, out=attn_out_t)

        attn_out_2d = attn_out_t.view((seq_len, self.num_heads * self.head_dim))

        # O projection
        o_out = buffers.o_proj_out_batch.slice_rows(seq_len)
        self.o_proj(attn_out_2d, out=o_out)

        return o_out


__all__ = [
    "Attention",
]
