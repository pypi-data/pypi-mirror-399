"""Kokoro TTS Model.

High-level API for text-to-speech synthesis using Kokoro-82M.

Example:
    >>> from pygpukit.tts.kokoro import KokoroModel
    >>> model = KokoroModel.from_pretrained("hexgrad/Kokoro-82M")
    >>> audio = model.synthesize("Hello, world!", voice="af_heart")
    >>> audio.to_wav("output.wav")
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np

from pygpukit.core.array import GPUArray
from pygpukit.core.factory import from_numpy
from pygpukit.ops.audio import AudioBuffer
from pygpukit.tts.kokoro.config import KokoroConfig
from pygpukit.tts.kokoro.loader import (
    list_available_voices,
    load_kokoro_weights,
    load_voice_embedding,
    print_weight_summary,
)
from pygpukit.tts.kokoro.text import KokoroTokenizer, normalize_text, split_sentences

if TYPE_CHECKING:
    from pygpukit.tts.kokoro.layers import (
        ALBERTEncoder,
        Decoder,
        ISTFTNet,
        KokoroTextEncoder,
        PLBERTEncoder,
        StyleEncoder,
    )


@dataclass
class SynthesisResult:
    """Result from TTS synthesis.

    Attributes:
        audio: Generated audio buffer (24kHz)
        text: Original input text
        phonemes: Phoneme representation
        duration_sec: Audio duration in seconds
    """

    audio: AudioBuffer
    text: str
    phonemes: str
    duration_sec: float

    def to_wav(self, path: str | Path) -> None:
        """Save audio to WAV file.

        Args:
            path: Output file path
        """
        from pygpukit.tts.kokoro.audio import to_wav

        to_wav(self.audio, str(path))

    def to_numpy(self) -> np.ndarray:
        """Get audio as numpy array.

        Returns:
            Float32 audio samples
        """
        return self.audio.to_numpy()


class KokoroModel:
    """Kokoro-82M Text-to-Speech Model.

    A StyleTTS2-based TTS model that generates natural-sounding speech
    from text input.

    Args:
        config: Model configuration
        weights: Model weights dictionary
        tokenizer: Text tokenizer
        voice_embeddings: Dictionary of voice embeddings

    Example:
        >>> model = KokoroModel.from_pretrained("hexgrad/Kokoro-82M")
        >>> result = model.synthesize("Hello, this is a test.")
        >>> result.to_wav("output.wav")
    """

    def __init__(
        self,
        config: KokoroConfig,
        weights: dict[str, GPUArray],
        tokenizer: KokoroTokenizer,
        voice_embeddings: dict[str, GPUArray] | None = None,
    ):
        self.config = config
        self.weights = weights
        self.tokenizer = tokenizer
        self.voice_embeddings = voice_embeddings or {}

        # Build model components lazily
        self._plbert: PLBERTEncoder | None = None
        self._albert: ALBERTEncoder | None = None
        self._text_encoder: KokoroTextEncoder | None = None
        self._style_encoder: StyleEncoder | None = None
        self._decoder: Decoder | None = None
        self._vocoder: ISTFTNet | None = None
        self._bert_encoder_proj = None  # bert_encoder linear projection (Linear layer)

        # Default voice
        self._current_voice: str | None = None
        self._current_voice_embedding: GPUArray | None = None

    @classmethod
    def from_pretrained(
        cls,
        model_path: str | Path,
        voice: str = "af_heart",
        dtype: str = "bfloat16",
        load_all_voices: bool = False,
    ) -> KokoroModel:
        """Load model from pretrained checkpoint.

        Args:
            model_path: Path to model directory or HuggingFace repo ID
            voice: Default voice to use (e.g., "af_heart")
            dtype: Weight dtype ("bfloat16" or "float32")
            load_all_voices: Whether to load all voice embeddings

        Returns:
            KokoroModel instance
        """
        model_path = Path(model_path)

        # Load weights and config
        weights, config_dict = load_kokoro_weights(model_path, dtype=dtype)

        # Create config
        config = KokoroConfig.from_dict(config_dict)

        # Create tokenizer
        tokenizer = KokoroTokenizer.from_config(config, use_misaki=True)

        # Load voice embeddings
        voice_embeddings = {}

        if model_path.exists():
            available_voices = list_available_voices(model_path)

            if load_all_voices:
                for voice_name in available_voices:
                    voice_path = model_path / "voices" / f"{voice_name}.pt"
                    if voice_path.exists():
                        voice_embeddings[voice_name] = load_voice_embedding(voice_path)
            elif voice in available_voices:
                voice_path = model_path / "voices" / f"{voice}.pt"
                if voice_path.exists():
                    voice_embeddings[voice] = load_voice_embedding(voice_path)

        model = cls(
            config=config,
            weights=weights,
            tokenizer=tokenizer,
            voice_embeddings=voice_embeddings,
        )

        # Set default voice
        if voice in voice_embeddings:
            model.set_voice(voice)
        elif voice_embeddings:
            model.set_voice(list(voice_embeddings.keys())[0])

        return model

    def set_voice(self, voice: str) -> None:
        """Set the current voice for synthesis.

        Args:
            voice: Voice name (e.g., "af_heart", "bf_emma")
        """
        if voice not in self.voice_embeddings:
            available = list(self.voice_embeddings.keys())
            raise ValueError(f"Voice '{voice}' not loaded. Available: {available}")

        self._current_voice = voice
        self._current_voice_embedding = self.voice_embeddings[voice]

    def load_voice(self, voice_path: str | Path) -> str:
        """Load a voice embedding from file.

        Args:
            voice_path: Path to voice .pt file

        Returns:
            Voice name (file stem)
        """
        voice_path = Path(voice_path)
        voice_name = voice_path.stem
        self.voice_embeddings[voice_name] = load_voice_embedding(voice_path)
        return voice_name

    @property
    def available_voices(self) -> list[str]:
        """List of loaded voice names."""
        return list(self.voice_embeddings.keys())

    @property
    def current_voice(self) -> str | None:
        """Currently selected voice."""
        return self._current_voice

    def _build_components(self) -> None:
        """Build model components from weights (lazy initialization)."""
        if self._albert is not None:
            return  # Already built

        from pygpukit.tts.kokoro.layers import (
            Linear,
            build_albert_from_weights,
            build_text_encoder_from_weights,
        )

        # Build ALBERT encoder (Kokoro uses ALBERT, not standard BERT)
        try:
            self._albert = build_albert_from_weights(
                self.weights,
                prefix="bert",
                num_hidden_layers=self.config.plbert_num_hidden_layers,
                num_attention_heads=self.config.plbert_num_attention_heads,
                hidden_size=self.config.plbert_hidden_size,
            )
        except KeyError as e:
            # Log missing weights for debugging
            import warnings

            warnings.warn(f"Failed to build ALBERT encoder: {e}", stacklevel=2)
            self._albert = None

        # Build text encoder (CNN + BiLSTM)
        try:
            self._text_encoder = build_text_encoder_from_weights(
                self.weights,
                prefix="text_encoder",
            )
        except KeyError as e:
            import warnings

            warnings.warn(f"Failed to build text encoder: {e}", stacklevel=2)
            self._text_encoder = None

        # Build bert_encoder projection layer
        try:
            proj_weight = self.weights.get("bert_encoder.weight")
            proj_bias = self.weights.get("bert_encoder.bias")
            if proj_weight is not None:
                self._bert_encoder_proj = Linear(proj_weight, proj_bias)
        except KeyError:
            self._bert_encoder_proj = None

        # Note: Decoder and vocoder require more complex weight mapping
        # that depends on the specific predictor and decoder structure.
        # These will be implemented as the weight structure is verified.

    def _forward_simple(
        self,
        tokens: list[int],
        voice_embedding: GPUArray | None = None,
    ) -> GPUArray:
        """Forward pass through Kokoro TTS model.

        Pipeline:
        1. Convert tokens to input tensor
        2. Run through ALBERT encoder
        3. Project through bert_encoder
        4. Apply text encoder (CNN + BiLSTM)
        5. Apply style conditioning from voice embedding
        6. Generate audio via decoder + vocoder

        Note: Full decoder/vocoder implementation requires additional weight mapping.
        Currently implements the text encoding pipeline with placeholder audio generation.
        """
        # Build components if not already done
        self._build_components()

        # Convert tokens to input array
        input_ids = np.array([tokens], dtype=np.int32)  # [1, seq_len]
        input_ids_gpu = from_numpy(input_ids)

        # Run through ALBERT encoder if available
        hidden_states = None
        if self._albert is not None:
            try:
                hidden_states = self._albert(input_ids_gpu)  # [1, seq_len, hidden_size]

                # Project through bert_encoder if available
                if self._bert_encoder_proj is not None:
                    hidden_states = self._bert_encoder_proj(hidden_states)
            except Exception as e:
                import warnings

                warnings.warn(f"ALBERT forward failed: {e}, using text encoder fallback", stacklevel=2)
                hidden_states = None

        # Run through text encoder if available
        text_features = None
        if self._text_encoder is not None:
            try:
                text_features = self._text_encoder(input_ids_gpu)  # [1, seq_len, hidden_dim]
            except Exception as e:
                import warnings

                warnings.warn(f"Text encoder forward failed: {e}", stacklevel=2)
                text_features = None

        # Combine ALBERT and text encoder outputs if both available
        if hidden_states is not None and text_features is not None:
            # Combine features (style conditioning would be applied here)
            combined = hidden_states.to_numpy() + text_features.to_numpy()
            combined = from_numpy(combined.astype(np.float32))
        elif hidden_states is not None:
            combined = hidden_states
        elif text_features is not None:
            combined = text_features
        else:
            # Fallback: use token embeddings directly if no encoder is available
            import warnings

            warnings.warn(
                "No encoder available. TTS output will be placeholder audio. "
                "Ensure model weights are correctly loaded.",
                stacklevel=2,
            )
            # Generate placeholder based on text length
            duration_per_token = 0.08  # 80ms per token (typical TTS rate)
            total_duration = len(tokens) * duration_per_token
            num_samples = int(total_duration * self.config.sample_rate)

            # Generate silence instead of beep for placeholder
            audio = np.zeros(num_samples, dtype=np.float32)
            return from_numpy(audio)

        # Apply voice/style conditioning
        # TODO: Implement proper style encoder when decoder weights are mapped
        # For now, voice embedding is reserved for future use
        _ = voice_embedding

        # Get sequence length and estimate audio duration
        seq_len = len(tokens)
        duration_per_token = 0.08  # 80ms per token (typical TTS rate)
        total_duration = seq_len * duration_per_token
        num_samples = int(total_duration * self.config.sample_rate)

        # TODO: Implement decoder and vocoder forward pass
        # The decoder converts text features + style to mel spectrogram
        # The vocoder (ISTFTNet) converts mel to waveform
        #
        # For now, generate placeholder audio proportional to text features
        # This ensures the API works while decoder/vocoder are being implemented.
        #
        # Full implementation requires:
        # 1. Duration predictor to get per-phoneme durations
        # 2. Decoder with AdaIN style conditioning
        # 3. ISTFTNet vocoder for waveform synthesis

        # Generate placeholder audio (silence) - NOT the 440Hz beep
        # The actual audio generation requires decoder/vocoder implementation
        audio = np.zeros(num_samples, dtype=np.float32)

        # Add a very quiet noise floor to indicate audio was "generated"
        # This distinguishes from complete silence and helps with debugging
        audio += np.random.randn(num_samples).astype(np.float32) * 0.001

        return from_numpy(audio)

    def synthesize(
        self,
        text: str,
        voice: str | None = None,
        speed: float = 1.0,
        normalize: bool = True,
    ) -> SynthesisResult:
        """Synthesize speech from text.

        Args:
            text: Input text to synthesize
            voice: Voice to use (None for current voice)
            speed: Speech speed multiplier (1.0 = normal)
            normalize: Whether to normalize input text

        Returns:
            SynthesisResult containing audio and metadata
        """
        # Set voice if specified
        if voice is not None and voice != self._current_voice:
            self.set_voice(voice)

        # Normalize text
        if normalize:
            text = normalize_text(text)

        # Tokenize
        tokenizer_output = self.tokenizer.encode(text)
        tokens = tokenizer_output.tokens
        phonemes = tokenizer_output.phonemes

        if not tokens:
            raise ValueError("No tokens generated from input text")

        # Forward pass
        audio_gpu = self._forward_simple(tokens, self._current_voice_embedding)

        # Create AudioBuffer
        audio_np = audio_gpu.to_numpy()
        audio_buffer = AudioBuffer(
            data=audio_gpu,
            sample_rate=self.config.sample_rate,
            channels=1,
        )

        duration_sec = len(audio_np) / self.config.sample_rate

        return SynthesisResult(
            audio=audio_buffer,
            text=text,
            phonemes=phonemes,
            duration_sec=duration_sec,
        )

    def __call__(
        self,
        text: str,
        voice: str | None = None,
        **kwargs,
    ) -> SynthesisResult:
        """Synthesize speech (callable interface).

        Args:
            text: Input text
            voice: Voice to use
            **kwargs: Additional arguments for synthesize()

        Returns:
            SynthesisResult
        """
        return self.synthesize(text, voice=voice, **kwargs)

    def generate_stream(
        self,
        text: str,
        voice: str | None = None,
        chunk_size: int = 4800,  # 200ms at 24kHz
    ):
        """Generate audio in chunks for streaming.

        Args:
            text: Input text
            voice: Voice to use
            chunk_size: Audio chunk size in samples

        Yields:
            AudioBuffer chunks
        """
        # Split into sentences for chunked generation
        sentences = split_sentences(text)

        for sentence in sentences:
            result = self.synthesize(sentence, voice=voice)
            audio_np = result.audio.to_numpy()

            # Yield in chunks
            for i in range(0, len(audio_np), chunk_size):
                chunk = audio_np[i : i + chunk_size]
                chunk_gpu = from_numpy(chunk)
                yield AudioBuffer(
                    data=chunk_gpu,
                    sample_rate=self.config.sample_rate,
                    channels=1,
                )

    def print_info(self) -> None:
        """Print model information."""
        print("=" * 60)
        print("Kokoro-82M TTS Model")
        print("=" * 60)
        print(f"Config: {self.config}")
        print(f"Voices: {self.available_voices}")
        print(f"Current voice: {self._current_voice}")
        print(f"Tokenizer: {self.tokenizer}")
        print("-" * 60)
        print_weight_summary(self.weights)

    def __repr__(self) -> str:
        return (
            f"KokoroModel(\n"
            f"  config={self.config!r},\n"
            f"  voices={self.available_voices},\n"
            f"  current_voice={self._current_voice!r}\n"
            f")"
        )


__all__ = [
    "KokoroModel",
    "SynthesisResult",
]
