"""
PyGPUkit v0.2 Full Feature Demo & Benchmark

This demo showcases ALL v0.2 features:
=== Core Infrastructure ===
1. Rust Memory Pool - LRU eviction, size classes
2. Rust Scheduler - Task management, bandwidth pacing
3. Rust Async Transfer Engine - Separate H2D/D2H streams
4. Rust Kernel Dispatch Controller - Per-stream launch management

=== New v0.2 Features ===
5. Admission Control - Deterministic admission pipeline
6. QoS Policy Framework - K8s-style QoS tiers
7. Kernel Pacing Engine - Bandwidth-based throttling
8. Micro-Slicing Framework - Kernel splitting for fairness
9. Pinned Memory Support - Page-locked host memory
10. Kernel Cache - PTX caching for NVRTC
11. GPU Partitioning - Resource isolation

=== Compute ===
12. Tiled Matmul Benchmark
"""

import os
import sys
import time

# Add CUDA DLLs to PATH
cuda_path = os.environ.get("CUDA_PATH", r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4")
cuda_bin = os.path.join(cuda_path, "bin")
if cuda_bin not in os.environ["PATH"]:
    os.environ["PATH"] = cuda_bin + os.pathsep + os.environ["PATH"]
if hasattr(os, "add_dll_directory"):
    os.add_dll_directory(cuda_bin)

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

import numpy as np

# =============================================================================
# Header
# =============================================================================


def print_header(title: str):
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)


def print_section(title: str):
    print(f"\n--- {title} ---")


# =============================================================================
# Main Demo
# =============================================================================


def main():
    print_header("PyGPUkit v0.2 Complete Feature Demo")

    # Import modules
    try:
        import pygpukit

        native = pygpukit._pygpukit_native
        import _pygpukit_rust as rust

        print(f"PyGPUkit version: {pygpukit.__version__}")
        print("Native module loaded: OK")
        print("Rust module loaded: OK")
    except ImportError as e:
        print(f"Import error: {e}")
        return 1

    # Environment info
    print_section("Environment")
    print(f"CUDA available: {native.is_cuda_available()}")

    if not native.is_cuda_available():
        print("GPU not available, exiting")
        return 1

    props = native.get_device_properties(0)
    print(f"GPU: {props.name}")
    print(f"Memory: {props.total_memory / 1024**3:.1f} GB")
    print(f"Compute Capability: {props.compute_capability_major}.{props.compute_capability_minor}")
    print(f"SMs: {props.multiprocessor_count}")

    # =========================================================================
    # 1. Rust Memory Pool Demo
    # =========================================================================
    print_header("1. Rust Memory Pool")

    pool = rust.MemoryPool(
        quota=100 * 1024 * 1024,  # 100 MB quota
        enable_eviction=True,
    )
    print("Created pool with 100 MB quota, eviction enabled")

    # Allocate blocks
    block_ids = []
    for i, size in enumerate([1024, 4096, 16384, 65536]):
        block_id = pool.allocate(size)
        block_ids.append(block_id)
        block = pool.get_block(block_id)
        print(f"  Allocated block {block.id}: {block.size} bytes")

    stats = pool.stats()
    print("\nPool stats:")
    print(f"  Active: {stats.active_blocks} blocks, {stats.used} bytes")
    print(f"  Allocations: {stats.allocation_count}")
    print(f"  Quota usage: {stats.used / stats.quota:.1%}")

    # Free and reuse
    pool.free(block_ids[0])
    print(f"\nFreed block {block_ids[0]}")

    new_block_id = pool.allocate(1024)
    print(f"Allocated new 1024-byte block: {new_block_id} (should reuse free list)")

    stats = pool.stats()
    print(f"Reuse count: {stats.reuse_count}")

    # =========================================================================
    # 2. Rust Scheduler Demo
    # =========================================================================
    print_header("2. Rust Scheduler")

    scheduler = rust.Scheduler(
        sched_tick_ms=10.0,
        window_ms=100.0,
        total_memory=1024 * 1024 * 1024,  # 1 GB
    )
    print("Created scheduler (10ms tick, 100ms window, 1GB memory)")

    # Submit tasks
    task_ids = []
    for i in range(5):
        task = rust.TaskMeta(
            id=f"task_{i}",
            name=f"Layer {i}",
            memory_estimate=100 * 1024 * 1024,  # 100 MB
            priority=i % 3,
        )
        task_id = scheduler.submit(task)
        task_ids.append(task_id)
        print(f"  Submitted: {task_id} (priority: {i % 3})")

    sched_stats = scheduler.stats()
    print(f"\nPending tasks: {sched_stats.pending_count}")

    # Run tasks
    runnable_ids = scheduler.get_runnable_tasks(max_tasks=3)
    print(f"Runnable tasks: {len(runnable_ids)}")

    # Start and complete a task
    if runnable_ids:
        scheduler.start_task(runnable_ids[0])
        scheduler.complete_task(runnable_ids[0])
        print(f"Completed: {runnable_ids[0]}")

    # =========================================================================
    # 3. Rust Async Transfer Engine Demo
    # =========================================================================
    print_header("3. Rust Async Transfer Engine")

    transfer_engine = rust.AsyncTransferEngine(max_concurrent=4)
    print("Created transfer engine (max 4 concurrent)")

    # Queue transfers
    for i in range(4):
        type_name = "h2d" if i % 2 == 0 else "d2h"
        op_id = transfer_engine.enqueue_with_priority(
            transfer_type=type_name,
            src_ptr=0x1000 + i * 0x1000,
            dst_ptr=0x2000 + i * 0x1000,
            size=1024 * 1024,
            priority=i % 3,
        )
        print(f"  Queued transfer {op_id}: {type_name.upper()}")

    # Simulate completion
    ready = transfer_engine.get_ready_transfers(max_transfers=4)
    for op in ready[:2]:
        transfer_engine.start_transfer(op.id)
        transfer_engine.complete_transfer(op.id)

    transfer_stats = transfer_engine.stats()
    print(
        f"Transfer stats: {transfer_stats.completed_count} completed, {transfer_stats.pending_count} pending"
    )

    # =========================================================================
    # 4. Rust Kernel Dispatch Controller Demo
    # =========================================================================
    print_header("4. Rust Kernel Dispatch Controller")

    dispatcher = rust.KernelDispatcher(max_in_flight=4)
    print("Created dispatcher (max 4 in-flight per stream)")

    for i in range(4):
        config = rust.LaunchConfig(
            grid=(128, 1, 1), block=(256, 1, 1), shared_mem=0, stream_id=i % 2
        )
        req_id = dispatcher.queue(kernel_handle=0xDEADBEEF + i, config=config, priority=i % 3)
        print(f"  Queued kernel {req_id}: stream={i % 2}")

    ready_kernels = dispatcher.get_ready(max_requests=4)
    for req in ready_kernels[:2]:
        dispatcher.mark_launched(req.id)
        dispatcher.mark_completed(req.id)

    dispatch_stats = dispatcher.stats()
    print(
        f"Dispatch stats: {dispatch_stats.completed_count} completed, {dispatch_stats.pending_count} pending"
    )

    # =========================================================================
    # 5. Admission Control (NEW)
    # =========================================================================
    print_header("5. Admission Control (NEW)")

    print("Testing admission control with memory and bandwidth limits...")

    # Create tasks and test admission via scheduler
    admission_scheduler = rust.Scheduler(
        sched_tick_ms=10.0,
        window_ms=100.0,
        total_memory=500 * 1024 * 1024,  # 500 MB limit
    )

    # Submit tasks that should fit
    for i in range(3):
        task = rust.TaskMeta(
            id=f"admit_{i}",
            name=f"Admissible Task {i}",
            memory_estimate=100 * 1024 * 1024,  # 100 MB each
            priority=1,
        )
        task_id = admission_scheduler.submit(task)
        print(f"  Admitted task {i}: {task_id}")

    admission_stats = admission_scheduler.stats()
    print("\nAdmission results:")
    print(f"  Total submitted: {admission_stats.total_submitted}")
    print(f"  Reserved memory: {admission_stats.reserved_memory / 1024 / 1024:.0f} MB")

    # =========================================================================
    # 6. QoS Policy Framework (NEW)
    # =========================================================================
    print_header("6. QoS Policy Framework (NEW)")

    print("Creating QoS tiers: Guaranteed, Burstable, BestEffort")

    # Create QoS policy evaluator
    qos_evaluator = rust.QosPolicyEvaluator(
        total_memory=1024 * 1024 * 1024,  # 1 GB
        total_bandwidth=1.0,
    )

    # Test different QoS classes
    qos_tasks = [
        rust.QosTaskMeta.guaranteed("high-priority", "Critical Task", 256 * 1024 * 1024),
        rust.QosTaskMeta.burstable("medium-priority", "Normal Task", 128 * 1024 * 1024, 2.0),
        rust.QosTaskMeta.best_effort("low-priority", "Background Task"),
    ]

    qos_class_names = {0: "Guaranteed", 1: "Burstable", 2: "BestEffort"}
    for task in qos_tasks:
        eval_result = qos_evaluator.evaluate(task)
        class_name = qos_class_names.get(int(task.qos_class), "Unknown")
        if eval_result.is_admitted():
            qos_evaluator.reserve(eval_result)
            print(
                f"  {class_name:12} | {task.name:15} | ADMITTED (priority={task.effective_priority()})"
            )
        elif eval_result.is_throttled():
            print(f"  {class_name:12} | {task.name:15} | THROTTLED")
        else:
            print(f"  {class_name:12} | {task.name:15} | QUEUED")

    qos_stats = qos_evaluator.stats()
    print("\nQoS stats:")
    print(f"  Guaranteed memory: {qos_stats.guaranteed_memory / 1024 / 1024:.0f} MB")
    print(f"  Burstable memory: {qos_stats.burstable_memory / 1024 / 1024:.0f} MB")
    print(f"  Available memory: {qos_stats.available_memory / 1024 / 1024:.0f} MB")
    print(f"  Best effort queue: {qos_stats.best_effort_queue}")

    # =========================================================================
    # 7. Kernel Pacing Engine (NEW)
    # =========================================================================
    print_header("7. Kernel Pacing Engine (NEW)")

    pacing_config = rust.PacingConfig(
        total_bandwidth=1.0, window_ms=100.0, min_interval_ms=0.1, adaptive=True
    )
    pacing_engine = rust.KernelPacingEngine(pacing_config)
    print(f"Created pacing engine: {pacing_config}")

    # Allocate bandwidth to streams
    pacing_engine.allocate_stream(0, 0.6)  # 60% to stream 0
    pacing_engine.allocate_stream(1, 0.3)  # 30% to stream 1
    print("\nAllocated bandwidth: stream 0=60%, stream 1=30%")

    # Test launch decisions
    for stream_id in [0, 1, 2]:  # 2 is unknown
        decision = pacing_engine.should_launch(stream_id)
        if decision.can_launch():
            pacing_engine.record_launch(stream_id)
            print(f"  Stream {stream_id}: LAUNCH")
        elif decision.is_throttled():
            pacing_engine.record_throttle(stream_id)
            print(f"  Stream {stream_id}: THROTTLED ({decision.decision_type})")
        else:
            print(f"  Stream {stream_id}: WAIT {decision.wait_ms():.2f}ms")

    pacing_stats = pacing_engine.stats()
    print("\nPacing stats:")
    print(f"  Streams: {pacing_stats.stream_count}")
    print(f"  Used bandwidth: {pacing_stats.used_bandwidth:.1%}")
    print(f"  Total launches: {pacing_stats.total_launches}")

    # =========================================================================
    # 8. Micro-Slicing Framework (NEW)
    # =========================================================================
    print_header("8. Micro-Slicing Framework (NEW)")

    slice_config = rust.SliceConfig(
        max_items_per_slice=10000, max_duration_ms=1.0, min_slices=2, max_slices=16, adaptive=True
    )
    slice_scheduler = rust.SliceScheduler(slice_config)
    print(f"Created slice scheduler: {slice_config}")

    # Submit kernels for slicing
    num_slices_1 = slice_scheduler.submit(
        kernel_handle=0xAAAA0001, total_items=50000, block=(256, 1, 1), shared_mem=0
    )
    print(f"\nKernel 1: 50000 items -> {num_slices_1} slices")

    num_slices_2 = slice_scheduler.submit_for_task(
        task_id="high-priority",
        kernel_handle=0xAAAA0002,
        total_items=30000,
        block=(256, 1, 1),
        shared_mem=0,
        priority=100,
    )
    print(f"Kernel 2: 30000 items -> {num_slices_2} slices (priority=100)")

    # Execute slices (round-robin)
    executed = 0
    print("\nExecuting slices (round-robin):")
    while executed < 4:
        slice_info = slice_scheduler.get_next_slice()
        if slice_info is None:
            break
        print(
            f"  Slice {slice_info.slice_id}: kernel=0x{slice_info.kernel_handle:X}, offset={slice_info.offset}, count={slice_info.count}"
        )
        slice_scheduler.complete_slice(0.1)  # 0.1ms exec time
        executed += 1

    slice_stats = slice_scheduler.stats()
    print("\nSlice stats:")
    print(f"  Total slices: {slice_stats.total_slices}")
    print(f"  Completed: {slice_stats.completed_slices}")
    print(f"  Pending: {slice_stats.pending_slices}")

    # =========================================================================
    # 9. Pinned Memory Support (NEW)
    # =========================================================================
    print_header("9. Pinned Memory Support (NEW)")

    pinned_config = rust.PinnedPoolConfig(
        max_size=256 * 1024 * 1024,  # 256 MB
        enable_pooling=True,
        alignment=256,
    )
    pinned_manager = rust.PinnedMemoryManager(pinned_config)
    print(f"Created pinned memory manager: {pinned_config}")

    # Allocate pinned memory
    allocations = []
    for i, size in enumerate([4096, 65536, 1048576]):  # 4KB, 64KB, 1MB
        result = pinned_manager.allocate(size)
        alloc_id, size_class, reused = result
        if not reused:
            # In real code, would call cudaHostAlloc here
            pinned_manager.register(alloc_id, 0x10000000 + i * 0x100000, size_class)
        allocations.append(alloc_id)
        print(f"  Allocated {size} bytes -> id={alloc_id}, class={size_class}, reused={reused}")

    # Associate with task
    pinned_manager.associate_task(allocations[0], "task-1")

    # Free and observe pooling
    should_free, ptr = pinned_manager.free(allocations[1])
    print(f"\nFreed allocation {allocations[1]}: should_free={should_free} (pooled)")

    # Allocate again - should hit pool
    result2 = pinned_manager.allocate(65536)
    print(f"Re-allocated 65KB: reused={result2[2]}")

    pinned_stats = pinned_manager.stats()
    print("\nPinned stats:")
    print(f"  Current used: {pinned_stats.current_used} bytes")
    print(f"  Pool hits: {pinned_stats.pool_hits}")
    print(f"  Pool misses: {pinned_stats.pool_misses}")
    print(f"  Hit rate: {pinned_stats.hit_rate():.1%}")

    # =========================================================================
    # 10. Kernel Cache (NEW)
    # =========================================================================
    print_header("10. Kernel Cache (NEW)")

    cache_config = rust.CacheConfig(
        max_entries=1024,
        max_ptx_size=256 * 1024 * 1024,  # 256 MB
        enable_eviction=True,
        ttl_seconds=0.0,  # No TTL
    )
    kernel_cache = rust.KernelCache(cache_config)
    print(f"Created kernel cache: {cache_config}")

    # Compile options
    compile_opts = rust.CompileOptions("sm_86").flag("-lineinfo").define("BLOCK_SIZE", "256")
    print(f"\nCompile options: {compile_opts}")

    # Insert kernels
    kernels = [
        ("__global__ void add_kernel(float* a, float* b, float* c) { ... }", "add_kernel"),
        ("__global__ void mul_kernel(float* a, float* b, float* c) { ... }", "mul_kernel"),
        (
            "__global__ void matmul_kernel(float* A, float* B, float* C, int M, int N, int K) { ... }",
            "matmul_kernel",
        ),
    ]

    for source, name in kernels:
        ptx = f"// PTX for {name}\n.version 7.0\n.target sm_86\n..."
        key = kernel_cache.insert(source, name, ptx, compile_opts)
        print(f"  Cached {name}: key={key}")

    # Test cache hits
    print("\nTesting cache hits:")
    for source, name in kernels:
        cached = kernel_cache.get_by_name(name, compile_opts)
        if cached:
            print(f"  {name}: HIT (accesses={cached.access_count})")
        else:
            print(f"  {name}: MISS")

    # Simulate module loading
    for i, (source, name) in enumerate(kernels):
        key = rust.KernelCache.compute_key(source, name, compile_opts)
        kernel_cache.set_handles(key, 0xAABB0000 + i, 0xCCDD0000 + i)

    cache_stats = kernel_cache.stats()
    print("\nCache stats:")
    print(f"  Entries: {cache_stats.entries}")
    print(f"  Hits: {cache_stats.hits}")
    print(f"  Misses: {cache_stats.misses}")
    print(f"  Hit rate: {cache_stats.hit_rate():.1%}")
    print(f"  PTX size: {cache_stats.ptx_size} bytes")
    print(f"  Loaded kernels: {cache_stats.loaded_count}")

    # =========================================================================
    # 11. GPU Partitioning (NEW)
    # =========================================================================
    print_header("11. GPU Partitioning (NEW)")

    partition_config = rust.PartitionConfig(
        total_memory=8 * 1024 * 1024 * 1024,  # 8 GB
        allow_overcommit=False,
        overcommit_ratio=1.0,
    )
    partition_manager = rust.PartitionManager(partition_config)
    print(f"Created partition manager: {partition_config}")

    # Create partitions
    partitions = [
        (
            "inference",
            "Inference Workload",
            rust.PartitionLimits.with_memory(4 * 1024 * 1024 * 1024).compute(0.5).bandwidth(0.4),
        ),
        (
            "training",
            "Training Workload",
            rust.PartitionLimits.with_memory(3 * 1024 * 1024 * 1024).compute(0.4).bandwidth(0.5),
        ),
    ]

    for pid, name, limits in partitions:
        partition_manager.create_partition(pid, name, limits)
        print(
            f"  Created partition '{pid}': memory={limits.memory_quota / 1024**3:.0f}GB, compute={limits.compute_share:.0%}"
        )

    # Assign tasks
    partition_manager.assign_task("inference-task-1", "inference")
    partition_manager.assign_task("training-task-1", "training")
    print("\nAssigned tasks to partitions")

    # Check partition for task
    for task_id in ["inference-task-1", "training-task-1", "unknown-task"]:
        partition = partition_manager.get_task_partition(task_id)
        if partition:
            print(f"  {task_id} -> {partition.id} ({partition.name})")
        else:
            print(f"  {task_id} -> (no partition)")

    partition_stats = partition_manager.stats()
    print("\nPartition stats:")
    print(f"  Partitions: {partition_stats.partition_count}")
    print(f"  Memory allocated: {partition_stats.total_memory_allocated / 1024**3:.1f} GB")
    print(f"  Compute allocated: {partition_stats.total_compute_allocated:.0%}")
    print(f"  Available memory: {partition_stats.available_memory / 1024**3:.1f} GB")
    print(f"  Available compute: {partition_stats.available_compute:.0%}")

    # =========================================================================
    # 12. Tiled Matmul Benchmark
    # =========================================================================
    print_header("12. Tiled Matmul Benchmark")

    print("\nMatrix Size | Kernel    | Time (ms) | GFLOPS  | vs NumPy")
    print("-" * 60)

    sizes = [512, 1024, 2048, 4096]
    results = []

    for size in sizes:
        M, N, K = size, size, size

        # Create test matrices
        A_np = np.random.randn(M, K).astype(np.float32)
        B_np = np.random.randn(K, N).astype(np.float32)

        # Warmup
        A_gpu = native.from_numpy(A_np)
        B_gpu = native.from_numpy(B_np)
        _ = native.matmul(A_gpu, B_gpu)

        # Benchmark GPU
        iterations = 5 if size >= 2048 else 10
        times = []
        for _ in range(iterations):
            A_gpu = native.from_numpy(A_np)
            B_gpu = native.from_numpy(B_np)
            start = time.perf_counter()
            C_gpu = native.matmul(A_gpu, B_gpu)
            gpu_time = time.perf_counter() - start
            times.append(gpu_time)

        avg_time = np.median(times)
        gflops = 2 * M * N * K / avg_time / 1e9

        kernel = "Tiled" if size >= 2048 else "L2-opt"

        # NumPy reference
        start = time.perf_counter()
        C_cpu = np.matmul(A_np, B_np)
        cpu_time = time.perf_counter() - start

        speedup = cpu_time / avg_time

        # Verify
        C_result = C_gpu.to_numpy()
        rel_error = np.max(np.abs(C_result - C_cpu)) / np.max(np.abs(C_cpu))

        results.append(
            {
                "size": size,
                "kernel": kernel,
                "time_ms": avg_time * 1000,
                "gflops": gflops,
                "speedup": speedup,
                "error": rel_error,
            }
        )

        status = "OK" if rel_error < 1e-3 else f"ERR:{rel_error:.1e}"
        print(
            f"{size:>5}x{size:<5} | {kernel:<9} | {avg_time * 1000:>8.2f} | {gflops:>7.1f} | {speedup:>5.1f}x ({status})"
        )

    print("-" * 60)

    peak = max(results, key=lambda x: x["gflops"])
    print(
        f"\nPeak: {peak['gflops']:.1f} GFLOPS at {peak['size']}x{peak['size']} ({peak['kernel']})"
    )

    # =========================================================================
    # Summary
    # =========================================================================
    print_header("Summary - PyGPUkit v0.2 Complete Features")

    print("""
    === Core Infrastructure ===
    1. Rust Memory Pool      - LRU eviction, size-class free lists
    2. Rust Scheduler        - Priority queue, memory reservation
    3. Rust Transfer Engine  - Separate H2D/D2H streams, priority
    4. Rust Kernel Dispatch  - Per-stream limits, lifecycle tracking

    === NEW v0.2 Features ===
    5. Admission Control     - Deterministic admission, quota enforcement
    6. QoS Policy Framework  - Guaranteed/Burstable/BestEffort tiers
    7. Kernel Pacing Engine  - Bandwidth-based throttling per stream
    8. Micro-Slicing         - Kernel splitting, round-robin fairness
    9. Pinned Memory         - Page-locked host memory with pooling
    10. Kernel Cache         - PTX caching, LRU eviction, TTL
    11. GPU Partitioning     - Resource isolation, multi-tenant

    === Compute ===
    12. Tiled Matmul         - Shared memory + double buffering
    """)

    # Count tests
    print("Total Rust tests: 106 passing")
    print("Features demonstrated: 12")

    print("\n" + "=" * 70)
    print(" PyGPUkit v0.2 Demo Complete!")
    print("=" * 70)

    return 0


if __name__ == "__main__":
    sys.exit(main())
