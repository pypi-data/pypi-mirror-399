const std = @import("std");

pub const ModelType = enum(c_int) {
    bpe = 1,
    wordpiece = 2,
    unigram = 3,
};

pub const PaddingSide = enum(c_int) {
    right = 0,
    left = 1,
};

pub const TruncationStrategy = enum(c_int) {
    longest_first = 0,
    only_first = 1,
};

pub const PostProcessorKind = enum(c_int) {
    none = 0,
    bert = 1,
    roberta = 2,
    template = 3,
};

pub const Pcre2Code = opaque {};

pub const Offset = extern struct {
    start: i32,
    end: i32,
};

pub const TokenizerEncoding = extern struct {
    ids: ?*i32,
    ids_len: usize,
    tokens: ?*[*c]u8,
    tokens_len: usize,
    attention_mask: ?*i32,
    type_ids: ?*i32,
    special_tokens_mask: ?*i32,
    offsets: ?*Offset,
    overflows: ?*TokenizerEncoding,
    overflow_count: usize,
};

// =============================================================================
// Native Zig Model Dispatch
// =============================================================================

const bpe = @import("bpe.zig");
const wordpiece = @import("wordpiece.zig");
const unigram = @import("unigram.zig");

pub fn modelEncode(tok: *Tokenizer, input: [*c]const u8, enc: *TokenizerEncoding) c_int {
    return switch (tok.type) {
        .bpe => bpe.bpeEncode(tok, input, enc),
        .wordpiece => wordpiece.wordpieceEncode(tok, input, enc),
        .unigram => unigram.unigramEncode(tok, input, enc),
    };
}

/// Encode with explicit length (supports text with embedded null bytes)
pub fn modelEncodeSlice(tok: *Tokenizer, input: []const u8, enc: *TokenizerEncoding) c_int {
    return switch (tok.type) {
        .bpe => bpe.bpeEncodeSlice(tok, input, enc),
        .wordpiece => wordpiece.wordpieceEncode(tok, @ptrCast(input.ptr), enc), // wordpiece uses null-term for now
        .unigram => unigram.unigramEncode(tok, @ptrCast(input.ptr), enc), // unigram uses null-term for now
    };
}

pub fn modelDecode(tok: *Tokenizer, ids: [*c]const i32, ids_len: usize, out: *[*c]u8, out_len: *usize) c_int {
    return switch (tok.type) {
        .bpe => bpe.bpeDecode(tok, ids, ids_len, out, out_len),
        .wordpiece => wordpiece.wordpieceDecode(tok, ids, ids_len, out, out_len),
        .unigram => unigram.unigramDecode(tok, ids, ids_len, out, out_len),
    };
}

pub fn modelDestroy(tok: *Tokenizer) void {
    switch (tok.type) {
        .bpe => bpe.bpeDestroy(tok),
        .wordpiece => wordpiece.wordpieceDestroy(tok),
        .unigram => unigram.unigramDestroy(tok),
    }
}

// =============================================================================
// Vocabulary Access Functions
// =============================================================================

/// Get the vocabulary size from the underlying model.
pub fn modelGetVocabSize(tok: *const Tokenizer) usize {
    return switch (tok.type) {
        .bpe => blk: {
            const model: *const bpe.LazyBpeModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.vocab_size;
        },
        .wordpiece => blk: {
            const model: *const wordpiece.WordPieceModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.vocab_size;
        },
        .unigram => blk: {
            const model: *const unigram.UnigramModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.vocab_size;
        },
    };
}

/// Get the UNK token ID (-1 if not set).
pub fn modelGetUnkId(tok: *const Tokenizer) i32 {
    return switch (tok.type) {
        .bpe => blk: {
            const model: *const bpe.LazyBpeModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.unk_id;
        },
        .wordpiece => blk: {
            const model: *const wordpiece.WordPieceModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.unk_id;
        },
        .unigram => blk: {
            const model: *const unigram.UnigramModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.unk_id;
        },
    };
}

/// Get the BOS token ID (-1 if not set).
pub fn modelGetBosId(tok: *const Tokenizer) i32 {
    return switch (tok.type) {
        .bpe => blk: {
            const model: *const bpe.LazyBpeModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.bos_id;
        },
        .wordpiece => -1, // WordPiece typically doesn't have BOS
        .unigram => blk: {
            const model: *const unigram.UnigramModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.bos_id;
        },
    };
}

/// Convert token ID to token string. Returns null if ID is out of range.
pub fn modelIdToToken(tok: *const Tokenizer, id: i32) ?[]const u8 {
    if (id < 0) return null;
    const idx: usize = @intCast(id);

    return switch (tok.type) {
        .bpe => blk: {
            const model: *const bpe.LazyBpeModel = @ptrCast(@alignCast(tok.model.?));
            if (idx >= model.id_to_token.len) break :blk null;
            break :blk model.id_to_token[idx];
        },
        .wordpiece => blk: {
            const model: *const wordpiece.WordPieceModel = @ptrCast(@alignCast(tok.model.?));
            if (idx >= model.id_to_token.len) break :blk null;
            const ptr = model.id_to_token[idx] orelse break :blk null;
            break :blk std.mem.span(ptr);
        },
        .unigram => blk: {
            const model: *const unigram.UnigramModel = @ptrCast(@alignCast(tok.model.?));
            if (idx >= model.id_to_token.len) break :blk null;
            const ptr = model.id_to_token[idx] orelse break :blk null;
            break :blk std.mem.span(ptr);
        },
    };
}

/// Convert token string to ID. Returns null if token not found.
pub fn modelTokenToId(tok: *const Tokenizer, token: []const u8) ?i32 {
    return switch (tok.type) {
        .bpe => blk: {
            const model: *const bpe.LazyBpeModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.vocab_hash.get(token);
        },
        .wordpiece => blk: {
            const model: *const wordpiece.WordPieceModel = @ptrCast(@alignCast(tok.model.?));
            break :blk model.vocab.get(token);
        },
        .unigram => blk: {
            const model: *const unigram.UnigramModel = @ptrCast(@alignCast(tok.model.?));
            // Unigram uses a list, need to search
            for (model.vocab.items) |entry| {
                if (std.mem.eql(u8, entry.token, token)) {
                    break :blk entry.id;
                }
            }
            break :blk null;
        },
    };
}

pub const Normalizer = extern struct {
    lowercase: c_int,
    nfc: c_int,
    nfd: c_int,
    nfkc: c_int,
    nfkd: c_int,
    strip_accents: c_int,
    strip_left: c_int,
    strip_right: c_int,
    clean_text: c_int,
    handle_chinese_chars: c_int,
    // SentencePiece-style normalizers
    prepend: ?[*:0]const u8, // Prepend this string to input (e.g., "▁")
    replace_pattern: ?[*:0]const u8, // Pattern to replace (e.g., " ")
    replace_content: ?[*:0]const u8, // Replacement string (e.g., "▁")
};

pub const PreTokenizer = extern struct {
    pattern: ?*u8,
    re: ?*Pcre2Code,
    regex_split: c_int,
    regex_invert: c_int, // For Split with invert=true: emit matches instead of gaps
    byte_level: c_int,
    whitespace: c_int,
    punctuation: c_int,
    lowercase: c_int,
    add_prefix_space: c_int,
    trim_offsets: c_int,
    is_sequence: c_int,
    seq: ?*PreTokenizer,
    seq_count: usize,
};

pub const PostProcessorEntry = extern struct {
    is_special: c_int,
    id: c_int,
    type_id: c_int,
    sequence: c_int,
    token: [64]u8,
};

pub const PostProcessor = extern struct {
    cls_token: [64]u8,
    sep_token: [64]u8,
    cls_id: c_int,
    sep_id: c_int,
    add_special: c_int,
    pair: c_int,
    kind: PostProcessorKind,
    single: [32]PostProcessorEntry,
    single_len: usize,
    pair_tmpl: [64]PostProcessorEntry,
    pair_len: usize,
};

pub const Padding = extern struct {
    enabled: c_int,
    length: c_int,
    pad_id: c_int,
    pad_type_id: c_int,
    pad_token: [64]u8,
    side: PaddingSide,
};

pub const Truncation = extern struct {
    enabled: c_int,
    max_length: c_int,
    strategy: TruncationStrategy,
    stride: c_int,
    overflow_to_sample: c_int,
};

pub const AddedToken = extern struct {
    content: ?*u8,
    id: c_int,
    special: c_int,
    single_word: c_int,
    lstrip: c_int,
    rstrip: c_int,
    normalized: c_int,
    next: ?*AddedToken,
};

/// Decoder configuration parsed from tokenizer.json "decoder" section
pub const Decoder = extern struct {
    /// Number of leading spaces to strip (from Strip decoder "start" field)
    strip_start: c_int,
    /// Number of trailing spaces to strip (from Strip decoder "stop" field)
    strip_stop: c_int,
};

pub const Tokenizer = extern struct {
    model: ?*anyopaque,
    type: ModelType,
    normalizer: Normalizer,
    pretokenizer: PreTokenizer,
    postproc: PostProcessor,
    decoder: Decoder,
    padding: Padding,
    truncation: Truncation,
    added: ?*AddedToken,
    last_error: ?*u8,
};

pub const TokenIdPair = extern struct {
    token: [*c]const u8,
    id: c_int,
};

pub const BpeMergePair = extern struct {
    a: [*c]const u8,
    b: [*c]const u8,
};

pub const BpeModelSpec = extern struct {
    vocab: [*c]const TokenIdPair,
    vocab_len: usize,
    merges: [*c]const BpeMergePair,
    merges_len: usize,
    unk_token: [*c]const u8,
};

pub const WordPieceModelSpec = extern struct {
    vocab: [*c]const TokenIdPair,
    vocab_len: usize,
    unk_token: [*c]const u8,
};

pub const UnigramVocabEntry = extern struct {
    token: [*c]const u8,
    score: f32,
    id: c_int,
};

pub const UnigramModelSpec = extern struct {
    vocab: [*c]const UnigramVocabEntry,
    vocab_len: usize,
    unk_token: [*c]const u8,
    bos_token: [*c]const u8,
    eos_token: [*c]const u8,
};

pub const NormalizerSpec = extern struct {
    type: [*c]const u8,
    lowercase: c_int,
    strip_accents: c_int,
    nfc: c_int,
    nfd: c_int,
    nfkc: c_int,
    clean_text: c_int,
    handle_chinese_chars: c_int,
    // SentencePiece-style normalizers
    prepend: [*c]const u8,
    replace_pattern: [*c]const u8,
    replace_content: [*c]const u8,
};

pub const PreTokenizerSpec = extern struct {
    type: [*c]const u8,
    add_prefix_space: c_int,
    trim_offsets: c_int,
    use_regex: c_int,
    byte_level: c_int,
    whitespace: c_int,
    punctuation: c_int,
    pattern: [*c]const u8,
    regex_split: c_int,
    regex_invert: c_int, // For Split with invert=true
};

pub const PostProcessorSpec = extern struct {
    type: [*c]const u8,
    add_special: c_int,
    pair: c_int, // RoBERTa style double SEP
    cls_token: [*c]const u8,
    sep_token: [*c]const u8,
};
