//! Graph Parser
//!
//! Parses compute graph definitions from JSON (_graphs/*.json files).
//! These JSON files are generated by Python's @architecture decorator.

const std = @import("std");
const Allocator = std.mem.Allocator;

const types = @import("types.zig");
const Op = types.Op;
const OpType = types.OpType;
const OpInput = types.OpInput;
const Architecture = types.Architecture;

// =============================================================================
// Public API
// =============================================================================

/// Parse architecture definition from JSON string.
pub fn parseFromJson(allocator: Allocator, json_str: []const u8) !Architecture {
    const parsed = try std.json.parseFromSlice(std.json.Value, allocator, json_str, .{});
    defer parsed.deinit();

    if (parsed.value != .object) return error.InvalidJson;
    const obj = parsed.value.object;

    // Parse name
    const name_val = obj.get("name") orelse return error.MissingName;
    const name = switch (name_val) {
        .string => |s| try allocator.dupe(u8, s),
        else => return error.InvalidName,
    };
    errdefer allocator.free(name);

    // Parse model_types
    const model_types_val = obj.get("model_types") orelse return error.MissingModelTypes;
    const model_types = switch (model_types_val) {
        .array => |arr| blk: {
            var mtypes = try allocator.alloc([]const u8, arr.items.len);
            for (arr.items, 0..) |item, i| {
                mtypes[i] = switch (item) {
                    .string => |s| try allocator.dupe(u8, s),
                    else => return error.InvalidModelType,
                };
            }
            break :blk mtypes;
        },
        else => return error.InvalidModelTypes,
    };
    errdefer {
        for (model_types) |mt| allocator.free(mt);
        allocator.free(model_types);
    }

    // Parse block ops
    const block_val = obj.get("block") orelse return error.MissingBlock;
    const block_ops = switch (block_val) {
        .array => |arr| try parseOps(allocator, arr.items),
        else => return error.InvalidBlock,
    };
    errdefer allocator.free(block_ops);

    // Parse pre_block ops (optional)
    const pre_block_ops: []const Op = if (obj.get("pre_block")) |v| switch (v) {
        .array => |arr| try parseOps(allocator, arr.items),
        else => &.{},
    } else &.{};
    errdefer if (pre_block_ops.len > 0) allocator.free(pre_block_ops);

    // Parse post_block ops (optional)
    const post_block_ops: []const Op = if (obj.get("post_block")) |v| switch (v) {
        .array => |arr| try parseOps(allocator, arr.items),
        else => &.{},
    } else &.{};
    errdefer if (post_block_ops.len > 0) allocator.free(post_block_ops);

    // Analyze ops to derive flags
    const flags = analyzeOps(block_ops, pre_block_ops);

    return Architecture{
        .name = name,
        .model_types = model_types,
        .block_ops = block_ops,
        .pre_block_ops = pre_block_ops,
        .post_block_ops = post_block_ops,
        .has_qk_norm = flags.has_qk_norm,
        .has_moe = flags.has_moe,
        .has_fused_qkv = flags.has_fused_qkv,
        .has_fused_gate_up = flags.has_fused_gate_up,
        .num_norms_per_block = flags.num_norms,
        .use_gelu = flags.use_gelu,
        .embedding_multiplier = flags.embedding_multiplier,
        .norm_weight_offset = flags.norm_weight_offset,
        .explicit_qk_norm_ops = flags.explicit_qk_norm_ops,
    };
}

// =============================================================================
// Op Parsing
// =============================================================================

fn parseOps(allocator: Allocator, items: []const std.json.Value) ![]const Op {
    var ops = try allocator.alloc(Op, items.len);
    errdefer allocator.free(ops);

    for (items, 0..) |item, i| {
        ops[i] = try parseOp(allocator, item);
    }
    return ops;
}

fn parseOp(allocator: Allocator, value: std.json.Value) !Op {
    if (value != .object) return error.InvalidBlockOp;
    const obj = value.object;

    // Parse op type
    const op_str = switch (obj.get("op") orelse return error.MissingOpType) {
        .string => |s| s,
        else => return error.InvalidOpType,
    };

    const op_type = parseOpType(op_str) orelse return error.UnknownOpType;

    // Parse optional fields
    const name: ?[]const u8 = if (obj.get("name")) |v| switch (v) {
        .string => |s| try allocator.dupe(u8, s),
        else => null,
    } else null;

    const activation: ?[]const u8 = if (obj.get("activation")) |v| switch (v) {
        .string => |s| try allocator.dupe(u8, s),
        else => null,
    } else null;

    const weight_offset: f32 = if (obj.get("weight_offset")) |v| switch (v) {
        .float => |f| @floatCast(f),
        .integer => |i| @floatFromInt(i),
        else => 0.0,
    } else 0.0;

    // Parse inputs array
    const inputs: []const OpInput = if (obj.get("inputs")) |v| switch (v) {
        .array => |arr| try parseInputs(allocator, arr.items),
        else => &.{},
    } else &.{};

    // Parse split_sizes array
    const split_sizes: []const i32 = if (obj.get("split_sizes")) |v| switch (v) {
        .array => |arr| blk: {
            var sizes = allocator.alloc(i32, arr.items.len) catch break :blk &[_]i32{};
            for (arr.items, 0..) |item, i| {
                sizes[i] = switch (item) {
                    .integer => |int| @intCast(int),
                    else => 0,
                };
            }
            break :blk sizes;
        },
        else => &.{},
    } else &.{};

    const dim0: i32 = if (obj.get("dim0")) |v| switch (v) {
        .integer => |int| @intCast(int),
        else => -1,
    } else -1;

    const dim1: i32 = if (obj.get("dim1")) |v| switch (v) {
        .integer => |int| @intCast(int),
        else => -1,
    } else -1;

    // Parse shape array
    const shape: []const i32 = if (obj.get("shape")) |v| switch (v) {
        .array => |arr| blk: {
            var dims = allocator.alloc(i32, arr.items.len) catch break :blk &[_]i32{};
            for (arr.items, 0..) |item, i| {
                dims[i] = switch (item) {
                    .integer => |int| @intCast(int),
                    .string => |s| if (std.mem.eql(u8, s, "B"))
                        -2
                    else if (std.mem.eql(u8, s, "T"))
                        -3
                    else
                        -1,
                    else => -1,
                };
            }
            break :blk dims;
        },
        else => &.{},
    } else &.{};

    // Parse outputs array
    const outputs: []const []const u8 = if (obj.get("outputs")) |v| switch (v) {
        .array => |arr| blk: {
            var outs = allocator.alloc([]const u8, arr.items.len) catch break :blk &[_][]const u8{};
            for (arr.items, 0..) |item, i| {
                outs[i] = switch (item) {
                    .string => |s| allocator.dupe(u8, s) catch "",
                    else => "",
                };
            }
            break :blk outs;
        },
        else => &.{},
    } else &.{};

    // Parse config flags
    var qk_norm = getBool(obj, "qk_norm") orelse false;
    var fused_qkv = getBool(obj, "fused_qkv") orelse false;
    var fused_gate_up = getBool(obj, "fused_gate_up") orelse false;

    // Also check "config" array for flags
    if (obj.get("config")) |config_val| {
        if (config_val == .array) {
            for (config_val.array.items) |item| {
                if (item == .string) {
                    if (std.mem.eql(u8, item.string, "qk_norm")) {
                        qk_norm = true;
                    } else if (std.mem.eql(u8, item.string, "fused_qkv")) {
                        fused_qkv = true;
                    } else if (std.mem.eql(u8, item.string, "fused_gate_up")) {
                        fused_gate_up = true;
                    }
                }
            }
        }
    }

    return Op{
        .op_type = op_type,
        .name = name,
        .inputs = inputs,
        .outputs = outputs,
        .weight_offset = weight_offset,
        .qk_norm = qk_norm,
        .fused_qkv = fused_qkv,
        .fused_gate_up = fused_gate_up,
        .sliding_window = getInt(obj, "sliding_window"),
        .activation = activation,
        .num_experts = getInt(obj, "num_experts") orelse 0,
        .experts_per_token = getInt(obj, "experts_per_token") orelse 0,
        .scale = getFloat(obj, "scale") orelse 1.0,
        .num_outputs = getInt(obj, "num_outputs") orelse 0,
        .dim = getInt(obj, "dim") orelse -1,
        .keepdim = getBool(obj, "keepdim") orelse false,
        .exponent = getFloat(obj, "exponent") orelse 1.0,
        .shape = shape,
        .split_sizes = split_sizes,
        .dim0 = dim0,
        .dim1 = dim1,
    };
}

fn parseOpType(s: []const u8) ?OpType {
    if (std.mem.eql(u8, s, "norm")) return .norm;
    if (std.mem.eql(u8, s, "multihead_attention") or std.mem.eql(u8, s, "attention")) return .multihead_attention;
    if (std.mem.eql(u8, s, "mlp")) return .mlp;
    if (std.mem.eql(u8, s, "moe")) return .moe;
    if (std.mem.eql(u8, s, "add")) return .add;
    if (std.mem.eql(u8, s, "mul")) return .mul;
    if (std.mem.eql(u8, s, "mean")) return .mean;
    if (std.mem.eql(u8, s, "pow")) return .pow;
    if (std.mem.eql(u8, s, "rsqrt")) return .rsqrt;
    if (std.mem.eql(u8, s, "matmul")) return .matmul;
    if (std.mem.eql(u8, s, "split")) return .split;
    if (std.mem.eql(u8, s, "transpose")) return .transpose;
    if (std.mem.eql(u8, s, "reshape")) return .reshape;
    if (std.mem.eql(u8, s, "softmax")) return .softmax;
    if (std.mem.eql(u8, s, "silu")) return .silu;
    if (std.mem.eql(u8, s, "gelu")) return .gelu;
    if (std.mem.eql(u8, s, "embedding")) return .embedding;
    if (std.mem.eql(u8, s, "linear")) return .linear;
    if (std.mem.eql(u8, s, "rope")) return .rope;
    if (std.mem.eql(u8, s, "triu")) return .triu;
    if (std.mem.eql(u8, s, "scaled_dot_product_attention")) return .scaled_dot_product_attention;
    return null;
}

fn parseInputs(allocator: Allocator, items: []const std.json.Value) ![]const OpInput {
    var inputs = try allocator.alloc(OpInput, items.len);
    errdefer allocator.free(inputs);

    for (items, 0..) |item, i| {
        inputs[i] = try parseInput(allocator, item);
    }
    return inputs;
}

fn parseInput(allocator: Allocator, value: std.json.Value) !OpInput {
    const obj = switch (value) {
        .object => |o| o,
        else => return error.InvalidInput,
    };

    if (obj.get("tensor")) |v| {
        return switch (v) {
            .string => |s| .{ .tensor = try allocator.dupe(u8, s) },
            else => error.InvalidInput,
        };
    }

    if (obj.get("scalar")) |v| {
        return switch (v) {
            .float => |f| .{ .scalar = @floatCast(f) },
            .integer => |i| .{ .scalar = @floatFromInt(i) },
            else => error.InvalidInput,
        };
    }

    return error.InvalidInput;
}

// =============================================================================
// JSON Helpers
// =============================================================================

fn getBool(obj: std.json.ObjectMap, key: []const u8) ?bool {
    const val = obj.get(key) orelse return null;
    return switch (val) {
        .bool => |b| b,
        else => null,
    };
}

fn getInt(obj: std.json.ObjectMap, key: []const u8) ?i32 {
    const val = obj.get(key) orelse return null;
    return switch (val) {
        .integer => |i| @intCast(i),
        else => null,
    };
}

fn getFloat(obj: std.json.ObjectMap, key: []const u8) ?f32 {
    const val = obj.get(key) orelse return null;
    return switch (val) {
        .float => |f| @floatCast(f),
        .integer => |i| @floatFromInt(i),
        else => null,
    };
}

// =============================================================================
// Op Analysis
// =============================================================================

const AnalysisFlags = struct {
    has_qk_norm: bool,
    has_moe: bool,
    has_fused_qkv: bool,
    has_fused_gate_up: bool,
    use_gelu: bool,
    num_norms: u8,
    norm_weight_offset: f32,
    explicit_qk_norm_ops: bool,
    embedding_multiplier: f32,
};

fn analyzeOps(block_ops: []const Op, pre_block_ops: []const Op) AnalysisFlags {
    var flags = AnalysisFlags{
        .has_qk_norm = false,
        .has_moe = false,
        .has_fused_qkv = false,
        .has_fused_gate_up = false,
        .use_gelu = false,
        .num_norms = 0,
        .norm_weight_offset = 0.0,
        .explicit_qk_norm_ops = false,
        .embedding_multiplier = 1.0,
    };

    for (block_ops) |op| {
        switch (op.op_type) {
            .norm => {
                if (isQKNormName(op.name)) {
                    flags.has_qk_norm = true;
                } else {
                    flags.num_norms += 1;
                }
                if (op.weight_offset != 0.0 and flags.norm_weight_offset == 0.0) {
                    flags.norm_weight_offset = op.weight_offset;
                }
            },
            .multihead_attention => {
                if (op.qk_norm) flags.has_qk_norm = true;
                if (op.fused_qkv) flags.has_fused_qkv = true;
            },
            .mlp => {
                if (op.activation) |act| {
                    if (std.mem.eql(u8, act, "gelu")) flags.use_gelu = true;
                }
                if (op.fused_gate_up) flags.has_fused_gate_up = true;
            },
            .moe => flags.has_moe = true,
            else => {},
        }

        // Check for explicit QK norm ops in inputs
        if (!flags.explicit_qk_norm_ops) {
            for (op.inputs) |inp| {
                switch (inp) {
                    .tensor => |t| {
                        if (std.mem.endsWith(u8, t, "q_norm.weight") or std.mem.endsWith(u8, t, "k_norm.weight")) {
                            flags.explicit_qk_norm_ops = true;
                            break;
                        }
                    },
                    else => {},
                }
            }
        }
    }

    // Analyze pre_block ops for embedding_multiplier
    for (pre_block_ops) |op| {
        if (op.op_type == .mul) {
            for (op.inputs) |inp| {
                switch (inp) {
                    .scalar => |s| {
                        flags.embedding_multiplier = s;
                    },
                    .tensor => {},
                }
            }
        }
    }

    return flags;
}

fn isQKNormName(name: ?[]const u8) bool {
    if (name) |n| {
        return std.mem.endsWith(u8, n, "q_norm") or std.mem.endsWith(u8, n, "k_norm");
    }
    return false;
}
