[FORMALIZER_AGENT_LLM]
model = kdavis/goedel-formalizer-v2:32b
# provider can be: ollama, vllm, lmstudio, or openai
provider = ollama
url = http://localhost:11434/v1
max_tokens = 50000
num_ctx = 40960
max_retries = 10
max_remote_retries = 5
# Optional vLLM-specific parameters (ignored by Ollama/LM Studio/OpenAI)
# use_beam_search = false
# best_of = 1
# top_k = -1
# repetition_penalty = 1.0
# length_penalty = 1.0
# Optional LM Studio parameters (ignored by Ollama/vLLM/OpenAI)
# ttl = 300

[PROVER_AGENT_LLM]
model = kdavis/Goedel-Prover-V2:32b
# provider can be: ollama, vllm, lmstudio, or openai
provider = ollama
url = http://localhost:11434/v1
max_tokens = 50000
num_ctx = 40960
max_self_correction_attempts = 2
max_depth = 20
max_pass = 32
max_remote_retries = 5
# Optional vLLM-specific parameters (ignored by Ollama/LM Studio/OpenAI)
# use_beam_search = false
# best_of = 1
# top_k = -1
# repetition_penalty = 1.0
# length_penalty = 1.0
# Optional LM Studio parameters (ignored by Ollama/vLLM/OpenAI)
# ttl = 300

[SEMANTICS_AGENT_LLM]
model = qwen3:30b
# provider can be: ollama, vllm, lmstudio, or openai
provider = ollama
url = http://localhost:11434/v1
max_tokens = 50000
num_ctx = 262144
max_remote_retries = 5
# Optional vLLM-specific parameters (ignored by Ollama/LM Studio/OpenAI)
# use_beam_search = false
# best_of = 1
# top_k = -1
# repetition_penalty = 1.0
# length_penalty = 1.0
# Optional LM Studio parameters (ignored by Ollama/vLLM/OpenAI)
# ttl = 300

[SEARCH_QUERY_AGENT_LLM]
model = qwen3:30b
# provider can be: ollama, vllm, lmstudio, or openai
provider = ollama
url = http://localhost:11434/v1
max_tokens = 50000
num_ctx = 262144
max_remote_retries = 5
# Optional vLLM-specific parameters (ignored by Ollama/LM Studio/OpenAI)
# use_beam_search = false
# best_of = 1
# top_k = -1
# repetition_penalty = 1.0
# length_penalty = 1.0
# Optional LM Studio parameters (ignored by Ollama/vLLM/OpenAI)
# ttl = 300

[DECOMPOSER_AGENT_LLM]
model = gpt-5.2-2025-12-11
# provider can be: ollama, vllm, lmstudio, or openai
provider = openai
# url is optional when provider=openai (defaults to https://api.openai.com/v1)
# url is required when provider=ollama/vllm/lmstudio (with provider-specific defaults)
url = https://api.openai.com/v1
max_tokens = 50000
# max_completion_tokens is deprecated, use max_tokens instead (supported for backward compatibility)
# max_completion_tokens = 50000
max_remote_retries = 5
max_self_correction_attempts = 6
# Optional parameters (only used when provider != "openai")
# num_ctx = 40960
# Optional vLLM-specific parameters (ignored by Ollama/LM Studio/OpenAI)
# use_beam_search = false
# best_of = 1
# top_k = -1
# repetition_penalty = 1.0
# length_penalty = 1.0
# Optional LM Studio parameters (ignored by Ollama/vLLM/OpenAI)
# ttl = 300

[KIMINA_LEAN_SERVER]
url = http://0.0.0.0:8000
max_retries = 5

[LEAN_EXPLORE_SERVER]
url = http://localhost:8001/api/v1
package_filters = Mathlib,Batteries,Std,Init,Lean

[PROOF_RECONSTRUCTION]
# If final verification fails after "Proof completed successfully.", try up to this many
# whole-file reconstruction variants and pick the first that Kimina marks complete.
max_candidates = 64
