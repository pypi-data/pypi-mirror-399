import time
import math
from pathlib import Path

class IntelligentFileRestorer():
    def __init__(self):
        self.max_files = 20
        self.max_tokens_per_file = 8192
        self.total_token_limit = 32768
        self.scoring_weights = {
            "temporal": 0.35,
            "frequency": 0.25,
            "operation": 0.20,
            "fileType": 0.20,
        }

    def calculate_importance_score(self, metadata):
        total_score = 0.0
        temporal_score = self.calculate_temporal_score(metadata)
        total_score += temporal_score * self.scoring_weights["temporal"]
        frequency_score = self.calculate_frequency_score(metadata)
        total_score += frequency_score * self.scoring_weights["frequency"]
        operation_score = self.calculate_operation_score(metadata)
        total_score += operation_score * self.scoring_weights["operation"]
        file_type_score = self.calculate_file_type_score(metadata)
        total_score += file_type_score * self.scoring_weights["fileType"]

        return round(total_score)

    def calculate_temporal_score(self, metadata):
        now = time.time() * 1000
        hours_since_last_access = (now - metadata["lastAccessTime"]) / (1000 * 60 * 60)
        if hours_since_last_access <= 1:
            return 100
        elif hours_since_last_access <= 6:
            return 90
        elif hours_since_last_access <= 24:
            return 75
        else:
            return max(10, 75 * math.exp(-0.1 * (hours_since_last_access - 24)))

    def calculate_frequency_score(self, metadata):
        total_operations = metadata["readCount"] + metadata["writeCount"] + metadata["editCount"]
        score = min(80, total_operations * 5)
        recent_operations = metadata.get("operationsInLastHour", 0)
        score += min(20, recent_operations * 10)
        return min(100, score)

    def calculate_operation_score(self, metadata):
        score = 0
        score += metadata["writeCount"] * 15
        score += metadata["editCount"] * 10
        score += metadata["readCount"] * 3
        if metadata.get("lastOperation") == "write":
            score += 25
        elif metadata.get("lastOperation") == "edit":
            score += 15
        return min(100, score)

    def calculate_file_type_score(self, metadata):
        extension = metadata["path"].split(".")[-1].lower()
        code_extensions = {
            "js": 100, "ts": 100, "jsx": 95, "tsx": 95,
            "py": 90, "java": 85, "cpp": 85, "c": 85,
            "go": 80, "rs": 80, "php": 75, "rb": 75
        }
        
        config_extensions = {
            "json": 70, "yaml": 65, "yml": 65, "toml": 60,
            "xml": 55, "ini": 50, "env": 50, "config": 50

        }

        doc_extensions = {
            "md": 40, "txt": 30, "doc": 25, "docx": 25,
            "pdf": 20, "html": 35, "css": 45

        }

        if extension in code_extensions:
            return code_extensions[extension]
        elif extension in config_extensions:
            return config_extensions[extension]
        elif extension in doc_extensions:
            return doc_extensions[extension]
        return 30

    def find_best_fit_file(self, files, remaining_tokens):
        sorted_files = sorted(files, key=lambda f: f["score"], reverse=True)
        for f in sorted_files:
            if f["estimatedTokens"] <= remaining_tokens:
                return f
        return None

    def select_optimal_file_set(self, ranked_files):
        selected_files = []
        total_tokens = 0
        file_count = 0
        sorted_files = sorted(ranked_files, key=lambda f: f["score"], reverse=True)
        i = 0
        while i < len(sorted_files):
            file = sorted_files[i]
            if file_count >= self.max_files:
                #print(f"ğŸ“Š è¾¾åˆ°æ–‡ä»¶æ•°é‡é™åˆ¶: {self.max_files}")
                break
            if file["estimatedTokens"] > self.max_tokens_per_file:
                #print(f"âš ï¸ æ–‡ä»¶ {file['path']} è¶…å‡ºå•æ–‡ä»¶é™åˆ¶ï¼Œè·³è¿‡")
                i += 1
                continue
            if total_tokens + file["estimatedTokens"] > self.total_token_limit:
                #print(f"ğŸ“Š æ·»åŠ  {file['path']} å°†è¶…å‡ºæ€»Tokené™åˆ¶")
                remaining_tokens = self.total_token_limit - total_tokens
                alternative_file = self.find_best_fit_file(sorted_files[i+1:], remaining_tokens)
                if alternative_file:
                    selected_files.append(alternative_file)
                    total_tokens += alternative_file["estimatedTokens"]
                    file_count += 1
                    sorted_files.remove(alternative_file)
                i += 1
                continue
            selected_files.append(file)
            total_tokens += file["estimatedTokens"]
            file_count += 1
            i += 1
        return {
            "files": selected_files,
            "totalFiles": file_count,
            "totalTokens": total_tokens,
            "efficiency": (total_tokens / self.total_token_limit) * 100 if self.total_token_limit > 0 else 0
        }

    # API
    def file_recover(self, file_counter) -> str:
        if not file_counter:
            #print("âš ï¸ æš‚æ— æ–‡ä»¶è®°å½•ï¼Œæ— æ³•æ¢å¤ã€‚")
            return '' 

        # 1. è®¡ç®—æ¯æ¡è®°å½•çš„ importance score
        ranked_files = []
        for meta in file_counter.values():
            meta_copy = meta.copy()              # é˜²æ­¢æ‰“åˆ†å‡½æ•°å†…éƒ¨ä¿®æ”¹
            meta_copy["score"] = self.calculate_importance_score(meta_copy)
            ranked_files.append(meta_copy)

        # 2. æŒ‰åˆ†æ•°+çº¦æŸé€‰æœ€ä¼˜æ–‡ä»¶é›†
        selected = self.select_optimal_file_set(ranked_files)

        # 3. æŒ‰åˆ†æ•°å€’åºè¯»å–å†…å®¹
        contents = []
        base_path = Path.cwd().resolve()
        for file_info in sorted(selected["files"], key=lambda f: f["score"], reverse=True):
            full_path = base_path / file_info["path"]
            try:
                text = full_path.read_text(encoding="utf-8")
                contents.append(
                    f"File: {file_info['path']}\n"
                    f"Score: {file_info['score']}\n"
                    f"Content:\n{text}\n\n"
                )
            except Exception as e:
                contents.append(
                    f"File: {file_info['path']}\n"
                    f"Error reading: {e}\n\n"
                )

        return "".join(contents)

    # API
    def update_file_metrics(self, arguments, result, file_metrics, tool_name):
        try:
            # 1) å–æ–‡ä»¶è·¯å¾„
            file_path_str = None
            if isinstance(result, dict) and "file_path" in result:
                file_path_str = result["file_path"]
            elif isinstance(arguments, dict):
                file_path_str = arguments.get("path")

            if not file_path_str:
                raise ValueError("missing file path")

            file_path = Path(file_path_str).resolve()

            # 2) è®¡ç®— key
            try:
                key = str(file_path.relative_to(Path.cwd()))
            except ValueError:
                key = str(file_path)

            # 3) é‡æ–° stat â€”â€” å¤±è´¥å°±æ•´ä½“è·³è¿‡ï¼Œä¸ç¡¬å‡‘
            st = file_path.stat()
            last_access_ms = int(st.st_atime * 1000)
            est_tokens = st.st_size // 4

        except Exception:
            # ä»»ä½•ä¸€æ­¥æ‹¿ä¸åˆ°å¯é æ•°æ®å°±ç›´æ¥æ”¾å¼ƒæœ¬æ¬¡æŒ‡æ ‡æ›´æ–°
            return

        # 3) å»ºæ¡£æ¡ˆï¼ˆå°½é‡ä» stat è¡¥å……ï¼›å¤±è´¥åˆ™ä½¿ç”¨å…œåº•å€¼ï¼‰
        if key not in file_metrics:
            # ç¬¬ä¸€æ¬¡è§ï¼šæ ¹æ®æœ¬æ¬¡å·¥å…·ç±»å‹åˆå§‹åŒ–è®¡æ•°
            init_read = 1 if tool_name == "read_file" else 0
            init_write = 1 if tool_name == "write_file" else 0
            init_edit = 1 if tool_name == "edit" else 0
            last_op = {"read_file": "read", "write_file": "write", "edit": "edit"}[tool_name]

            file_metrics[key] = {
                "path": key,
                "lastAccessTime": last_access_ms,
                "readCount": init_read,
                "writeCount": init_write,
                "editCount": init_edit,
                "operationsInLastHour": 0,
                "lastOperation": last_op,
                "estimatedTokens": est_tokens,
            }
        else:
            # å·²å­˜åœ¨ï¼šåªç´¯åŠ è®¡æ•°ã€åˆ·æ–°æ—¶é—´å’Œå¤§å°
            meta = file_metrics[key]

            if tool_name == "read_file":
                meta["readCount"] += 1
                meta["lastOperation"] = "read"
            elif tool_name == "write_file":
                meta["writeCount"] += 1
                meta["lastOperation"] = "write"
            elif tool_name == "edit":
                meta["editCount"] += 1
                meta["lastOperation"] = "edit"

            meta["lastAccessTime"] = last_access_ms
            meta["estimatedTokens"] = est_tokens
