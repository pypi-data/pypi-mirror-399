# Meitei Senter

A lightweight sentence boundary detector for **Meitei Mayek (Manipuri)** text.

[![PyPI version](https://badge.fury.io/py/meitei-senter.svg)](https://pypi.org/project/meitei-senter/)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
![Model Size](https://img.shields.io/badge/model%20size-1MB-green)
![F-Score](https://img.shields.io/badge/F--Score-94.7%25-brightgreen)

## Features

- ğŸš€ **Lightweight** - Only ~1MB model, minimal dependencies
- ğŸ¯ **Accurate** - 94.7% F-Score on Meitei text
- ğŸ”§ **Easy to use** - Simple Python API and CLI
- âš¡ **Fast** - Optimized for quick inference

---

## Installation

```bash
pip install meitei-senter
```

### Optional: spaCy Backend (for higher accuracy)
```bash
pip install meitei-senter[spacy]
```

---

## Quick Start

### Python API

```python
from meitei_senter import MeiteiSentenceSplitter

# Initialize the splitter
splitter = MeiteiSentenceSplitter()

# Split text into sentences
text = "ê¯†ê¯¦ê¯”ê¯£ê¯€ê¯¤ ê¯‘ê¯ê¯¤ ê¯‘ê¯£ê¯€ê¯­ê¯‚ê¯¥ê¯ê¯£ê¯ƒê¯¥ê¯’ê¯¤ ê¯ê¯ê¯”ê¯…ê¯¤ ê¯« ê¯ƒê¯ê¯¤ ê¯Œê¯¥ê¯ê¯… ê¯†ê¯¥ê¯ê¯ ê¯«"
sentences = splitter.split_sentences(text)

for i, sent in enumerate(sentences, 1):
    print(f"{i}. {sent}")
```

**Output:**
```
1. ê¯†ê¯¦ê¯”ê¯£ê¯€ê¯¤ ê¯‘ê¯ê¯¤ ê¯‘ê¯£ê¯€ê¯­ê¯‚ê¯¥ê¯ê¯£ê¯ƒê¯¥ê¯’ê¯¤ ê¯ê¯ê¯”ê¯…ê¯¤ ê¯«
2. ê¯ƒê¯ê¯¤ ê¯Œê¯¥ê¯ê¯… ê¯†ê¯¥ê¯ê¯ ê¯«
```

### Command Line

```bash
# Interactive mode
meitei-senter --interactive

# Direct text input
meitei-senter --text "ê¯†ê¯¦ê¯”ê¯£ê¯€ê¯¤ ê¯‘ê¯ê¯¤ ê¯‘ê¯£ê¯€ê¯­ê¯‚ê¯¥ê¯ê¯£ê¯ƒê¯¥ê¯’ê¯¤ ê¯ê¯ê¯”ê¯…ê¯¤ ê¯« ê¯ƒê¯ê¯¤ ê¯Œê¯¥ê¯ê¯… ê¯†ê¯¥ê¯ê¯ ê¯«"

# Show version
meitei-senter --version
```

---

## Advanced Usage

### Using the Convenient Loader

```python
from meitei_senter import load_splitter

# Load with default (delimiter-based) backend
splitter = load_splitter()

# Or with spaCy backend (requires spacy extra)
splitter = load_splitter(use_spacy=True)

sentences = splitter.split_sentences("Your Meitei text here ê¯«")
```

### Using Neural Network Mode

```python
from meitei_senter import MeiteiSentenceSplitter

# Enable neural mode for context-aware splitting
splitter = MeiteiSentenceSplitter(use_neural=True)
sentences = splitter.split_sentences(text)
```

### Direct Callable Interface

```python
from meitei_senter import MeiteiSentenceSplitter

splitter = MeiteiSentenceSplitter()

# Call splitter directly
sentences = splitter("ê¯†ê¯¦ê¯”ê¯£ê¯€ê¯¤ ê¯‘ê¯ê¯¤... ê¯« ê¯ƒê¯ê¯¤ ê¯Œê¯¥ê¯ê¯… ê¯†ê¯¥ê¯ê¯ ê¯«")
```

### With spaCy (Custom Tokenizer)

```python
import spacy
from meitei_senter import MeiteiTokenizer

# Create blank spaCy model with custom tokenizer
nlp = spacy.blank("xx")
nlp.tokenizer = MeiteiTokenizer("path/to/meitei_tokenizer.model", nlp.vocab)

doc = nlp("ê¯†ê¯¦ê¯”ê¯£ê¯€ê¯¤ ê¯‘ê¯ê¯¤ ê¯‘ê¯£ê¯€ê¯­ê¯‚ê¯¥ê¯ê¯£ê¯ƒê¯¥ê¯’ê¯¤ ê¯ê¯ê¯”ê¯…ê¯¤ ê¯«")
print([token.text for token in doc])
```

---

## ğŸ“Š Model Details

| Feature | Specification |
|---------|---------------|
| **Model Size** | ~1 MB |
| **Tokenizer** | SentencePiece (Unigram, 8K vocab) |
| **Architecture** | CNN (HashEmbedCNN) |
| **F-Score** | 94.71% |
| **Precision** | 93.94% |
| **Recall** | 95.49% |

---

## ğŸ“‚ Repository Structure

```
mni_tokenizer/
â”œâ”€â”€ meitei_senter/              # Main package
â”‚   â”œâ”€â”€ __init__.py             # Package exports
â”‚   â”œâ”€â”€ cli.py                  # Command-line interface
â”‚   â”œâ”€â”€ model.py                # PyTorch model & splitter
â”‚   â”œâ”€â”€ tokenizer.py            # spaCy tokenizer
â”‚   â”œâ”€â”€ meitei_tokenizer.model  # SentencePiece model
â”‚   â”œâ”€â”€ meitei_senter.pth       # PyTorch weights
â”‚   â””â”€â”€ meitei_senter.json      # Model config
â”œâ”€â”€ pyproject.toml              # Build configuration
â””â”€â”€ README.md                   # This file
```

---

## API Reference

### `MeiteiSentenceSplitter`

Main class for sentence splitting.

```python
MeiteiSentenceSplitter(
    pth_path: str = None,      # Path to PyTorch model
    spm_path: str = None,      # Path to SentencePiece model
    config_path: str = None,   # Path to config JSON
    use_neural: bool = False   # Enable neural network mode
)
```

**Methods:**
| Method | Description |
|--------|-------------|
| `split_sentences(text)` | Split text into list of sentences |
| `tokenize(text)` | Tokenize text into pieces and IDs |
| `__call__(text)` | Direct callable interface |

### `MeiteiTokenizer`

spaCy-compatible tokenizer using SentencePiece.

```python
MeiteiTokenizer(model_path: str, vocab: spacy.Vocab)
```

### `load_splitter`

Convenience function to load a pre-configured splitter.

```python
load_splitter(use_spacy: bool = False)
```

---

## ğŸ”§ Development

```bash
# Clone repository
git clone https://github.com/Okramjimmy/mni_tokenizer.git
cd mni_tokenizer

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest

# Build package
python -m build

# Upload to PyPI
twine upload dist/*
```

---

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ“š Citation

If you use this in your research, please cite:

```bibtex
@software{meitei_senter,
  author = {Okram Jimmy},
  title = {Meitei Senter: Sentence Boundary Detection for Meitei Mayek},
  year = {2024},
  url = {https://github.com/Okramjimmy/mni_tokenizer}
}
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.


## ğŸ“§ Contact

- **Author**: Okram Jimmy
- **Email**: okramjimmy@gmail.com
- **GitHub**: [@Okramjimmy](https://github.com/Okramjimmy)
