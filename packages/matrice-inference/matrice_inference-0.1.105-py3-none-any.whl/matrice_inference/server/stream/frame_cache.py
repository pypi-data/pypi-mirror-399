import logging
import queue
import threading
import time
from typing import Optional, Dict, Any

# TODO: MAKE THIS ASYNC

try:
    import redis  # type: ignore
except ImportError:  # pragma: no cover
    redis = None  # type: ignore


class RedisFrameCache:
    """Non-blocking Redis cache for frames with optimized resource management.

    Stores base64 string content under key 'stream:frames:{frame_id}' with field 'frame'.
    Each insert sets or refreshes the TTL.
    """

    DEFAULT_TTL_SECONDS = 300
    DEFAULT_MAX_QUEUE = 10000
    DEFAULT_WORKER_THREADS = 5  # Increased from 2 for 100K FPS throughput
    DEFAULT_CONNECT_TIMEOUT = 2.0
    DEFAULT_SOCKET_TIMEOUT = 0.5
    DEFAULT_HEALTH_CHECK_INTERVAL = 30
    DEFAULT_PREFIX = "stream:frames:"
    DEFAULT_MAX_CONNECTIONS = 200  # Increased from 10 for high concurrency (100K FPS)

    def __init__(
        self,
        host: str = "localhost",
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        username: Optional[str] = None,
        ttl_seconds: int = DEFAULT_TTL_SECONDS,
        prefix: str = DEFAULT_PREFIX,
        max_queue: int = DEFAULT_MAX_QUEUE,
        worker_threads: int = DEFAULT_WORKER_THREADS,
        connect_timeout: float = DEFAULT_CONNECT_TIMEOUT,
        socket_timeout: float = DEFAULT_SOCKET_TIMEOUT,
        max_connections: int = DEFAULT_MAX_CONNECTIONS,
        # ================================================================
        # SHM_MODE: Disable frame caching when SHM is active
        # ================================================================
        shm_mode: bool = False,  # When True, frame caching becomes no-op
    ) -> None:
        self.logger = logging.getLogger(f"{__name__}.frame_cache")

        # ================================================================
        # SHM_MODE: Skip initialization if SHM mode is active
        # Frames are stored in shared memory, not Redis
        # ================================================================
        self.shm_mode = shm_mode
        if shm_mode:
            self.logger.info(
                "[SHM_MODE] Frame cache DISABLED - frames stored in shared memory"
            )
            self.ttl_seconds = ttl_seconds
            self.prefix = prefix
            self.running = False
            self._worker_threads = 0
            self.queue = None
            self.threads = []
            self._client = None
            self._cached_frame_ids = set()
            self._cached_ids_lock = threading.Lock()
            self._metrics = {}
            self._metrics_lock = threading.Lock()
            return

        self.ttl_seconds = max(1, int(ttl_seconds))
        self.prefix = prefix
        self.running = False
        self._worker_threads = max(1, int(worker_threads))

        self.queue: queue.Queue = queue.Queue(maxsize=max_queue)
        self.threads: list = []
        self._client: Optional[redis.Redis] = None

        # Track cached frame_ids to prevent duplicates
        self._cached_frame_ids: set = set()
        self._cached_ids_lock = threading.Lock()

        # Metrics for monitoring and debugging
        self._metrics = {
            "frames_queued": 0,
            "frames_cached": 0,
            "frames_failed": 0,
            "frames_dropped": 0,
            "frames_skipped_duplicate": 0,
            "frames_skipped_empty": 0,
            "last_cache_time": None,
            "last_frame_id": None,
        }
        self._metrics_lock = threading.Lock()

        if not self._is_redis_available():
            return

        self._client = self._create_redis_client(
            host, port, db, password, username, connect_timeout, socket_timeout, max_connections
        )

    def _is_redis_available(self) -> bool:
        """Check if Redis package is available."""
        if redis is None:
            self.logger.warning("redis package not installed; frame caching disabled")
            return False
        return True

    def _create_redis_client(
        self,
        host: str,
        port: int,
        db: int,
        password: Optional[str],
        username: Optional[str],
        connect_timeout: float,
        socket_timeout: float,
        max_connections: int
    ) -> Optional[redis.Redis]:
        """Create Redis client with connection pooling for better concurrency.

        Args:
            host: Redis host
            port: Redis port
            db: Redis database number
            password: Redis password (optional)
            username: Redis username (optional)
            connect_timeout: Connection timeout in seconds
            socket_timeout: Socket timeout in seconds
            max_connections: Maximum connections in pool

        Returns:
            Redis client with connection pool or None if initialization fails
        """
        self.logger.info(
            f"[REDIS_CLIENT_INIT] Creating Redis client: host={host}:{port}, db={db}, "
            f"has_password={password is not None}, has_username={username is not None}"
        )
        
        try:
            # Create connection pool for better concurrency (20-30% throughput improvement)
            pool = redis.ConnectionPool(
                host=host,
                port=port,
                db=db,
                password=password,
                username=username,
                socket_connect_timeout=connect_timeout,
                socket_timeout=socket_timeout,
                max_connections=max_connections,
                health_check_interval=self.DEFAULT_HEALTH_CHECK_INTERVAL,
                retry_on_timeout=True,
                decode_responses=False,  # Keep binary data as bytes (no UTF-8 decoding)
            )

            client = redis.Redis(connection_pool=pool)
            
            # Test connection with PING
            try:
                client.ping()
                self.logger.info(
                    f"[REDIS_CLIENT_OK] Redis connection verified: host={host}:{port}, db={db}, "
                    f"max_connections={max_connections}"
                )
            except Exception as ping_error:
                self.logger.error(
                    f"[REDIS_CLIENT_PING_FAIL] Redis PING failed: host={host}:{port}, "
                    f"error={ping_error}"
                )
                # Still return client, it might work for actual operations
            
            return client
        except Exception as e:
            self.logger.error(
                f"[REDIS_CLIENT_FAIL] Failed to initialize Redis client: host={host}:{port}, "
                f"error={e}",
                exc_info=True
            )
            return None

    def start(self) -> None:
        """Start the frame cache with worker threads."""
        # ================================================================
        # SHM_MODE: No-op start - frame cache disabled
        # ================================================================
        if self.shm_mode:
            self.logger.info("[SHM_MODE] Frame cache start skipped - using shared memory")
            return

        if not self._client:
            self.logger.error(
                "[FRAME_CACHE_START_FAIL] Cannot start frame cache: Redis client not initialized"
            )
            return

        if self.running:
            self.logger.warning("[FRAME_CACHE_START] Frame cache already running")
            return

        self.running = True
        self._start_worker_threads()

        self.logger.info(
            f"[FRAME_CACHE_STARTED] RedisFrameCache ready: prefix={self.prefix}, ttl={self.ttl_seconds}s, "
            f"workers={self._worker_threads}, queue_size={self.queue.maxsize}, "
            f"running={self.running}, client_ready={self._client is not None}"
        )

    def _start_worker_threads(self) -> None:
        """Start worker threads for processing cache operations."""
        for i in range(self._worker_threads):
            thread = threading.Thread(
                target=self._worker,
                name=f"FrameCache-{i}",
                daemon=True
            )
            thread.start()
            self.threads.append(thread)

    def stop(self) -> None:
        """Stop the frame cache and cleanup resources."""
        if not self.running:
            return

        self.running = False
        self._stop_worker_threads()
        self.threads.clear()

    def _stop_worker_threads(self) -> None:
        """Stop all worker threads gracefully."""
        # Signal threads to stop
        for _ in self.threads:
            try:
                self.queue.put_nowait(None)
            except queue.Full:
                pass

        # Wait for threads to finish
        for thread in self.threads:
            try:
                thread.join(timeout=2.0)
            except Exception as e:
                self.logger.warning(f"Error joining thread {thread.name}: {e}")

    def put(self, frame_id: str, binary_content: bytes) -> None:
        """Enqueue a cache write for the given frame.

        Checks for duplicates and empty content before caching.

        Args:
            frame_id: unique identifier for the frame (must be unique across all apps)
            binary_content: raw image bytes (JPEG/PNG/etc - no base64 encoding needed)
        """
        # ================================================================
        # SHM_MODE: Skip caching - frames already in shared memory
        # ================================================================
        if self.shm_mode:
            return

        self.logger.debug(
            f"[PUT_FRAME_CALLED] frame_id={frame_id}, "
            f"content_size={len(binary_content) if binary_content else 0}"
        )

        if not self._is_cache_ready():
            self.logger.error(
                f"[PUT_FRAME_FAIL] Cache not ready: frame_id={frame_id}, "
                f"running={self.running}, client={self._client is not None}"
            )
            return

        # Check for empty content first (fast check)
        if not binary_content or len(binary_content) == 0:
            with self._metrics_lock:
                self._metrics["frames_skipped_empty"] += 1
            self.logger.debug(f"Skipping cache for frame_id={frame_id} - empty content")
            return

        if not self._validate_input(frame_id, binary_content):
            return

        # Check if this frame_id has already been cached (prevents duplicates)
        with self._cached_ids_lock:
            if frame_id in self._cached_frame_ids:
                with self._metrics_lock:
                    self._metrics["frames_skipped_duplicate"] += 1
                self.logger.debug(f"Skipping cache for frame_id={frame_id} - already cached")
                return
            # Mark as cached immediately to prevent race conditions
            self._cached_frame_ids.add(frame_id)

        try:
            # Build Redis key with prefix to avoid collisions
            key = f"{self.prefix}{frame_id}"
            content_len = len(binary_content)

            self.queue.put_nowait((key, binary_content, frame_id))

            # Update metrics
            with self._metrics_lock:
                self._metrics["frames_queued"] += 1
                self._metrics["last_frame_id"] = frame_id

            self.logger.debug(
                f"Queued frame for caching: frame_id={frame_id}, "
                f"redis_key={key}, content_size={content_len}, "
                f"queue_size={self.queue.qsize()}"
            )
        except queue.Full:
            # Remove from cached set if queueing failed
            with self._cached_ids_lock:
                self._cached_frame_ids.discard(frame_id)
            self._handle_queue_full(frame_id)

    def _is_cache_ready(self) -> bool:
        """Check if cache is ready for operations."""
        return bool(self._client and self.running)

    def _validate_input(self, frame_id: str, binary_content: bytes) -> bool:
        """Validate input parameters.

        CRITICAL: frame_id must come from upstream (streaming gateway).
        This validation ensures we don't cache frames with invalid IDs.
        """
        if not frame_id or not isinstance(frame_id, str) or not frame_id.strip():
            self.logger.error(
                f"[FRAME_ID_MISSING] Cannot cache frame - invalid frame_id: "
                f"{frame_id!r} (type: {type(frame_id).__name__})"
            )
            return False
        if not binary_content or not isinstance(binary_content, bytes):
            self.logger.warning(
                f"Invalid binary_content for frame_id={frame_id}: "
                f"type={type(binary_content).__name__}, "
                f"len={len(binary_content) if binary_content else 0}"
            )
            return False
        return True

    def _handle_queue_full(self, frame_id: str) -> None:
        """Handle queue full condition."""
        with self._metrics_lock:
            self._metrics["frames_dropped"] += 1
        self.logger.warning(
            f"Frame cache queue full (size={self.queue.maxsize}); "
            f"dropping frame_id={frame_id}. Consider increasing max_queue or worker_threads."
        )

    def _worker(self) -> None:
        """Worker thread for processing cache operations."""
        while self.running:
            item = self._get_work_item()
            if item is None:
                continue
            if self._is_stop_signal(item):
                break

            self._process_cache_item(item)

    def _get_work_item(self) -> Optional[tuple]:
        """Get work item from queue with timeout."""
        try:
            return self.queue.get(timeout=0.5)
        except queue.Empty:
            return None

    def _is_stop_signal(self, item: tuple) -> bool:
        """Check if item is a stop signal."""
        return item is None

    def _process_cache_item(self, item: tuple) -> None:
        """Process a single cache item."""
        frame_id = "unknown"
        try:
            key, base64_content, frame_id = item
            self._store_frame_data(key, base64_content, frame_id)
        except ValueError as e:
            # Handle old tuple format without frame_id for backwards compatibility
            try:
                key, base64_content = item
                frame_id = key.replace(self.prefix, "") if key.startswith(self.prefix) else key
                self._store_frame_data(key, base64_content, frame_id)
            except Exception as inner_e:
                self.logger.error(f"Failed to unpack cache item: {inner_e}")
                with self._metrics_lock:
                    self._metrics["frames_failed"] += 1
        except Exception as e:
            self.logger.error(f"Failed to process cache item for frame_id={frame_id}: {e}")
            with self._metrics_lock:
                self._metrics["frames_failed"] += 1
        finally:
            self._mark_task_done()

    def _store_frame_data(self, key: str, binary_content: bytes, frame_id: str) -> None:
        """Store frame data in Redis with TTL using pipeline for batching.

        Uses Redis pipeline to batch HSET + EXPIRE operations (40-50% faster).
        Multiple apps can safely write to different frame_ids without conflicts.
        Stores raw binary data (no base64 encoding) - Redis is binary-safe.
        """
        start_time = time.time()
        try:
            content_len = len(binary_content)
            self.logger.debug(
                f"Writing to Redis: frame_id={frame_id}, key={key}, "
                f"content_size={content_len} bytes (raw binary), ttl={self.ttl_seconds}s"
            )

            # Use Redis pipeline to batch HSET + EXPIRE operations
            # This reduces round-trips from 2 to 1 (40-50% latency reduction)
            # Store raw bytes directly - Redis is binary-safe (no base64 overhead)
            pipeline = self._client.pipeline()
            pipeline.hset(key, "frame", binary_content)
            pipeline.expire(key, self.ttl_seconds)
            pipeline.execute()

            elapsed = time.time() - start_time
            
            # Update metrics
            with self._metrics_lock:
                self._metrics["frames_cached"] += 1
                self._metrics["last_cache_time"] = time.time()
                self._metrics["last_frame_id"] = frame_id
            
            self.logger.info(
                f"Successfully cached frame: frame_id={frame_id}, key={key}, "
                f"content_size={content_len}, ttl={self.ttl_seconds}s, "
                f"elapsed={elapsed:.3f}s"
            )
        except redis.RedisError as e:
            with self._metrics_lock:
                self._metrics["frames_failed"] += 1
            self.logger.error(
                f"Redis error caching frame: frame_id={frame_id}, key={key}, "
                f"error={e.__class__.__name__}: {e}"
            )
        except Exception as e:
            with self._metrics_lock:
                self._metrics["frames_failed"] += 1
            self.logger.error(
                f"Unexpected error caching frame: frame_id={frame_id}, key={key}, "
                f"error={e}", exc_info=True
            )

    def _mark_task_done(self) -> None:
        """Mark queue task as done."""
        try:
            self.queue.task_done()
        except Exception:
            pass
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get cache performance metrics for monitoring and debugging.
        
        Returns:
            Dictionary containing cache metrics including:
            - frames_queued: Total frames queued for caching
            - frames_cached: Total frames successfully cached
            - frames_failed: Total frames that failed to cache
            - frames_dropped: Total frames dropped due to queue full
            - queue_size: Current queue size
            - last_cache_time: Timestamp of last successful cache
            - last_frame_id: Last frame_id cached
        """
        with self._metrics_lock:
            metrics = dict(self._metrics)
        
        metrics.update({
            "running": self.running,
            "queue_size": self.queue.qsize(),
            "queue_maxsize": self.queue.maxsize,
            "worker_threads": self._worker_threads,
            "prefix": self.prefix,
            "ttl_seconds": self.ttl_seconds,
        })
        
        return metrics

    def put_overlay(
        self,
        frame_id: str,
        camera_id: str,
        app_deployment_id: str,
        overlay_data: bytes,
        ttl_seconds: Optional[int] = None
    ) -> bool:
        """Store overlay/results data with composite key for multiple app support.

        Key format: overlay:{frame_id}_{camera_id}_{app_deployment_id}
        This allows multiple apps to store their results for the same frame independently.

        Args:
            frame_id: Unique frame identifier from upstream
            camera_id: Camera identifier
            app_deployment_id: App deployment identifier
            overlay_data: Serialized overlay/results data (JSON bytes)
            ttl_seconds: Optional TTL override (defaults to cache TTL)

        Returns:
            bool: True if successfully queued, False otherwise
        """
        self.logger.debug(
            f"[PUT_OVERLAY_CALLED] frame_id={frame_id}, camera_id={camera_id}, "
            f"app_deployment_id={app_deployment_id}, data_size={len(overlay_data) if overlay_data else 0}"
        )
        
        if not self._is_cache_ready():
            self.logger.error(
                f"[PUT_OVERLAY_FAIL] Cache not ready: running={self.running}, "
                f"client={self._client is not None}"
            )
            return False

        if not frame_id or not camera_id or not app_deployment_id:
            self.logger.error(
                f"[PUT_OVERLAY_FAIL] Invalid params: frame_id={frame_id}, "
                f"camera_id={camera_id}, app_deployment_id={app_deployment_id}"
            )
            return False

        if not overlay_data:
            self.logger.warning(
                f"[PUT_OVERLAY_FAIL] Empty overlay data: frame_id={frame_id}, camera_id={camera_id}"
            )
            return False

        # Build composite key: overlay:{frame_id}_{camera_id}_{app_deployment_id}
        composite_key = f"overlay:{frame_id}_{camera_id}_{app_deployment_id}"
        ttl = ttl_seconds if ttl_seconds is not None else self.ttl_seconds

        self.logger.debug(
            f"[PUT_OVERLAY_STORING] key={composite_key}, ttl={ttl}s, "
            f"data_size={len(overlay_data)} bytes"
        )

        try:
            # Store directly (not queued) since overlay data is typically smaller
            # and we want immediate availability for clients
            self._store_overlay_data(composite_key, overlay_data, ttl, frame_id, camera_id)
            self.logger.debug(f"[PUT_OVERLAY_SUCCESS] key={composite_key}")
            return True
        except Exception as e:
            self.logger.error(
                f"[PUT_OVERLAY_ERROR] key={composite_key}, error={e}"
            )
            return False

    def _store_overlay_data(
        self,
        key: str,
        overlay_data: bytes,
        ttl_seconds: int,
        frame_id: str,
        camera_id: str
    ) -> None:
        """Store overlay data in Redis with TTL.

        Uses Redis pipeline for atomic SET + EXPIRE.

        Args:
            key: Redis key (composite format)
            overlay_data: Serialized overlay data
            ttl_seconds: TTL for the key
            frame_id: Frame ID for logging
            camera_id: Camera ID for logging
        """
        start_time = time.time()
        try:
            content_len = len(overlay_data)
            self.logger.debug(
                f"[REDIS_OVERLAY_WRITE] Writing overlay to Redis: key={key}, "
                f"content_size={content_len} bytes, ttl={ttl_seconds}s"
            )

            # Use pipeline for atomic SET + EXPIRE
            pipeline = self._client.pipeline()
            pipeline.set(key, overlay_data)
            pipeline.expire(key, ttl_seconds)
            results = pipeline.execute()

            elapsed = time.time() - start_time
            self.logger.debug(
                f"[REDIS_OVERLAY_OK] Stored overlay: key={key}, "
                f"frame_id={frame_id}, camera_id={camera_id}, "
                f"content_size={content_len}, ttl={ttl_seconds}s, "
                f"elapsed={elapsed:.3f}s, pipeline_results={results}"
            )
        except Exception as e:
            self.logger.error(
                f"[REDIS_OVERLAY_ERROR] key={key}, "
                f"frame_id={frame_id}, camera_id={camera_id}, "
                f"error={e.__class__.__name__}: {e}",
                exc_info=True
            )
            raise  # Re-raise to signal failure to caller


