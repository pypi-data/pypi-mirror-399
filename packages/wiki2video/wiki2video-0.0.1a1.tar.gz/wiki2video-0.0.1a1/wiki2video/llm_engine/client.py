# llm_engine/client.py
from __future__ import annotations

from typing import List, Optional, Dict, Any

from .types import ChatMessage, ChatResult
from .errors import LLMConfigError
from .router import get_llm_provider_config
from .settings import (
    get_llm_backoff_base,
    get_llm_backoff_max_time,
    get_llm_backoff_max_tries,
    get_llm_timeout_seconds,
)
from jinja2 import Template
# ðŸ”¥ å¼•å…¥æ¨¡æ¿åŠ è½½å™¨ï¼ˆä½ é¡¹ç›®å·²æœ‰ï¼‰
from wiki2video.llm_engine.markdown_loader import MarkdownPromptLoader
from wiki2video.config.config_manager import config


class LLMEngine:
    """
    é¡¹ç›®ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯å°è£…ï¼š

    æ”¯æŒä¸¤ç±»è¾“å…¥ï¼š
    1) chat(messages) â€”â€” åŽŸå§‹ LLM æ¶ˆæ¯
    2) chat_with_template(template_path_or_key, variables) â€”â€” æ¨¡æ¿ + æ›¿æ¢

    å¹¶æä¾› ask_text() / ask_template() ç­‰ä¾¿æ·å°è£…ã€‚
    """

    def __init__(
        self,
        api_url: Optional[str] = None,
        api_key: Optional[str] = None,
        default_model: Optional[str] = None,
        ):
        provider_cfg = get_llm_provider_config()

        resolved_api_key = api_key or provider_cfg["api_key"]
        resolved_default_model = default_model or provider_cfg["default_model"]

        # Special handling for Google platform which uses project_id instead of api_key
        if provider_cfg["name"] == "google":
            project_id = config.get("google", "project_id")
            if not project_id:
                raise LLMConfigError(
                    f"Missing project_id for LLM platform 'google'. "
                    f"Set google.project_id in config.json."
                )
        elif not resolved_api_key:
            raise LLMConfigError(
                f"Missing API key for LLM platform '{provider_cfg['name']}'. "
                f"Set {provider_cfg['name']}.api_key in config.json or pass api_key explicitly."
            )

        self.default_model = resolved_default_model
        provider_cls = provider_cfg["provider_cls"]
        self.provider_name = provider_cfg["name"]
        self.provider = provider_cls(
            api_url=api_url,
            api_key=resolved_api_key,
            timeout_seconds=get_llm_timeout_seconds(),
            max_retries=get_llm_backoff_max_tries(),
            backoff_base=get_llm_backoff_base(),
            backoff_max_time=get_llm_backoff_max_time(),
        )

        self.prompt_loader = MarkdownPromptLoader()

    # -------------------------------------------------------------------------
    # ðŸ“Œ 1. åŽŸå§‹åŸºç¡€ LLM æŽ¥å£
    # -------------------------------------------------------------------------
    def chat(self, messages: List[ChatMessage], *, model: Optional[str] = None, **kw) -> ChatResult:
        """ä½Žå±‚ç»Ÿä¸€èŠå¤©æŽ¥å£ï¼Œæ‰€æœ‰ provider éƒ½å¿…é¡»å®žçŽ° provider.chatã€‚"""
        model_name = model or self.default_model
        if not model_name:
            raise LLMConfigError("No default LLM model configured.")
        return self.provider.chat(messages=messages, model=model_name, **kw)


    def chat_with_template(
            self,
            template_ref: str,
            variables: Dict[str, Any],
            *,
            model: Optional[str] = None,
            **kw,
    ) -> ChatResult:

        if "." not in template_ref:
            raise ValueError("Registry æ¨¡æ¿å¼•ç”¨å¿…é¡»æ˜¯ 'category.key'")
        category, key = template_ref.split(".", 1)
        template = self.prompt_loader.load_from_registry(category, key)

        # âœ” Jinja2 æ¸²æŸ“ â€”â€” ä¸ä¼šè¯¯è§£æž JSON æˆ– Markdown
        final_prompt = Template(template).render(**variables)

        return self.chat(
            messages=[{"role": "user", "content": final_prompt}],
            model=model or self.default_model,
            **kw,
        )

    def ask_text(self, prompt: str, **kw) -> str:
        res = self.chat([{"role": "user", "content": prompt}], **kw)
        return res["content"].strip()

    def ask_template(
        self,
        template_ref: str,
        variables: Dict[str, Any],
        *,
        model: Optional[str] = None,
        **kw,
    ) -> str:
        res = self.chat_with_template(
            template_ref=template_ref,
            variables=variables,
            model=model,
            **kw,
        )
        return res["content"].strip()


# å…¨å±€å•ä¾‹
_engine_singleton: Optional[LLMEngine] = None


def get_engine() -> LLMEngine:
    global _engine_singleton
    if _engine_singleton is None:
        _engine_singleton = LLMEngine()
    return _engine_singleton
