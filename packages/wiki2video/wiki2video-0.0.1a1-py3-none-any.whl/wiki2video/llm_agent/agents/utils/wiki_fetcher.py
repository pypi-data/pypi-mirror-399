import re
import mwclient
import mwparserfromhell
from urllib.parse import urlparse, unquote
from pathlib import Path
import requests
import time

from ....core.paths import get_project_dir
from ....llm_engine.client import get_engine
from ..utils.svg_converter import sanitize_filename

# å°è¯•å¯¼å…¥ cairosvgï¼Œå¦‚æœå¤±è´¥åˆ™è®¤ä¸ºä¸æ”¯æŒ SVG è½¬æ¢
SVG_SUPPORTED = False

# é»˜è®¤å ä½å‡½æ•°ï¼Œç›´æ¥è¿”å›åŸè·¯å¾„
def _ensure_non_svg_fallback(path: Path) -> Path:
    """å ä½å‡½æ•°ï¼Œå½“ CairoSVG ä¸å¯ç”¨æ—¶ä½¿ç”¨"""
    return path

ensure_non_svg = _ensure_non_svg_fallback

try:
    import cairosvg
    # éªŒè¯ cairosvg æ˜¯å¦çœŸçš„å¯ç”¨ï¼ˆæŸäº›ç³»ç»Ÿå¯èƒ½å¯¼å…¥æˆåŠŸä½†è¿è¡Œæ—¶å¤±è´¥ï¼‰
    _ = cairosvg.svg2png
    from ..utils.svg_converter import ensure_non_svg
    SVG_SUPPORTED = True
except (ImportError, OSError, AttributeError) as e:
    print(f"âš ï¸  CairoSVG not available: {e}")
    print("âš ï¸  SVG images will be skipped")
    # ensure_non_svg å·²ç»è®¾ç½®ä¸ºå ä½å‡½æ•°ï¼Œæ— éœ€ä¿®æ”¹


class WikiFetcherAndCleanerWorker:

    COMMONS = mwclient.Site("commons.wikimedia.org")
    ENWIKI = mwclient.Site("en.wikipedia.org")

    def __init__(self) -> None:
        self.engine = get_engine()

    # ---------------------------
    # normalize title
    # ---------------------------
    def normalize_title(self, user_input: str) -> str:
        print(f"\nğŸ”µ STEP: normalize_title('{user_input}')")
        if user_input.startswith(("http://", "https://")):
            parsed = urlparse(user_input)
            m = re.match(r"^/wiki/(.+)$", parsed.path)
            if not m:
                raise ValueError("Invalid Wikipedia URL")
            title = unquote(m.group(1)).replace("_", " ")
            print(f"ğŸŸ¡ Normalized title from URL = {title}")
            return title
        print(f"ğŸŸ¡ Normalized title = {user_input.strip()}")
        return user_input.strip()

    # ---------------------------
    def clean_raw_text(self, wikicode):
        text = wikicode.strip_code()
        lines = [l.strip() for l in text.split("\n") if l.strip()]
        return "\n".join(lines)

    # ---------------------------
    def extract_images(self, wikicode):
        imgs = []
        for node in wikicode.filter_wikilinks():
            title = str(node.title)
            if title.lower().startswith(("file:", "image:")):
                imgs.append({
                    "file_name": title.split(":", 1)[1],
                    "caption": str(node.text or "")
                })
        return imgs

    # ---------------------------
    def deep_clean_text(self, t: str) -> str:
        t = re.sub(r"<ref.*?>.*?</ref>", "", t, flags=re.DOTALL)
        t = re.sub(r"<ref[^>]*\s*/>", "", t)
        t = re.sub(r"\{\{.*?\}\}", "", t)
        t = re.sub(r"'{2,}", "", t)
        t = re.sub(r"\n{2,}", "\n", t)
        return t.strip()

    # ---------------------------
    def get_real_url(self, file_name):
        print(f"    ğŸ” Resolving image URL for: {file_name}")
        title = f"File:{file_name}"
        for site in (self.COMMONS, self.ENWIKI):
            try:
                data = site.api("query", prop="imageinfo", titles=title, iiprop="url")
                pages = data.get("query", {}).get("pages", {})
                for p in pages.values():
                    info = p.get("imageinfo")
                    if info:
                        url = info[0].get("url")
                        print(f"    ğŸŸ¡ Found URL = {url}")
                        return url
            except Exception as e:
                print(f"    ğŸ”´ Error fetching from site: {e}")
                continue
        print("    ğŸ”´ No URL resolved!")
        return None

    # ---------------------------
    def summarize_section(self, text: str):
        print(f"    ğŸ”µ STEP: summarize_section (len={len(text)} chars)")
        try:
            summary = self.engine.ask_template(
                template_ref="wiki_summary.section_summary",
                variables={"SECTION_TEXT": text},
                temperature=0.3,
                max_tokens=180,
            )
            print(f"    ğŸŸ¡ Summary done (len={len(summary)} chars)")
            return summary
        except Exception as e:
            print(f"    ğŸ”´ [ERROR] LLM generation failed: {e}")
            return ""

    # ---------------------------
    def download_image(self, url, out_path: Path):
        print(f"    ğŸ“¥ Downloading {url} -> {out_path}")
        out_path.parent.mkdir(parents=True, exist_ok=True)
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Referer": "https://en.wikipedia.org/"
        }
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
            with open(out_path, "wb") as f:
                f.write(r.content)
            print(f"    ğŸŸ¡ Image saved.")
            return True
        except Exception as e:
            print(f"    ğŸ”´ [Image] Download error: {e}")
            return False

    # ---------------------------
    # ä¸»æµç¨‹
    # ---------------------------
    def run(self, user_input: str, project_name: str):
        print("\n==============================")
        print("   ğŸš€ WikiFetcher RUN START")
        print("==============================\n")

        t0 = time.time()

        title = self.normalize_title(user_input)

        print(f"\nğŸ”µ STEP: Fetching page '{title}'")
        site = mwclient.Site("en.wikipedia.org")
        page = site.pages[title]

        if page.redirect:
            print("ğŸŸ¡ Page is redirect â†’ resolving...")
            page = page.resolve_redirect()

        print("ğŸ”µ STEP: Reading raw wiki text")
        raw = page.text()
        print(f"ğŸŸ  Raw text length: {len(raw)} chars")

        wikicode = mwparserfromhell.parse(raw)
        sections = wikicode.get_sections(include_lead=True, flat=True)
        print(f"ğŸŸ¡ Found {len(sections)} sections")

        structured = []
        all_images = []
        all_text = []

        img_dir =  get_project_dir(project_name) / "images"
        img_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸŸ¡ Image directory: {img_dir}")

        # ---------------------------
        # SECTION LOOP
        # ---------------------------
        for idx, sec in enumerate(sections):
            print(f"\n====================")
            print(f" ğŸ”µ SECTION {idx+1}/{len(sections)}")
            print("====================")

            heading_nodes = sec.filter_headings()
            heading = heading_nodes[0].title.strip() if heading_nodes else "Introduction"
            print(f"ğŸŸ¡ Heading = {heading}")

            raw_text = self.clean_raw_text(sec)
            cleaned = self.deep_clean_text(raw_text)
            wc = len(cleaned.split())
            print(f"ğŸŸ  Cleaned word_count = {wc}")

            summary = "" if wc <= 3000 else self.summarize_section(cleaned)

            imgs = self.extract_images(sec)
            print(f"ğŸŸ  Found {len(imgs)} wiki image refs")

            sec_imgs = []

            for img in imgs:
                file_name = sanitize_filename(img["file_name"])
                
                # æ£€æŸ¥æ˜¯å¦ä¸º SVG ä¸”ä¸æ”¯æŒè½¬æ¢
                if file_name.lower().endswith('.svg') and not SVG_SUPPORTED:
                    print(f"    âš ï¸  Skip SVG image (CairoSVG not available): {file_name}")
                    continue
                
                url = self.get_real_url(file_name)
                if not url:
                    print("ğŸ”´ Skip (no URL)")
                    continue

                local_path = img_dir / file_name
                self.download_image(url, local_path)

                final_path = local_path
                if SVG_SUPPORTED:
                    final_path = ensure_non_svg(local_path)

                obj = {
                    "file_name": final_path.name,
                    "caption": self.deep_clean_text(img["caption"]),
                    "url": url,
                    "local_path": str(final_path),
                    "section": heading
                }

                sec_imgs.append(obj)
                all_images.append(obj)

            structured.append({
                "heading": heading,
                "summary": summary if summary else cleaned,
                "word_count": wc,
                "images": sec_imgs,
            })

            all_text.append(cleaned)

        output = {
            "clean_text": "\n\n".join(all_text),
            "images": all_images,
            "sections": structured
        }

        print("\n==============================")
        print("   âœ… WikiFetcher RUN DONE")
        print("==============================")
        print(f"â± Total time = {time.time() - t0:.2f} sec")
        print(f"ğŸŸ  Total sections = {len(structured)}")
        print(f"ğŸŸ  Total images   = {len(all_images)}\n")

        return output
