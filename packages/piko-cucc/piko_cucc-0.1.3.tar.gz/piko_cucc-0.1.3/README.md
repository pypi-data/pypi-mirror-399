# Piko: Data-Oriented Async Task Orchestrator

[![PyPI version](https://img.shields.io/pypi/v/piko.svg)](https://pypi.org/project/piko-cucc/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Piko** æ˜¯ä¸€ä¸ªä¸“ä¸ºæ•°æ®å·¥ç¨‹è®¾è®¡çš„å¾®å†…æ ¸å¼‚æ­¥ä»»åŠ¡ç¼–æ’æ¡†æ¶ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªå®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼Œæ›´æ˜¯ä¸€ä¸ªåŸºäº `asyncio` çš„é«˜å¹¶å‘æµæ°´çº¿å¼•æ“ã€‚

ä¸ä¼ ç»Ÿè°ƒåº¦å™¨ä¸åŒï¼ŒPiko æ—¨åœ¨è§£å†³**é«˜å¹¶å‘ I/O** ä¸**å¤æ‚èµ„æºç®¡ç†**ä¹‹é—´çš„çŸ›ç›¾ï¼Œé€šè¿‡**å¾®å†…æ ¸è®¾è®¡**ä¸**ä¾èµ–æ³¨å…¥**æœºåˆ¶ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿè½»æ¾æ„å»ºæ”¯æ’‘æ•°ä¸‡ QPS çš„æ•°æ®æŠ“å–ã€æ¸…æ´—ä¸åŒæ­¥æœåŠ¡ã€‚

---

## å‰ç½®è¦æ±‚ (Prerequisites)

Piko ä¾èµ– **MySQL** (5.7 æˆ– 8.0+) ä½œä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå­˜å‚¨ä»»åŠ¡å…ƒæ•°æ®ã€çŠ¶æ€å›å¡«è¿›åº¦ä»¥åŠå®ç°åˆ†å¸ƒå¼é”ã€‚

åœ¨å¯åŠ¨ Piko ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æ‹¥æœ‰ä¸€ä¸ªå¯ç”¨çš„ MySQL å®ä¾‹ï¼Œå¹¶åˆ›å»ºå¥½æ•°æ®åº“ã€‚

```sql
# ç¤ºä¾‹ï¼šåˆ›å»ºæ•°æ®åº“
CREATE DATABASE piko_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
```

## æ ¸å¿ƒç‰¹æ€§ (Key Features)

- **å¼‚æ­¥å¾®å†…æ ¸ (Async Micro-kernel)**: åŸºäº `asyncio` + `uvloop` (å¯é€‰) æ„å»ºï¼ŒåŸç”Ÿæ”¯æŒåç¨‹ï¼Œå•èŠ‚ç‚¹å³å¯å¤„ç†æ•°ä¸‡å¹¶å‘ä»»åŠ¡ã€‚
- **èµ„æºä¾èµ–æ³¨å…¥ (Dependency Injection)**: å‘Šåˆ«å…¨å±€å˜é‡ä¸è¿æ¥æ³„éœ²ã€‚é€šè¿‡ `@job(resources=...)` å£°æ˜ä¾èµ–ï¼Œæ¡†æ¶è‡ªåŠ¨åœ¨å¹¶å‘ä»»åŠ¡é—´ç®¡ç†è¿æ¥æ± çš„å€Ÿç”¨ä¸å½’è¿˜ã€‚
- **æ™ºèƒ½å›å¡« (Stateful Backfill)**: ç³»ç»Ÿåœæœºæˆ–é€»è¾‘é”™è¯¯ï¼ŸPiko ä¼šè‡ªåŠ¨è®¡ç®—æ•°æ®çª—å£ï¼ˆData Intervalï¼‰ï¼Œç²¾å‡†è¡¥å½•æ¯ä¸€ä»½ä¸¢å¤±çš„æ•°æ®ã€‚
- **ç±»å‹å®‰å…¨ Sink (Typed Sinks)**: åŸºäº Python ç±»å‹ç³»ç»Ÿçš„è‡ªåŠ¨è·¯ç”±åˆ†å‘ï¼Œè®©å¼‚æ„æ•°æ®çš„å†™å…¥é€»è¾‘æ¸…æ™°å¯ç»´æŠ¤ã€‚

------

## ğŸ“¦ å®‰è£… (Installation)

```bash
pip install piko-cucc
```

------

## ğŸš€ æ¶æ„èŒƒå¼ (Architectural Patterns)

Piko çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå…¶å¯¹**å¼‚æ­¥**ä¸**å¹¶å‘**çš„åŸç”Ÿæ”¯æŒã€‚ä»¥ä¸‹ä¸‰ä¸ªä¾‹å­å±•ç¤ºäº† Piko åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ€ä½³å®è·µã€‚

### åœºæ™¯ä¸€ï¼šé«˜å¹¶å‘ç½‘ç»œ I/O (The "C10K" Crawler)

åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æé«˜é¢‘åœ°æŠ“å– APIã€‚Piko åˆ©ç”¨ `asyncio` çš„éé˜»å¡ç‰¹æ€§ï¼Œå¯ä»¥åœ¨å•ä¸ªè¿›ç¨‹å†…åŒæ—¶æŒ‚èµ·æ•°åƒä¸ªç½‘ç»œè¯·æ±‚ï¼Œæœ€å¤§åŒ– I/O ååé‡ã€‚

```python
import asyncio
import aiohttp
from piko.core.registry import job
from piko.core.runner import job_runner

# 1. å®šä¹‰ä¸€ä¸ªé«˜é¢‘ä»»åŠ¡
@job(
    job_id="fetch_stock_price",
    cron="* * * * *",       # æ¯åˆ†é’Ÿè§¦å‘
    misfire_grace_time=10   # å…è®¸ä¸€å®šçš„å»¶è¿Ÿ
)
async def fetch_handler(ctx, scheduled_time):
    """
    è¿™æ˜¯ä¸€ä¸ªçº¯å¼‚æ­¥çš„ Handlerã€‚
    Piko ä¸ä¼šå› ä¸º await è€Œé˜»å¡ï¼Œå®ƒä¼šç«‹å³åˆ‡æ¢å»æ‰§è¡Œå…¶ä»–ä»»åŠ¡ã€‚
    """
    symbol = ctx["config"].get("symbol", "AAPL")
    
    async with aiohttp.ClientSession() as session:
        async with session.get(f"[https://api.stocks.com/](https://api.stocks.com/){symbol}") as resp:
            data = await resp.json()
            print(f"[{symbol}] Price: {data['price']} at {scheduled_time}")

# 2. æ¨¡æ‹Ÿé«˜å¹¶å‘è§¦å‘
# åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒPiko Runner ä¼šè‡ªåŠ¨è°ƒåº¦ã€‚
# è¿™é‡Œæ¼”ç¤ºå¦‚ä½•æ‰‹åŠ¨è§¦å‘ 1000 ä¸ªå¹¶å‘ä»»åŠ¡ã€‚
async def main():
    # ç¬é—´ç”Ÿæˆ 1000 ä¸ªåç¨‹ä»»åŠ¡
    tasks = [
        job_runner.run_job("fetch_stock_price", config={"symbol": f"STK_{i}"}) 
        for i in range(1000)
    ]
    # Piko è½»æ¾å¤„ç†å¹¶å‘
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
```

### åœºæ™¯äºŒï¼šå—æ§å¹¶å‘ä¸èµ„æºæ± åŒ– (The "Shared Pool" Pattern)

å½“å¹¶å‘é‡å¾ˆå¤§æ—¶ï¼Œæ•°æ®åº“è¿æ¥å¾€å¾€æˆä¸ºç“¶é¢ˆã€‚Piko çš„ **èµ„æºæ³¨å…¥ (DI)** æœºåˆ¶èƒ½ç¡®ä¿æˆåƒä¸Šä¸‡ä¸ªä»»åŠ¡å…±äº«ä¸€ä¸ªæœ‰é™çš„è¿æ¥æ± ï¼Œæ—¢ä¿è¯äº†å¹¶å‘åº¦ï¼Œåˆé˜²æ­¢äº†æ•°æ®åº“è¢«æ‰“æŒ‚ã€‚

```python
from contextlib import asynccontextmanager
from piko.core.resource import Resource
from piko.core.registry import job

# 1. å®šä¹‰èµ„æºï¼šä¸€ä¸ªå¸¦æœ‰è¿æ¥æ± çš„æ•°æ®åº“å®¢æˆ·ç«¯
class DBPoolResource(Resource):
    def __init__(self):
        # å‡è®¾è¿™æ˜¯ä¸€ä¸ªè¿æ¥æ± ï¼Œæœ€å¤§è¿æ¥æ•° 50
        self.pool = MyAsyncDBPool(max_size=50)

    @asynccontextmanager
    async def acquire(self, ctx):
        # å½“ä»»åŠ¡æ‰§è¡Œæ—¶ï¼Œä»æ± ä¸­å€Ÿå‡ºä¸€ä¸ªè¿æ¥
        async with self.pool.acquire() as conn:
            yield conn
        # ä»»åŠ¡ç»“æŸï¼Œè¿æ¥è‡ªåŠ¨å½’è¿˜å›æ± ä¸­

# 2. æ³¨å†Œä»»åŠ¡ï¼šå£°æ˜æˆ‘éœ€è¦ "db" èµ„æº
@job(
    job_id="heavy_etl_task",
    cron="*/5 * * * *",
    resources={"db": DBPoolResource}  # <-- æ³¨å…¥å£°æ˜
)
async def etl_handler(ctx, scheduled_time, db):
    """
    dbå‚æ•°: ä¸æ˜¯æ•´ä¸ªè¿æ¥æ± ï¼Œè€Œæ˜¯ä¸€ä¸ªå·²ç» connected çš„è¿æ¥å¯¹è±¡ã€‚
    
    å³ä¾¿ Piko åŒæ—¶æ‹‰èµ·äº† 5000 ä¸ª etl_handlerï¼Œ
    ç”±äº DBPoolResource çš„é™åˆ¶ï¼Œå®ƒä»¬ä¼šæ’é˜Ÿå¤ç”¨é‚£ 50 ä¸ªæ•°æ®åº“è¿æ¥ï¼Œ
    å®ç°äº†"é«˜å¹¶å‘è°ƒåº¦"ä¸"æœ‰é™èµ„æºä¿æŠ¤"çš„å®Œç¾å¹³è¡¡ã€‚
    """
    await db.execute("INSERT INTO logs ...")
    print("Write success")
```

#### åœºæ™¯ä¸‰ï¼šCPU å¯†é›†å‹ä»»åŠ¡å¸è½½ (The "Multiprocessing" Pattern)

å½“ä¸šåŠ¡åŒ…å«å¤æ‚çš„æ•°å­¦è®¡ç®—ã€å›¾åƒå¤„ç†æˆ–è¶…å¤§æ–‡ä»¶è§£æï¼ˆä¾‹å¦‚è§£å‹ 1GB çš„ gzip æ–‡ä»¶ï¼‰æ—¶ï¼Œç›´æ¥åœ¨ Handler ä¸­è¿è¡Œä¼šé˜»å¡ Piko çš„äº‹ä»¶å¾ªç¯ï¼ˆEvent Loopï¼‰ï¼Œå¯¼è‡´å¿ƒè·³è¶…æ—¶ã€‚

æ ‡å‡†åšæ³•æ˜¯å°†è¿™äº›â€œé‡æ´»â€å¸è½½åˆ° **è¿›ç¨‹æ±  (Process Pool)** ä¸­ã€‚

```python
import asyncio
from concurrent.futures import ProcessPoolExecutor
from piko.core.resource import Resource
from piko.core.registry import job

# 1. å®šä¹‰çº¯å‡½æ•° (å¿…é¡»æ˜¯é¡¶å±‚å‡½æ•°ï¼Œä»¥ä¾¿ Pickle åºåˆ—åŒ–)
def heavy_calculation(data_chunk: bytes) -> int:
    """æ¨¡æ‹Ÿä¸€ä¸ªè€—æ—¶ 10 ç§’çš„ CPU å¯†é›†å‹è®¡ç®—"""
    # æ¯”å¦‚ï¼šå›¾åƒè½¬ç ã€åŠ è§£å¯†ã€å¤æ‚æ•°æ®æ¸…æ´—
    import time
    time.sleep(10) # æ¨¡æ‹Ÿ CPU æ»¡è½½
    return len(data_chunk)

# 2. å®šä¹‰èµ„æºï¼šè¿›ç¨‹æ± 
class CpuPoolResource(Resource):
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŒ…å« 4 ä¸ªå·¥äººçš„è¿›ç¨‹æ± 
        self.pool = ProcessPoolExecutor(max_workers=4)

    async def acquire(self, ctx):
        # å°†æ± å­æœ¬èº«äº¤ç»™ Handler
        yield self.pool
        # Piko é€€å‡ºæ—¶ä¸éœ€è¦æ‰‹åŠ¨ shutdownï¼ŒPython è§£é‡Šå™¨ä¼šå¤„ç†ï¼Œ
        # æˆ–è€…åœ¨è¿™é‡Œå®ç°æ›´ä¼˜é›…çš„å…³é—­é€»è¾‘

# 3. æ³¨å†Œä»»åŠ¡ï¼šæ³¨å…¥ CPU èµ„æº
@job(
    job_id="process_large_file",
    cron="0 0 * * *",
    resources={"cpu_pool": CpuPoolResource}
)
async def data_mining_handler(ctx, scheduled_time, cpu_pool):
    """
    æ³¨æ„ï¼šHandler æœ¬èº«ä¾ç„¶æ˜¯å¼‚æ­¥çš„ï¼Œä½†ä»–é€šè¿‡ run_in_executor å°†
    è®¡ç®—ä»»åŠ¡â€œæ‰”â€ç»™äº†å­è¿›ç¨‹ã€‚
    """
    loop = asyncio.get_running_loop()
    
    # æ¨¡æ‹Ÿè¯»å–æ•°æ®
    huge_data = b"0" * 1024 * 1024 * 100 

    print(f"[{scheduled_time}] Start calculation...")
    
    # å…³é”®ç‚¹ï¼šawait run_in_executor
    # 1. Piko ä¸»çº¿ç¨‹ç«‹å³é‡Šæ”¾æ§åˆ¶æƒï¼Œç»§ç»­å¤„ç†å¿ƒè·³å’Œå…¶ä»–çŸ­ä»»åŠ¡
    # 2. heavy_calculation åœ¨ç‹¬ç«‹çš„å­è¿›ç¨‹ä¸­è¿è¡Œï¼Œç‹¬å ä¸€ä¸ª CPU æ ¸å¿ƒ
    # 3. è®¡ç®—å®Œæˆåï¼Œç»“æœè‡ªåŠ¨ä¼ å›è¿™é‡Œ
    result = await loop.run_in_executor(
        cpu_pool, 
        heavy_calculation, 
        huge_data
    )
    
    print(f"Calculation done. Result: {result}")
```

é€šè¿‡è¿™ä¸‰ä¸ªåœºæ™¯ï¼ŒPiko è¦†ç›–äº†æ•°æ®å·¥ç¨‹çš„å®Œæ•´åœºæ™¯ï¼š

1. **I/O å¯†é›†** â†’ ç”¨ `asyncio` åŸç”Ÿå¹¶å‘ï¼ˆåœºæ™¯ä¸€ï¼‰ã€‚
2. **èµ„æºå—é™** â†’ ç”¨ `Resource` æ³¨å…¥å®ç°æ± åŒ–ç®¡ç†ï¼ˆåœºæ™¯äºŒï¼‰ã€‚
3. **CPU å¯†é›†** â†’ ç”¨ `run_in_executor` + æ³¨å…¥è¿›ç¨‹æ± å®ç°è®¡ç®—å¸è½½ï¼ˆåœºæ™¯ä¸‰ï¼‰ã€‚

------

## é…ç½® (Configuration)

Piko é‡‡ç”¨åˆ†å±‚é…ç½®ç­–ç•¥ï¼Œä¼˜å…ˆçº§é¡ºåºä¸ºï¼š**ç¯å¢ƒå˜é‡ > `settings.toml` é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼**ã€‚

### 1. é…ç½®æ–‡ä»¶ (`settings.toml`)

é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ `settings.toml` æ˜¯æ¨èçš„é…ç½®æ–¹å¼ã€‚

```toml
[piko]
log_level = "INFO"
log_json = false  # å¼€å‘æ¨¡å¼å¼€å¯å½©è‰²æ—¥å¿—

[mysql]
dsn = "mysql+aiomysql://user:pass@localhost/piko_db"
pool_size = 20
pool_recycle = 3600
```

### 2. ç¯å¢ƒå˜é‡ (Environment Variables)

æ¨èåœ¨ Docker/Kubernetes ä¸­ä½¿ç”¨ã€‚æ‰€æœ‰å˜é‡å¿…é¡»ä»¥ `PIKO_` å¼€å¤´ï¼Œå¹¶ä½¿ç”¨åŒä¸‹åˆ’çº¿ `__` åˆ†éš”å±‚çº§ã€‚

**ç¤ºä¾‹æ˜ å°„ï¼š**

| TOML é…ç½®          | å¯¹åº”çš„ç¯å¢ƒå˜é‡    | è¯´æ˜                    |
| ------------------ | ----------------- | ----------------------- |
| `[mysql] dsn`      | `PIKO_MYSQL__DSN` | **[å¿…é¡»]** MySQL è¿æ¥ä¸² |
| `[piko] log_level` | `PIKO_LOG_LEVEL`  | æ—¥å¿—çº§åˆ« (DEBUG/INFO)   |
| `[piko] log_json`  | `PIKO_LOG_JSON`   | ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸º `true` |
