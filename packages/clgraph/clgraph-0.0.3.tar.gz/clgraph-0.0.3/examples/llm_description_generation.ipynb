{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa22299",
   "metadata": {},
   "source": [
    "# Llm Description Generation\n",
    "\n",
    "**Example: LLM-Powered Description Generation**\n",
    "\n",
    "\n",
    "This example demonstrates how to use LangChain LLMs to automatically generate\n",
    "natural language descriptions for columns in a SQL pipeline.\n",
    "\n",
    "Requirements:\n",
    "- Install: uv pip install -e .\n",
    "- For Ollama: Install Ollama and run: ollama pull llama3.2\n",
    "- For OpenAI: Set OPENAI_API_KEY environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764faf6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2c8f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:10:34.992992Z",
     "iopub.status.busy": "2025-12-30T20:10:34.992706Z",
     "iopub.status.idle": "2025-12-30T20:10:43.205024Z",
     "shell.execute_reply": "2025-12-30T20:10:43.204611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üöÄ LLM Description Generation Examples\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Example 1: Using Ollama with qwen3-coder:30b (Local LLM)\n",
      "================================================================================\n",
      "\n",
      "üìä Parsing SQL pipeline...\n",
      "‚úÖ Found 15 columns\n",
      "\n",
      "üìù Setting source column descriptions...\n",
      "\n",
      "ü§ñ Configuring Ollama LLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama configured (model: qwen3-coder:30b)\n",
      "\n",
      "üîÆ Generating descriptions using LLM...\n",
      "(This may take 10-30 seconds depending on your machine)\n",
      "\n",
      "üìä Generating descriptions for 10 columns...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed 10/10 columns...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Generated 10 descriptions\n",
      "\n",
      "================================================================================\n",
      "Generated Descriptions\n",
      "================================================================================\n",
      "\n",
      "üìä analytics.user_metrics\n",
      "--------------------------------------------------------------------------------\n",
      "  avg_order_value      [ü§ñ LLM] Average order value per customer derived from raw orders data in USD.\n",
      "  last_order_date      [ü§ñ LLM] Last order date per day aggregated from staging.user_orders sourced from raw.orders table.\n",
      "  order_count          [ü§ñ LLM] Total number of orders per user aggregated from raw orders table.\n",
      "  total_revenue        [ü§ñ LLM] Total revenue per user aggregated from order amounts in USD.\n",
      "  user_id              [ü§ñ LLM] Unique user identifier per customer record from raw orders table, used for user-level analytics.\n",
      "\n",
      "üìä raw.orders\n",
      "--------------------------------------------------------------------------------\n",
      "  amount               [üë§ USER] Order amount in USD\n",
      "  order_date           [üë§ USER] Date when order was placed\n",
      "  order_id             [üë§ USER] Unique identifier for the order\n",
      "  status               [üë§ USER] Order status: pending, completed, cancelled\n",
      "  user_id              [üë§ USER] Unique identifier for the user\n",
      "\n",
      "üìä staging.user_orders\n",
      "--------------------------------------------------------------------------------\n",
      "  amount               [ü§ñ LLM] Order amount in USD per customer from raw.orders table.\n",
      "  order_date           [ü§ñ LLM] Order date when customers placed their purchases, sourced from raw.orders table per day aggregation.\n",
      "  order_id             [ü§ñ LLM] Unique order identifier per customer from the raw orders table.\n",
      "  status               [ü§ñ LLM] Order status indicator showing pending, completed, or cancelled states per raw.orders source.\n",
      "  user_id              [ü§ñ LLM] Unique user identifier from raw orders table, per customer record.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Example 3: Fallback Mode (No LLM)\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  No LLM configured - columns will show expression-based descriptions\n",
      "\n",
      "Column Information (without LLM):\n",
      "--------------------------------------------------------------------------------\n",
      "query_0_result.user_id                   Expression: user_id\n",
      "analytics.user_metrics.user_id           Expression: user_id\n",
      "query_0_result.total_revenue             Expression: total_revenue\n",
      "analytics.user_metrics.total_revenue     Expression: total_revenue\n",
      "query_0_result.order_count               Expression: order_count\n",
      "analytics.user_metrics.order_count       Expression: order_count\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Examples complete!\n",
      "\n",
      "üí° Tips:\n",
      "   - Ollama is free and runs locally (recommended for development)\n",
      "   - OpenAI provides higher quality descriptions but requires API key\n",
      "   - Fallback mode works without any LLM but produces simple descriptions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add src to path for direct execution\n",
    "\n",
    "from clgraph.multi_query import MultiQueryParser\n",
    "from clgraph.pipeline import PipelineLineageBuilder\n",
    "\n",
    "\n",
    "def example_with_ollama():\n",
    "    \"\"\"Example using local Ollama with qwen3-coder:30b (free, no API key needed)\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Example 1: Using Ollama with qwen3-coder:30b (Local LLM)\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Sample SQL pipeline\n",
    "    sql_queries = [\n",
    "        \"\"\"\n",
    "        CREATE OR REPLACE TABLE staging.user_orders AS\n",
    "        SELECT\n",
    "            user_id,\n",
    "            order_id,\n",
    "            order_date,\n",
    "            amount,\n",
    "            status\n",
    "        FROM raw.orders\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE OR REPLACE TABLE analytics.user_metrics AS\n",
    "        SELECT\n",
    "            user_id,\n",
    "            COUNT(*) as order_count,\n",
    "            SUM(amount) as total_revenue,\n",
    "            AVG(amount) as avg_order_value,\n",
    "            MAX(order_date) as last_order_date\n",
    "        FROM staging.user_orders\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY user_id\n",
    "        \"\"\",\n",
    "    ]\n",
    "\n",
    "    # Parse the pipeline\n",
    "    print(\"üìä Parsing SQL pipeline...\")\n",
    "    parser = MultiQueryParser()\n",
    "    table_graph = parser.parse_queries(sql_queries)\n",
    "\n",
    "    # Build lineage graph\n",
    "    builder = PipelineLineageBuilder()\n",
    "    lineage_graph = builder.build(table_graph)\n",
    "    print(f\"‚úÖ Found {len(lineage_graph.columns)} columns\")\n",
    "    print()\n",
    "\n",
    "    # IMPORTANT: Set source descriptions BEFORE generating descriptions\n",
    "    # This allows the LLM to use source column context when generating\n",
    "    print(\"üìù Setting source column descriptions...\")\n",
    "    for col in lineage_graph.columns.values():\n",
    "        if col.table_name == \"raw.orders\":\n",
    "            if col.column_name == \"user_id\":\n",
    "                col.set_source_description(\"Unique identifier for the user\")\n",
    "            elif col.column_name == \"order_id\":\n",
    "                col.set_source_description(\"Unique identifier for the order\")\n",
    "            elif col.column_name == \"amount\":\n",
    "                col.set_source_description(\"Order amount in USD\")\n",
    "            elif col.column_name == \"order_date\":\n",
    "                col.set_source_description(\"Date when order was placed\")\n",
    "            elif col.column_name == \"status\":\n",
    "                col.set_source_description(\"Order status: pending, completed, cancelled\")\n",
    "    print()\n",
    "\n",
    "    # Configure LLM (Ollama with qwen3-coder:30b)\n",
    "    print(\"ü§ñ Configuring Ollama LLM...\")\n",
    "    try:\n",
    "        from langchain_ollama import ChatOllama\n",
    "\n",
    "        llm = ChatOllama(\n",
    "            model=\"qwen3-coder:30b\",\n",
    "            temperature=0.3,  # Lower temperature for more consistent descriptions\n",
    "        )\n",
    "        lineage_graph.llm = llm\n",
    "        print(\"‚úÖ Ollama configured (model: qwen3-coder:30b)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure Ollama: {e}\")\n",
    "        print(\"üí° Make sure Ollama is installed and running:\")\n",
    "        print(\"   brew install ollama\")\n",
    "        print(\"   ollama pull qwen3-coder:30b\")\n",
    "        print(\"   ollama serve\")\n",
    "        return\n",
    "    print()\n",
    "\n",
    "    # Generate descriptions using LLM\n",
    "    print(\"üîÆ Generating descriptions using LLM...\")\n",
    "    print(\"(This may take 10-30 seconds depending on your machine)\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        lineage_graph.generate_all_descriptions(verbose=True)\n",
    "        print()\n",
    "\n",
    "        # Display results\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Generated Descriptions\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "\n",
    "        # Group by table\n",
    "        tables = {}\n",
    "        for col in lineage_graph.columns.values():\n",
    "            if col.table_name not in tables:\n",
    "                tables[col.table_name] = []\n",
    "            tables[col.table_name].append(col)\n",
    "\n",
    "        for table_name in sorted(tables.keys()):\n",
    "            print(f\"üìä {table_name}\")\n",
    "            print(\"-\" * 80)\n",
    "            for col in sorted(tables[table_name], key=lambda c: c.column_name):\n",
    "                source_marker = (\n",
    "                    \"üë§ USER\"\n",
    "                    if col.description_source and col.description_source.value == \"source\"\n",
    "                    else \"ü§ñ LLM\"\n",
    "                )\n",
    "                print(f\"  {col.column_name:20} [{source_marker}] {col.description}\")\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to generate descriptions: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def example_with_openai():\n",
    "    \"\"\"Example using OpenAI GPT-4 (requires API key)\"\"\"\n",
    "    import os\n",
    "\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ö†Ô∏è  Skipping OpenAI example - OPENAI_API_KEY not set\")\n",
    "        return\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Example 2: Using OpenAI GPT-4\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Sample SQL\n",
    "    sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE user_engagement AS\n",
    "    SELECT\n",
    "        user_id,\n",
    "        DATE_TRUNC('week', activity_date) as week,\n",
    "        COUNT(DISTINCT session_id) as weekly_sessions,\n",
    "        SUM(page_views) as total_page_views,\n",
    "        AVG(session_duration_minutes) as avg_session_duration\n",
    "    FROM user_activity\n",
    "    WHERE activity_date >= CURRENT_DATE - INTERVAL '90 days'\n",
    "    GROUP BY user_id, DATE_TRUNC('week', activity_date)\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse\n",
    "    parser = MultiQueryParser()\n",
    "    table_graph = parser.parse_queries([sql])\n",
    "    builder = PipelineLineageBuilder()\n",
    "    lineage_graph = builder.build(table_graph)\n",
    "\n",
    "    # Configure OpenAI\n",
    "    print(\"ü§ñ Configuring OpenAI GPT-4...\")\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.3)\n",
    "    lineage_graph.llm = llm\n",
    "    print(\"‚úÖ OpenAI configured\")\n",
    "    print()\n",
    "\n",
    "    # Generate descriptions\n",
    "    print(\"üîÆ Generating descriptions using GPT-4...\")\n",
    "    lineage_graph.generate_all_descriptions(verbose=True)\n",
    "    print()\n",
    "\n",
    "    # Display results\n",
    "    print(\"Generated Descriptions:\")\n",
    "    print(\"-\" * 80)\n",
    "    for col in lineage_graph.columns.values():\n",
    "        if col.description:\n",
    "            print(f\"{col.full_name:40} {col.description}\")\n",
    "\n",
    "\n",
    "def example_fallback():\n",
    "    \"\"\"Example showing fallback when no LLM is available\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Example 3: Fallback Mode (No LLM)\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT\n",
    "        user_id,\n",
    "        total_revenue,\n",
    "        order_count\n",
    "    FROM analytics.user_metrics\n",
    "    \"\"\"\n",
    "\n",
    "    parser = MultiQueryParser()\n",
    "    table_graph = parser.parse_queries([sql])\n",
    "    builder = PipelineLineageBuilder()\n",
    "    lineage_graph = builder.build(table_graph)\n",
    "\n",
    "    # Don't set an LLM - columns will use fallback generation\n",
    "    print(\"‚ö†Ô∏è  No LLM configured - columns will show expression-based descriptions\")\n",
    "    print()\n",
    "\n",
    "    print(\"Column Information (without LLM):\")\n",
    "    print(\"-\" * 80)\n",
    "    for col in lineage_graph.columns.values():\n",
    "        # Show column expression instead of trying to generate description\n",
    "        expr = col.expression if col.expression else \"N/A\"\n",
    "        print(f\"{col.full_name:40} Expression: {expr}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\")\n",
    "    print(\"üöÄ LLM Description Generation Examples\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Run examples\n",
    "    example_with_ollama()\n",
    "    print(\"\\n\" * 2)\n",
    "\n",
    "    # Uncomment to run OpenAI example if you have an API key\n",
    "    # example_with_openai()\n",
    "    # print(\"\\n\" * 2)\n",
    "\n",
    "    example_fallback()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"‚úÖ Examples complete!\")\n",
    "    print()\n",
    "    print(\"üí° Tips:\")\n",
    "    print(\"   - Ollama is free and runs locally (recommended for development)\")\n",
    "    print(\"   - OpenAI provides higher quality descriptions but requires API key\")\n",
    "    print(\"   - Fallback mode works without any LLM but produces simple descriptions\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
