{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Pipeline Execution Example\n",
    "\n",
    "**Pipeline Execution Example**\n",
    "\n",
    "This example demonstrates the three execution modes for SQL pipelines:\n",
    "1. Synchronous native execution with pipeline.run()\n",
    "2. Asynchronous native execution with pipeline.async_run()\n",
    "3. Airflow DAG generation with pipeline.to_airflow_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-md",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:10:47.955591Z",
     "iopub.status.busy": "2025-12-30T20:10:47.955305Z",
     "iopub.status.idle": "2025-12-30T20:10:48.017472Z",
     "shell.execute_reply": "2025-12-30T20:10:48.016988Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from clgraph.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-md",
   "metadata": {},
   "source": [
    "### Example 1: Synchronous Native Execution\n",
    "\n",
    "Demonstrate synchronous pipeline execution using `pipeline.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "example1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:10:48.018802Z",
     "iopub.status.busy": "2025-12-30T20:10:48.018703Z",
     "iopub.status.idle": "2025-12-30T20:10:48.243413Z",
     "shell.execute_reply": "2025-12-30T20:10:48.242547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 1: Synchronous Native Execution (pipeline.run())\n",
      "======================================================================\n",
      "üöÄ Starting pipeline execution (3 queries)\n",
      "\n",
      "üìä Level 1: 1 queries\n",
      "      [Executing] staging.orders\n",
      "  ‚úÖ staging_orders\n",
      "\n",
      "üìä Level 2: 2 queries\n",
      "      [Executing] analytics.daily_revenue\n",
      "      [Executing] analytics.customer_metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ daily_revenue\n",
      "  ‚úÖ customer_metrics\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline completed in 0.21s\n",
      "   Successful: 3\n",
      "   Failed: 0\n",
      "============================================================\n",
      "\n",
      "Result Summary:\n",
      "  Completed: 3 queries\n",
      "  Failed: 0 queries\n",
      "  Total time: 0.21s\n"
     ]
    }
   ],
   "source": [
    "# Define a simple pipeline\n",
    "queries = [\n",
    "    (\n",
    "        \"staging_orders\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE staging.orders AS\n",
    "        SELECT\n",
    "            order_id,\n",
    "            customer_id,\n",
    "            order_date,\n",
    "            amount\n",
    "        FROM raw.orders\n",
    "        WHERE order_date >= CURRENT_DATE() - 7\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"daily_revenue\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE analytics.daily_revenue AS\n",
    "        SELECT\n",
    "            DATE(order_date) as date,\n",
    "            COUNT(*) as order_count,\n",
    "            SUM(amount) as total_revenue\n",
    "        FROM staging.orders\n",
    "        GROUP BY DATE(order_date)\n",
    "    \"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"customer_metrics\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE analytics.customer_metrics AS\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            COUNT(*) as order_count,\n",
    "            SUM(amount) as total_spend\n",
    "        FROM staging.orders\n",
    "        GROUP BY customer_id\n",
    "    \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(queries, dialect=\"bigquery\")\n",
    "\n",
    "\n",
    "# Mock executor for demo (in real use, this would execute SQL on your database)\n",
    "def execute_sql(sql: str):\n",
    "    \"\"\"Mock executor - simulates SQL execution\"\"\"\n",
    "    if \"CREATE TABLE\" in sql:\n",
    "        table_start = sql.find(\"CREATE TABLE\") + 13\n",
    "        table_end = sql.find(\"AS\", table_start)\n",
    "        table_name = sql[table_start:table_end].strip()\n",
    "        print(f\"      [Executing] {table_name}\")\n",
    "        time.sleep(0.1)  # Simulate query execution time\n",
    "\n",
    "\n",
    "# Run pipeline synchronously\n",
    "print(\"EXAMPLE 1: Synchronous Native Execution (pipeline.run())\")\n",
    "print(\"=\" * 70)\n",
    "result = pipeline.run(executor=execute_sql, max_workers=4, verbose=True)\n",
    "\n",
    "print(\"\\nResult Summary:\")\n",
    "print(f\"  Completed: {len(result['completed'])} queries\")\n",
    "print(f\"  Failed: {len(result['failed'])} queries\")\n",
    "print(f\"  Total time: {result['elapsed_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-md",
   "metadata": {},
   "source": [
    "### Example 2: Asynchronous Native Execution\n",
    "\n",
    "Demonstrate asynchronous pipeline execution using `pipeline.async_run()`.\n",
    "\n",
    "**Note:** In Jupyter notebooks, we use `await` directly instead of `asyncio.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "example2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:10:48.245816Z",
     "iopub.status.busy": "2025-12-30T20:10:48.245599Z",
     "iopub.status.idle": "2025-12-30T20:10:48.463681Z",
     "shell.execute_reply": "2025-12-30T20:10:48.460464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 2: Asynchronous Native Execution (pipeline.async_run())\n",
      "======================================================================\n",
      "üöÄ Starting async pipeline execution (3 queries)\n",
      "\n",
      "üìä Level 1: 1 queries\n",
      "      [Async Executing] staging.orders\n",
      "  ‚úÖ staging_orders\n",
      "\n",
      "üìä Level 2: 2 queries\n",
      "      [Async Executing] analytics.daily_revenue\n",
      "      [Async Executing] analytics.customer_metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ daily_revenue\n",
      "  ‚úÖ customer_metrics\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline completed in 0.20s\n",
      "   Successful: 3\n",
      "   Failed: 0\n",
      "============================================================\n",
      "\n",
      "Async Result Summary:\n",
      "  Completed: 3 queries\n",
      "  Failed: 0 queries\n",
      "  Total time: 0.20s\n"
     ]
    }
   ],
   "source": [
    "# Define queries for async execution\n",
    "queries_async = [\n",
    "    (\n",
    "        \"staging_orders\",\n",
    "        \"CREATE TABLE staging.orders AS SELECT * FROM raw.orders\",\n",
    "    ),\n",
    "    (\n",
    "        \"daily_revenue\",\n",
    "        \"CREATE TABLE analytics.daily_revenue AS SELECT DATE(order_date) as date, SUM(amount) FROM staging.orders GROUP BY 1\",\n",
    "    ),\n",
    "    (\n",
    "        \"customer_metrics\",\n",
    "        \"CREATE TABLE analytics.customer_metrics AS SELECT customer_id, COUNT(*) FROM staging.orders GROUP BY 1\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipeline_async = Pipeline(queries_async, dialect=\"bigquery\")\n",
    "\n",
    "\n",
    "# Async executor\n",
    "async def async_execute_sql(sql: str):\n",
    "    \"\"\"Async mock executor\"\"\"\n",
    "    if \"CREATE TABLE\" in sql:\n",
    "        table_start = sql.find(\"CREATE TABLE\") + 13\n",
    "        table_end = sql.find(\"AS\", table_start)\n",
    "        table_name = sql[table_start:table_end].strip()\n",
    "        print(f\"      [Async Executing] {table_name}\")\n",
    "        await asyncio.sleep(0.1)  # Simulate async query execution\n",
    "\n",
    "\n",
    "print(\"EXAMPLE 2: Asynchronous Native Execution (pipeline.async_run())\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# In Jupyter, we use await directly\n",
    "result_async = await pipeline_async.async_run(\n",
    "    executor=async_execute_sql, max_workers=4, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nAsync Result Summary:\")\n",
    "print(f\"  Completed: {len(result_async['completed'])} queries\")\n",
    "print(f\"  Failed: {len(result_async['failed'])} queries\")\n",
    "print(f\"  Total time: {result_async['elapsed_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3-md",
   "metadata": {},
   "source": [
    "### Example 3: Airflow DAG Generation\n",
    "\n",
    "Demonstrate Airflow DAG generation using `pipeline.to_airflow_dag()`.\n",
    "\n",
    "**Note:** This requires Apache Airflow to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "example3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:10:48.467028Z",
     "iopub.status.busy": "2025-12-30T20:10:48.466736Z",
     "iopub.status.idle": "2025-12-30T20:10:48.478225Z",
     "shell.execute_reply": "2025-12-30T20:10:48.477455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 3: Airflow DAG Generation (pipeline.to_airflow_dag())\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  Airflow not installed\n",
      "   Install with: pip install apache-airflow\n",
      "   This is optional - you can still use run() and async_run()\n"
     ]
    }
   ],
   "source": [
    "# Define queries for Airflow DAG\n",
    "queries_airflow = [\n",
    "    (\n",
    "        \"staging_orders\",\n",
    "        \"CREATE TABLE staging.orders AS SELECT * FROM raw.orders\",\n",
    "    ),\n",
    "    (\n",
    "        \"daily_revenue\",\n",
    "        \"CREATE TABLE analytics.daily_revenue AS SELECT DATE(order_date) as date, SUM(amount) FROM staging.orders GROUP BY 1\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipeline_airflow = Pipeline(queries_airflow, dialect=\"bigquery\")\n",
    "\n",
    "\n",
    "# Executor function (would execute SQL in production)\n",
    "def airflow_execute_sql(sql: str):\n",
    "    \"\"\"Executor for Airflow tasks\"\"\"\n",
    "    print(f\"Would execute: {sql[:50]}...\")\n",
    "\n",
    "\n",
    "print(\"EXAMPLE 3: Airflow DAG Generation (pipeline.to_airflow_dag())\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate Airflow DAG with advanced parameters\n",
    "try:\n",
    "    dag = pipeline_airflow.to_airflow_dag(\n",
    "        executor=airflow_execute_sql,\n",
    "        dag_id=\"revenue_pipeline\",\n",
    "        schedule=\"@daily\",\n",
    "        start_date=datetime(2024, 1, 1),\n",
    "        description=\"Daily revenue analytics pipeline\",\n",
    "        catchup=False,\n",
    "        max_active_runs=3,\n",
    "        max_active_tasks=10,\n",
    "        tags=[\"analytics\", \"revenue\", \"daily\"],\n",
    "        default_view=\"graph\",\n",
    "        orientation=\"LR\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Airflow DAG generated successfully!\")\n",
    "    print(f\"   DAG ID: {dag.dag_id}\")\n",
    "    print(f\"   Description: {dag.description}\")\n",
    "    print(\"   Schedule: @daily\")\n",
    "    print(f\"   Max Active Runs: {dag.max_active_runs}\")\n",
    "    print(f\"   Tags: {dag.tags}\")\n",
    "    print(f\"   Tasks: {len(dag.task_dict)} tasks\")\n",
    "    print(f\"   Task IDs: {list(dag.task_dict.keys())}\")\n",
    "    print(\"\\nTo use this DAG:\")\n",
    "    print(\"1. Save this to your Airflow dags/ folder\")\n",
    "    print(\"2. Airflow will automatically detect and schedule it\")\n",
    "    print(\"3. View in Airflow UI: http://localhost:8080/dags/revenue_pipeline\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Airflow not installed\")\n",
    "    print(\"   Install with: pip install apache-airflow\")\n",
    "    print(\"   This is optional - you can still use run() and async_run()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This example demonstrated three ways to execute SQL pipelines:\n",
    "\n",
    "1. **Synchronous** (`pipeline.run()`): Simple, blocking execution\n",
    "2. **Asynchronous** (`pipeline.async_run()`): Non-blocking, concurrent execution\n",
    "3. **Airflow DAG** (`pipeline.to_airflow_dag()`): Generate production-ready DAGs\n",
    "\n",
    "All three methods use the same pipeline definition - write once, run anywhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zx88msbrgim",
   "metadata": {},
   "source": "### Visualize Pipeline Lineage\n\nDisplay the simplified column lineage for the execution pipelines."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lb17gui5ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from clgraph import visualize_pipeline_lineage\n",
    "\n",
    "if shutil.which(\"dot\") is None:\n",
    "    print(\"‚ö†Ô∏è  Graphviz not installed. Install with: brew install graphviz\")\n",
    "else:\n",
    "    print(\"Sync Pipeline - Simplified Lineage:\")\n",
    "    display(visualize_pipeline_lineage(pipeline.column_graph.to_simplified()))\n",
    "\n",
    "    print(\"\\nAsync Pipeline - Simplified Lineage:\")\n",
    "    display(visualize_pipeline_lineage(pipeline_async.column_graph.to_simplified()))\n",
    "\n",
    "    print(\"\\nAirflow Pipeline - Simplified Lineage:\")\n",
    "    display(visualize_pipeline_lineage(pipeline_airflow.column_graph.to_simplified()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}