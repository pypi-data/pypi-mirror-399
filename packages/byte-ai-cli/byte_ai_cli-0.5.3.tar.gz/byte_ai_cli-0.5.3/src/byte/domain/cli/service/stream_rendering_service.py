import re

from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage
from rich.live import Live

from byte.core import Service
from byte.core.utils import extract_content_from_message
from byte.domain.cli import ConsoleService, MarkdownStream, RuneSpinner


class StreamRenderingService(Service):
    """Service for rendering streaming AI responses with rich formatting and visual feedback.

    Manages the display of streaming content from AI agents, including markdown rendering,
    tool execution visualization, and spinner management. Provides a unified interface
    for handling different message types and streaming modes from LangGraph agents.
    Usage: `await stream_service.handle_message(chunk, "CoderAgent")` -> renders streaming content
    """

    async def boot(self) -> None:
        """Initialize the stream rendering service with console and state management.

        Sets up the Rich console, initializes streaming state variables, and prepares
        the markdown stream renderer for handling AI agent responses.
        Usage: Called automatically during service container boot process
        """
        self.console = await self.make(ConsoleService)

        self.current_stream_id = None
        self.accumulated_content = ""
        self.agent_name = ""
        self.display_mode = "verbose"
        self.spinner = None
        self.active_stream = MarkdownStream(
            console=self.console.console,
            mdargs={"code_theme": self._config.cli.syntax_theme},
        )

    async def _start_tool_message(self):
        """Display tool execution start indicator with visual separation.

        Shows a visual rule separator to indicate transition from AI response
        to tool execution phase, providing clear visual feedback to users.
        Usage: Called automatically when AI response indicates tool usage
        """
        # Tool messages might need different visual treatment

        self.console.print()
        self.console.rule("Using Tool")

        # We reset the stream_id here to make it look like a new stream
        self.current_stream_id = ""

    async def _update_active_stream(self, final: bool = False):
        """Update the active markdown stream renderer with accumulated content.

        Passes accumulated content to the markdown stream for progressive
        rendering, with optional final flag to indicate stream completion.
        Usage: Called internally during content accumulation and finalization
        """
        if self.active_stream and self.display_mode == "verbose":
            await self.active_stream.update(self.accumulated_content, final=final)

    async def _handle_ai_message(self, message_chunk, metadata):
        """Handle AI assistant message chunks with content accumulation and stream management.

        Processes streaming text content from AI responses, manages stream lifecycle
        based on thread IDs, and handles stop conditions for tool usage transitions.
        Usage: Called internally when processing AIMessageChunk instances
        """

        # Check if we need to start a new stream
        if metadata.get("thread_id") and metadata["thread_id"] != self.current_stream_id:
            await self.start_stream_render(metadata["thread_id"])

        # Append message content to accumulated content
        if message_chunk.content:
            self.accumulated_content += extract_content_from_message(message_chunk)
            if self.display_mode == "verbose":
                await self._update_active_stream()

        if hasattr(message_chunk, "response_metadata"):
            # Check for stream ending conditions
            if (
                message_chunk.response_metadata.get("stop_reason") == "end_turn"
                or message_chunk.response_metadata.get("stop_reason") == "tool_use"
            ):
                await self.end_stream_render()

                if message_chunk.response_metadata.get("stop_reason") == "tool_use":
                    await self._start_tool_message()

    async def _end_tool_message(self, message_chunk, metadata):
        """Handle completion of tool execution with spinner restart.

        Processes tool execution results and restarts the thinking spinner
        to indicate AI is processing the tool output for next response.
        Usage: Called automatically when ToolMessage chunks are received
        """
        # PLaceholder for maybe displaying the result of the tool message?
        self.console.print()
        # Restart the spinner since usually the AI will think after this.
        await self.start_spinner()

    async def _handle_default_message(self, message_chunk, metadata):
        """Handle non-AI, non-Tool message types with basic content processing.

        Provides fallback handling for message types not explicitly supported,
        extracting and accumulating content using basic text extraction methods.
        Usage: Called automatically for unrecognized message chunk types
        """
        # Fallback for other message types - basic content handling
        if message_chunk.content:
            content = extract_content_from_message(message_chunk)
            if content:
                self.accumulated_content += content
                if self.display_mode == "verbose":
                    await self._update_active_stream()

    async def handle_message(self, chunk, agent_name: str):
        """Handle streaming message chunks from AI agents with type-specific processing.

        Routes different message types (AI, Tool, etc.) to appropriate handlers for
        proper display formatting and user experience. Manages stream lifecycle
        and content accumulation for smooth rendering.
        Usage: `await service.handle_message((message, metadata), "CoderAgent")` -> processes chunk
        """

        message_chunk, metadata = chunk

        # Set the Agent "class" as the Agent Name
        self.agent_name = agent_name

        # Handle different message types with isinstance checks
        if isinstance(message_chunk, AIMessageChunk):
            # Handle AI assistant responses - normal streaming content
            await self._handle_ai_message(message_chunk, metadata)

        elif isinstance(message_chunk, ToolMessage):
            # Handle tool execution results - might want different formatting
            await self._end_tool_message(message_chunk, metadata)

        else:
            # Handle other message types with default behavior
            await self._handle_default_message(message_chunk, metadata)

    async def handle_update(self, chunk):
        """Handle state update chunks from LangGraph execution with node transition management.

        Processes graph node transitions, managing stream lifecycle when moving
        between assistant and tool nodes. Handles stream pausing and resumption
        for smooth visual transitions during complex agent workflows.
        Usage: Called automatically when processing graph state updates
        """

        if isinstance(chunk, dict):
            for key in chunk.keys():
                # If we see a 'tools' key, we're transitioning to tool execution
                if key == "tools":
                    # End the current stream since tools don't stream content
                    await self.end_stream_render()
                    # Store the previous stream ID so we can resume later
                    self._previous_stream_id = self.current_stream_id

                # If we're transitioning back to assistant after tools
                elif key == "assistant" and hasattr(self, "_previous_stream_id"):
                    # Resume streaming with the previous stream ID
                    await self.start_stream_render(self._previous_stream_id)
                    # Clear the stored previous ID
                    delattr(self, "_previous_stream_id")

                # For any other new node transition
                elif key != self.current_stream_id:
                    await self.end_stream_render()

    async def end_stream_render(self):
        """End the current stream rendering and reset accumulated content."""
        if self.display_mode == "verbose":
            await self._update_active_stream(final=True)
        self.accumulated_content = ""
        self.active_stream = None
        self.current_stream_id = None
        await self.stop_spinner()

    def _format_agent_name(self, agent_name: str) -> str:
        """Convert agent class name to readable headline case.

        Usage: "CoderAgent" -> "Coder Agent"
        """
        # Insert space before capital letters (except the first one)
        spaced = re.sub(r"(?<!^)(?=[A-Z])", " ", agent_name)
        return spaced

    async def start_stream_render(self, stream_id):
        """Initialize a new streaming render session with agent identification.

        Sets up a new markdown stream renderer, displays agent header with
        formatted name, and prepares for content accumulation. Cleans up
        any existing stream before starting the new one.
        Usage: `await service.start_stream_render("thread_123")` -> starts new stream
        """
        if self.active_stream and self.display_mode == "verbose":
            await self._update_active_stream(final=True)

        await self.stop_spinner()

        self.current_stream_id = stream_id
        self.accumulated_content = ""  # Reset accumulated content for new stream
        if self.display_mode == "verbose":
            self.active_stream = MarkdownStream(
                console=self.console.console,
                mdargs={
                    "code_theme": self._config.cli.syntax_theme,
                    "inline_code_lexer": "text",
                },
            )  # Reset the stream renderer

            formatted_name = self._format_agent_name(self.agent_name)
            self.console.rule(f"[primary]{formatted_name}[/primary]", style="primary")

    async def end_stream(self):
        """Complete the streaming session and clean up all rendering state.

        Stops any active spinner, finalizes the current stream render, and
        resets all streaming state for the next interaction cycle.
        Usage: Called automatically at the end of agent execution
        """
        await self.stop_spinner()
        await self.end_stream_render()

    async def stop_spinner(self):
        """Stop the active thinking spinner if one is running.

        Safely stops the Rich Live spinner display, handling cleanup
        and preventing display artifacts during stream transitions.
        Usage: Called automatically during stream lifecycle management
        """
        if self.spinner:
            self.spinner.stop()

    async def start_spinner(self):
        """Start a thinking spinner to indicate AI processing activity.

        Creates and starts a Rich Live spinner with "Thinking..." text to
        provide visual feedback during AI processing or tool execution phases.
        Usage: Called automatically when AI needs to process or think
        """
        if self.display_mode in ["verbose", "thinking"]:
            # Start with animated spinner
            spinner = RuneSpinner(text="Thinking...", size=15)
            self.spinner = Live(spinner, console=self.console.console, transient=True, refresh_per_second=20)
            self.spinner.start()

    def set_display_mode(self, mode: str) -> None:
        """Set the display mode for stream rendering.

        Args:
                mode: Display mode - must be "verbose", "thinking", or "silent"

        Raises:
                ValueError: If mode is not one of the valid options

        Usage: `stream_service.set_display_mode("thinking")` -> sets spinner-only mode
        """
        valid_modes = ["verbose", "thinking", "silent"]
        if mode not in valid_modes:
            raise ValueError(f"Invalid display mode '{mode}'. Must be one of: {valid_modes}")
        self.display_mode = mode
