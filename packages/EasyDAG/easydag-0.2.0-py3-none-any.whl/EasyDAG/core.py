import multiprocessing as mp
import pickle
import time
import traceback
from collections import defaultdict, deque
from contextlib import contextmanager
from multiprocessing import Pool, Manager
from multiprocessing.pool import Pool as PoolType
from pathlib import Path
from queue import Queue
from typing import Any, Callable, Dict, List, Optional

from .queue import MultiprocessQueue
from .node import DAGNode, _node_worker
from .dag_types import DAGQueue, NodeJobResult, NodeJob, NodeError
from .interface import DagInterface, EasyInterface


class EasyDAG(DagInterface):
    """A lightweight DAG executor using multiprocessing.Pool.

    Usage:
        dag = DistributedDAG(processes=4)
        dag.add_node(DAGNode('A', func_a))
        dag.add_node(DAGNode('B', func_b))
        dag.add_edge('A', 'B')  # B depends on A
        outputs = dag.run()  # dict of node_id -> output

    Node functions receive all dependency outputs packed in a dict under the keyword 'inputs'.
    For example, if B depends on A and C, then 'inputs' passed to B will be {'A': <outA>, 'C': <outC>}.
    """

    def __init__(self, processes: int = None, fail_fast: bool = True, mp_queue: MultiprocessQueue | None = None):
        """
        processes: number of worker processes (default: CPU count - 1)
        fail_fast: if True, stop scheduling new nodes when any node fails (default: True)
        """
        self.nodes: Dict[str, DAGNode] = {}
        self.adj: Dict[str, List[str]] = defaultdict(list)
        self.rev_adj: Dict[str, List[str]] = defaultdict(list)
        self.processes = processes or max(1, mp.cpu_count() - 1)
        self.fail_fast = fail_fast
        self._message_queue = mp_queue
        self.dag_id = None

    def add_node(self, node: DAGNode) -> None:
        if node.id in self.nodes:
            raise ValueError(f"Node with id '{node.id}' already exists")
        self.nodes[node.id] = node

    def add_edge(self, from_id: str, to_id: str) -> None:
        if from_id not in self.nodes:
            raise KeyError(f"Unknown from-node '{from_id}'")
        if to_id not in self.nodes:
            raise KeyError(f"Unknown to-node '{to_id}'")

        # Check for self-loops
        if from_id == to_id:
            raise ValueError(f"Self-loop detected: {from_id} -> {to_id}")

        # Check for duplicate edges
        if to_id in self.adj[from_id]:
            raise ValueError(f"Duplicate edge: {from_id} -> {to_id}")

        self.adj[from_id].append(to_id)
        self.rev_adj[to_id].append(from_id)

    @contextmanager
    def _create_pool(self):
        """Context manager for proper pool resource management."""
        pool = Pool(processes=self.processes)
        try:
            yield pool
        finally:
            pool.terminate()
            pool.join()

    @staticmethod
    def _get_cached(node_id: str, cache_path: Optional[Path]) -> Optional[Any]:
        """Retrieve cached result for a node if available."""
        if cache_path is None:
            return None
        cache_file = cache_path / f"{node_id}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"DAG Cache corrupted, rebuilding. {repr(e)}''")
                # If cache is corrupted, ignore and recompute
                return None
        return None

    @staticmethod
    def _save_to_cache(node_id: str, result: Any, cache_path: Optional[Path]) -> None:
        """Save node result to cache."""
        if cache_path is None:
            return
        try:
            cache_file = cache_path / f"{node_id}.pkl"
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)  # type: ignore[arg-type]
        except Exception as e:
            # Non-critical if caching fails
            print(f"DAG Cache failed: {repr(e)}''")

    def run(
            self,
            dag_timeout: Optional[float] = None,
            cache_dir: Optional[str] = None,
            progress_callback: Optional[Callable[[int, int, str], None]] = None,
            interface: Optional[EasyInterface] = None
    ) -> Dict[str, Any]:
        """Execute the DAG and return a dict mapping node_id -> output.

        timeout: maximum seconds to wait for the whole DAG to complete. None means wait forever.
        cache_dir: if provided, cache node outputs to disk and reuse on subsequent runs.
        progress_callback: optional callable(completed, total, node_id) called after each node completes.

        Note: Node functions can send messages to the main thread by using the 'message_queue'
              parameter that is automatically provided to them. Use register_message_handler()
              to set up handlers for different message types before calling run().
        """
        if not self.nodes:
            return {}

        # Validate DAG
        self._topological_check()

        # Setup caching
        cache_path = None
        if cache_dir:
            cache_path = Path(cache_dir)
            cache_path.mkdir(parents=True, exist_ok=True)

        manager = Manager()
        mp_queue = None
        if self._message_queue:
            mp_queue = DAGQueue(manager.Queue())
            self._message_queue.register_queue(mp_queue)  # register queue to wrapper
        outputs = manager.dict()  # shared dict: node_id -> result
        errors = manager.dict()
        times = manager.dict()
        stop_state = manager.dict()

        # Setup message queue for child processes to communicate with main thread
        if self._message_queue:
            self._message_queue.start_message_listener()

        # Local state in the parent process
        indeg: Dict[str, int] = {nid: 0 for nid in self.nodes}
        for u, outs in self.adj.items():
            for v in outs:
                indeg[v] += 1

        # Use thread-safe queue for ready nodes
        ready: Queue = Queue()
        for n in [n for n, d in indeg.items() if d == 0]:
            ready.put(n)

        # Track progress and retries

        total_nodes = len(self.nodes)
        completed = [0]  # Use list for closure mutability
        retry_counts = defaultdict(int)

        # For callback bookkeeping
        pending = set()  # node ids currently running


        # Helper to submit a node
        def submit(node_id: str, p: PoolType):
            times[node_id] = time.time()
            # Check cache first
            cached_result = self._get_cached(node_id, cache_path)
            if cached_result is not None:
                outputs[node_id] = cached_result
                on_done(NodeJobResult(node_id, cached_result))
                return

            node = self.nodes[node_id]
            # Build inputs dict from dependencies' outputs
            dep_ids = self.rev_adj.get(node_id, [])
            resolved_inputs = {dep.lower(): outputs[dep] for dep in dep_ids}

            payload = NodeJob(node_id, node.func, node.args, node.kwargs, resolved_inputs, mp_queue)
            pending.add(node_id)

            try:
                # IMPORTANT: provide error_callback so exceptions from worker get routed
                p.apply_async(_node_worker, args=(payload,), callback=on_done,
                              error_callback=lambda ecb, nid=node_id: _handle_async_error(ecb, nid))
            except Exception as e:
                # This catches immediate scheduling errors (e.g. pickling error when serializing payload)
                times[node_id] = time.time() - times[node_id]
                pending.discard(node_id)
                # record error info
                tb = traceback.format_exc()
                errors[node_id] = NodeError(tb, str(resolved_inputs)[:500], f"Schedule error: {str(e)}")
                if self.fail_fast:
                    stop_state["Schedule Error"] = node_id
                if interface:
                    node_md = {
                        "time": times[node_id],
                        "traceback": tb
                    }
                    interface.node_errored(node_id, f"Schedule Error: {e}", node_md)

        def _handle_async_error(e, node_id):
            # print(f"Async error for node {node_id}: {exc}")
            # Remove from pending and record error similar to on_done's behavior
            times[node_id] = time.time() - times[node_id]
            pending.discard(node_id)
            tb = "".join(traceback.format_exception_only(type(e), e))
            errors[node_id] = NodeError(tb, "<unknown - worker infrastructure error>", str(e))
            if self.fail_fast:
                stop_state["Infrastructure Error"] = node_id
            if interface:
                node_md = {
                    "time": times[node_id],
                    "traceback": tb
                }
                interface.node_errored(node_id, f"Infrastructure Error: {e}", node_md)

        # Callback executed in parent process when a worker finishes
        def on_done(job_result: NodeJobResult):
            node_id = job_result.node_id
            times[node_id] = time.time() - times[node_id]
            pending.discard(node_id)

            if job_result.error_info is not None:
                node = self.nodes[node_id]
                # Check if we should retry
                if retry_counts[node_id] < node.max_retries:
                    retry_counts[node_id] += 1
                    # Don't mark as completed, resubmit
                    ready.put(node_id)
                    return

                # No more retries, record error
                errors[node_id] = job_result.error_info
                if self.fail_fast:
                    # Signal to stop submitting new work
                    stop_state["Node Runtime Error"] = node_id
                if interface:
                    node_md = {
                        "time": times[node_id],
                        "traceback": job_result.error_info.traceback,
                        "inputs": str(job_result.error_info.inputs),
                    }
                    interface.node_errored(node_id, f"Node error: {job_result.error_info.exception}", node_md)
            else:
                result = job_result.result
                outputs[node_id] = result
                # Save to cache
                self._save_to_cache(node_id, result, cache_path)

                # Update progress
                completed[0] += 1
                if progress_callback:
                    progress_callback(completed[0], total_nodes, node_id)

                # Decrease in-degree of successors and enqueue any that reach 0
                for successor in self.adj.get(node_id, []):
                    indeg[successor] -= 1
                    if indeg[successor] == 0:
                        ready.put(successor)

                # Update interface
                if interface:
                    node_md = {
                        "time": times[node_id],
                    }
                    interface.node_finished(node_id, result, node_md)

        if interface:
            interface.dag_started(self.dag_id)
        # Main execution loop
        with self._create_pool() as pool:
            try:
                dag_start = time.time()

                # Submit initial ready nodes
                while not ready.empty():
                    nid = ready.get()
                    if interface:
                        interface.node_started(nid)
                    submit(nid, pool)

                # Main loop: as nodes finish, new nodes become ready via callback
                while pending or not ready.empty():
                    # Check for interface cancellation
                    if interface and interface.cancel_dag_flag:
                        stop_state["Cancelled"] = None
                        break

                    # Check for fail-fast condition
                    if self.fail_fast and len(stop_state) > 0:
                        break

                    # Submit newly ready nodes
                    while not ready.empty():
                        nid = ready.get()
                        if interface:
                            interface.node_started(nid)
                        submit(nid, pool)

                    if dag_timeout is not None and (time.time() - dag_start) > dag_timeout:
                        raise TimeoutError(f"Dag_ID: {self.dag_id} timed out.")

                    # Small sleep to yield control and allow callbacks to run
                    time.sleep(0.1)

                # --- STOP HANDLING ---
                if len(stop_state) > 0:
                    pool.terminate()
                else:
                    pool.close()
                pool.join()
            except Exception:
                pool.terminate()
                pool.join()
                raise

            finally:
                # Stop the message listener
                if self._message_queue:
                    self._message_queue.stop_message_listener()
                # Send finished to interface
                dag_time_elapsed = time.time() - dag_start
                if interface:
                    dag_md = {
                        "outputs": dict(outputs),
                        "errors": dict(errors),
                        "time": dag_time_elapsed,
                        "num_nodes": total_nodes,
                        "num_success": len(outputs),
                        "num_fail": len(errors),
                        "num_skipped": total_nodes - len(outputs) - len(errors),
                        "stop_state": dict(stop_state),
                    }
                    interface.dag_finished(self.dag_id, len(errors) == 0, metadata=dag_md)

        # If any errors, raise an aggregated exception with tracebacks
        error_messages = []
        if len(errors) > 0:
            for k in errors.keys():
                error_info: NodeError = errors[k]
                msg = (
                    f"Node {k} failed:\n"
                    f"  Exception: {error_info.exception}\n"
                    f"  Inputs: {error_info.inputs}\n"
                    f"  Traceback:\n{error_info.traceback}"
                )
                error_messages.append(msg)

            combined = "\n\n".join(error_messages)
            raise RuntimeError(f"One or more nodes failed:\n{combined}")

        if len(stop_state) > 0:
            for reason in stop_state.keys():
                n: str = stop_state[reason]
                msg = (
                    f"From node: {n}\n"
                    f"With reason: {reason}"
                )
                error_messages.append(msg)
            combined = "\n\n".join(error_messages)
            raise RuntimeError(f"Stop state triggered:\n{combined}")

        # Return outputs from manager.dict
        return dict(outputs)

    def _topological_check(self) -> None:
        # Check acyclic using Kahn's algorithm (without modifying internal state)
        indeg = {nid: 0 for nid in self.nodes}
        for u, outs in self.adj.items():
            for v in outs:
                indeg[v] += 1
        q = deque([n for n, d in indeg.items() if d == 0])
        seen = 0
        while q:
            u = q.popleft()
            seen += 1
            for v in self.adj.get(u, []):
                indeg[v] -= 1
                if indeg[v] == 0:
                    q.append(v)
        if seen != len(self.nodes):
            raise ValueError("Graph contains cycles or unreachable nodes!")

    def to_graphviz(self) -> str:
        """
        Generate DOT format for visualization.

        Can be rendered with graphviz:
            graphviz.Source(dag.to_graphviz()).render('dag', view=True)
        """
        lines = ["digraph DAG {", "  rankdir=LR;", "  node [shape=box, style=rounded];"]

        for node_id in self.nodes:
            lines.append(f'  "{node_id}";')

        for from_id, to_ids in self.adj.items():
            for to_id in to_ids:
                lines.append(f'  "{from_id}" -> "{to_id}";')

        lines.append("}")
        return "\n".join(lines)

    def load_dag_from_spec(self, spec_list: List[Dict], function_lookup: Dict[str, Callable]):
        """
        spec_list: list of dicts with keys:
            - id: node identifier
            - func: string name of function
            - args: optional list of positional arguments
            - kwargs: optional dict of keyword arguments
            - parents: list of parent node ids (dependencies)
            - max_retries: optional number of retries on failure

        function_lookup: dict mapping string -> actual function
                         e.g. {"task_a": task_a}
        """

        # Unpack and add all nodes
        for spec in spec_list:
            node = DAGNode(
                spec["id"],
                function_lookup[spec["func"]],
                args=tuple(spec.get("args", [])),
                kwargs=spec.get("kwargs", {}),
                max_retries=spec.get("max_retries", 0)
            )
            self.add_node(node)

        # Add edges
        for spec in spec_list:
            node_id = spec["id"]
            for parent in spec.get("parents", []):
                self.add_edge(parent, node_id)
