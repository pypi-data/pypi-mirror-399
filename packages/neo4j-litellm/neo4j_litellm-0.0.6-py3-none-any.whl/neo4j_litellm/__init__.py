from typing import Optional, List
from litellm import completion, acompletion
from neo4j_graphrag.llm import LLMInterface, LLMResponse
from neo4j_litellm.data_module import ChatHistory
import logging

'''
ChatHistory: 
role: str: system/assistant/user
content: str: system prompt or user prompt or text generated by llm
'''
logger = logging.getLogger("LiteLLMInterface")
# LLM interface of litellm
class LiteLLMInterface(LLMInterface):
    def __init__(self, provider:str, model_name:str, base_url:str, api_key:str, timeout:int = 5):
        '''
        :param provider: LLM provider
        :param model: Name of the llm
        :param base_url: URL of the litellm provider
        :param api_key: API key
        '''
        super().__init__(model_name)
        self.provider = provider
        self.base_url = base_url
        self.api_key = api_key
        self.timeout = timeout
    # LLM invoke
    def invoke(
        self,
        input: str,
        message_history: Optional[List[ChatHistory]] = None,
        system_instruction: Optional[str] = None
    ) -> LLMResponse:
        logging.info(f"get message{input}")
        # messages
        message = []
        # add chat message if message_history exist
        if not(message_history is None):
            logger.info("Adding chat history")
            message.extend(message_history)
        # check whether system_instruction exist
        if not (system_instruction is None):
            logger.info("Adding system prompt")
            message.append({
                "role": "system",
                "content": system_instruction
            })
        # user message
        message.append({
            "role": "user",
            "content": input
        })
        # LLM request
        logger.info("Requesting the LLM")
        response = completion(model=f"{self.provider}/{self.model_name}", messages=message, api_key=self.api_key,
                              api_base=self.base_url, timeout=self.timeout)
        result = response.choices[0].message.content
        return LLMResponse(
            content=result
        )
    # Async invoke
    async def ainvoke(
        self,
        input: str,
        message_history: Optional[List[ChatHistory]] = None,
        system_instruction: Optional[str] = None
    ) -> LLMResponse:
        '''
        Same as above
        '''
        logging.info(f"get message{input}")
        # messages
        message = []
        # add chat message if message_history exist
        if not (message_history is None):
            logger.info("Adding chat history")
            message.extend(message_history)
        # check whether system_instruction exist
        if not (system_instruction is None):
            logger.info("Adding system prompt")
            message.append({
                "role": "system",
                "content": system_instruction
            })
        # user message
        message.append({
            "role": "user",
            "content": input
        })
        # LLM request
        logger.info("Requesting the LLM")
        response = await acompletion(model=f"{self.provider}/{self.model_name}", messages=message, api_key=self.api_key,
                              api_base=self.base_url, timeout=self.timeout)
        result = response.choices[0].message.content
        return LLMResponse(
            content=result
        )
__all__ = ["LiteLLMInterface", "ChatHistory"]