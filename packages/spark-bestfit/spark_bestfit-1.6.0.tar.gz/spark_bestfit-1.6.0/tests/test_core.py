"""Tests for core distribution fitting module."""

from unittest.mock import patch

import numpy as np
import pytest

from spark_bestfit import (
    DEFAULT_EXCLUDED_DISCRETE_DISTRIBUTIONS,
    DEFAULT_EXCLUDED_DISTRIBUTIONS,
    DiscreteDistributionFitter,
    DistributionFitter,
)
from spark_bestfit.distributions import DistributionRegistry
from spark_bestfit.results import FitResults

class TestDistributionFitter:
    """Tests for DistributionFitter class."""

    @pytest.mark.parametrize("excluded,seed,expected_excluded,expected_seed", [
        (None, 42, DEFAULT_EXCLUDED_DISTRIBUTIONS, 42),  # defaults
        (("norm", "expon"), 123, ("norm", "expon"), 123),  # custom
        ((), 42, (), 42),  # empty exclusions
    ])
    def test_initialization(self, spark_session, excluded, seed, expected_excluded, expected_seed):
        """Test fitter initialization with various configurations."""
        fitter = DistributionFitter(spark_session, excluded_distributions=excluded, random_seed=seed)

        assert fitter.spark is spark_session
        assert fitter.excluded_distributions == expected_excluded
        assert fitter.random_seed == expected_seed

    def test_fit_basic(self, spark_session, small_dataset):
        """Test basic fitting operation returns valid results."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)

        # Should return results
        assert results.count() == 5  # Requested 5 distributions

        # Should find best distribution with valid data
        best = results.best(n=1)[0]
        assert isinstance(best.distribution, str) and len(best.distribution) > 0
        assert best.sse < np.inf
        assert len(best.parameters) >= 2  # At least loc and scale
        assert isinstance(best.ks_statistic, float) and 0 <= best.ks_statistic <= 1
        assert isinstance(best.pvalue, float) and 0 <= best.pvalue <= 1

    def test_fit_identifies_correct_distribution(self, spark_session, normal_data):
        """Test that fitter can fit normal data well with appropriate distributions."""
        df = spark_session.createDataFrame([(float(x),) for x in normal_data], ["value"])

        fitter = DistributionFitter(spark_session)
        # Fit a smaller set to ensure norm is included (distributions sorted alphabetically)
        results = fitter.fit(df, column="value", max_distributions=5)

        # Best distribution should have good fit for normal data
        best = results.best(n=1)[0]
        assert best.sse < 0.1, f"Best fit SSE too high: {best.sse}"
        assert best.ks_statistic < 0.1, f"Best fit KS too high: {best.ks_statistic}"

    def test_fit_with_custom_bins(self, spark_session, small_dataset):
        """Test fitting with custom number of bins."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", bins=25, max_distributions=5)

        # Should fit all 5 requested distributions
        assert results.count() == 5

    def test_fit_support_at_zero(self, spark_session, positive_dataset):
        """Test fitting only non-negative distributions."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(positive_dataset, column="value", support_at_zero=True, max_distributions=5)

        # Should fit all 5 requested non-negative distributions
        assert results.count() == 5

        # All distributions should be non-negative
        registry = DistributionRegistry()
        df_pandas = results.df.toPandas()
        for dist_name in df_pandas["distribution"]:
            assert registry._has_support_at_zero(dist_name) is True

    def test_fit_with_sampling(self, spark_session, medium_dataset):
        """Test fitting with sampling enabled."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(
            medium_dataset,
            column="value",
            enable_sampling=True,
            sample_fraction=0.5,
            sample_threshold=50_000,
            max_distributions=5,
        )

        # Should fit all 5 requested distributions
        assert results.count() == 5

    @pytest.mark.parametrize("enable,threshold,desc", [
        (False, 10_000_000, "sampling disabled"),
        (True, 100_000, "below threshold"),
    ])
    def test_apply_sampling_returns_original(self, spark_session, small_dataset, enable, threshold, desc):
        """Test that original DataFrame is returned when sampling doesn't apply."""
        fitter = DistributionFitter(spark_session)
        df_sampled = fitter._apply_sampling(
            small_dataset, row_count=10_000, enable_sampling=enable,
            sample_fraction=None, max_sample_size=1_000_000, sample_threshold=threshold
        )
        assert df_sampled.count() == small_dataset.count()

    def test_apply_sampling_with_fraction(self, spark_session, medium_dataset):
        """Test sampling with specified fraction."""
        fitter = DistributionFitter(spark_session)
        df_sampled = fitter._apply_sampling(
            medium_dataset, row_count=100_000, enable_sampling=True,
            sample_fraction=0.5, max_sample_size=1_000_000, sample_threshold=50_000
        )

        # Should sample ~50% of data
        sampled_count = df_sampled.count()
        assert 45_000 < sampled_count < 55_000  # Allow some variance

    def test_apply_sampling_auto_fraction(self, spark_session, medium_dataset):
        """Test sampling with auto-determined fraction."""
        fitter = DistributionFitter(spark_session)
        df_sampled = fitter._apply_sampling(
            medium_dataset, row_count=100_000, enable_sampling=True,
            sample_fraction=None, max_sample_size=50_000, sample_threshold=50_000
        )

        # Should sample to max_sample_size
        sampled_count = df_sampled.count()
        assert sampled_count <= 55_000  # Allow some variance

    def test_create_fitting_sample(self, spark_session, small_dataset):
        """Test creating sample for distribution fitting."""
        fitter = DistributionFitter(spark_session)
        row_count = small_dataset.count()

        sample = fitter._create_fitting_sample(small_dataset, "value", row_count)

        # Should be numpy array
        assert isinstance(sample, np.ndarray)

        # Should be <= 10k (default sample size)
        assert len(sample) <= 10_000

    @pytest.mark.parametrize("num_dists", [5, 100])
    def test_calculate_partitions(self, spark_session, num_dists):
        """Test partition calculation returns reasonable values."""
        fitter = DistributionFitter(spark_session)
        partitions = fitter._calculate_partitions(num_dists)
        assert 1 <= partitions <= num_dists

    def test_fit_caches_results(self, spark_session, small_dataset):
        """Test that fit results are cached and consistent."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)

        # Access results multiple times (should use cache)
        count1 = results.count()
        count2 = results.count()
        best1 = results.best(n=1)[0]
        best2 = results.best(n=1)[0]

        # Counts should be consistent
        assert count1 == count2
        assert count1 > 0

        # Best results should be identical
        assert best1.distribution == best2.distribution
        assert best1.sse == best2.sse
        assert best1.parameters == best2.parameters

        # DataFrame should also be consistent
        df1 = results.df.toPandas()
        df2 = results.df.toPandas()
        assert len(df1) == len(df2)
        assert list(df1["distribution"]) == list(df2["distribution"])

    def test_fit_filters_failed_fits(self, spark_session, small_dataset):
        """Test that failed fits are filtered out."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)

        # All results should have finite SSE
        df_pandas = results.df.toPandas()
        assert all(np.isfinite(df_pandas["sse"]))

    def test_fit_with_constant_data(self, spark_session, constant_dataset):
        """Test fitting with constant data (edge case)."""
        fitter = DistributionFitter(spark_session)

        # Should handle gracefully without crashing
        results = fitter.fit(constant_dataset, column="value", max_distributions=5)

        # Returns valid FitResults (may have 0 or more distributions)
        assert isinstance(results, FitResults)
        # Verify we can call methods on it without error
        _ = results.df.toPandas()

    def test_fit_with_rice_rule(self, spark_session, small_dataset):
        """Test fitting with Rice rule for bins."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", use_rice_rule=True, max_distributions=5)

        # Should fit all 5 requested distributions
        assert results.count() == 5

    def test_fit_excluded_distributions(self, spark_session, small_dataset):
        """Test that excluded distributions are not fitted."""
        fitter = DistributionFitter(spark_session, excluded_distributions=("norm", "expon"))
        results = fitter.fit(small_dataset, column="value", max_distributions=5)

        # norm and expon should not be in results
        df_pandas = results.df.toPandas()
        assert "norm" not in df_pandas["distribution"].values
        assert "expon" not in df_pandas["distribution"].values

    def test_fit_multiple_columns_sequential(self, spark_session):
        """Test fitting multiple columns sequentially."""
        # Create DataFrame with multiple columns
        np.random.seed(42)
        data1 = np.random.normal(50, 10, 10_000)
        data2 = np.random.exponential(5, 10_000)

        df = spark_session.createDataFrame([(float(x), float(y)) for x, y in zip(data1, data2)], ["col1", "col2"])

        fitter = DistributionFitter(spark_session)

        # Fit first column
        results1 = fitter.fit(df, column="col1", max_distributions=5)
        best1 = results1.best(n=1)[0]

        # Fit second column
        results2 = fitter.fit(df, column="col2", max_distributions=5)
        best2 = results2.best(n=1)[0]

        # Both should succeed
        assert best1.sse < np.inf
        assert best2.sse < np.inf

        # Should identify different distributions
        top1 = [r.distribution for r in results1.best(n=3)]
        top2 = [r.distribution for r in results2.best(n=3)]

        # Normal should be in top for col1, expon should be in top for col2
        assert "norm" in top1 or best1.sse < 0.01
        assert "expon" in top2 or best2.sse < 0.01

    def test_fit_reproducibility(self, spark_session, small_dataset):
        """Test that fitting is reproducible with same seed."""
        fitter1 = DistributionFitter(spark_session, random_seed=42)
        fitter2 = DistributionFitter(spark_session, random_seed=42)

        # Use max_distributions to speed up test
        results1 = fitter1.fit(small_dataset, column="value", max_distributions=5)
        results2 = fitter2.fit(small_dataset, column="value", max_distributions=5)

        # Should get same best distribution
        best1 = results1.best(n=1)[0]
        best2 = results2.best(n=1)[0]

        assert best1.distribution == best2.distribution
        # SSE might differ slightly due to sampling, but should be close
        assert np.isclose(best1.sse, best2.sse, rtol=0.1)


class TestMultiColumnFitting:
    """Tests for multi-column distribution fitting."""

    def test_fit_multiple_columns_basic(self, spark_session):
        """Test basic multi-column fitting."""
        np.random.seed(42)
        data1 = np.random.normal(50, 10, 5000)
        data2 = np.random.exponential(5, 5000)

        df = spark_session.createDataFrame(
            [(float(a), float(b)) for a, b in zip(data1, data2)],
            ["col1", "col2"]
        )

        fitter = DistributionFitter(spark_session)
        results = fitter.fit(df, columns=["col1", "col2"], max_distributions=3)

        # Should have results for both columns
        assert results.count() == 6  # 2 columns Ã— 3 distributions
        assert set(results.column_names) == {"col1", "col2"}

    def test_fit_multiple_columns_filtering(self, spark_session):
        """Test filtering multi-column results by column."""
        np.random.seed(42)
        data1 = np.random.normal(50, 10, 5000)
        data2 = np.random.exponential(5, 5000)

        df = spark_session.createDataFrame(
            [(float(a), float(b)) for a, b in zip(data1, data2)],
            ["normal_col", "expon_col"]
        )

        fitter = DistributionFitter(spark_session)
        results = fitter.fit(df, columns=["normal_col", "expon_col"], max_distributions=3)

        # Filter to single column
        normal_results = results.for_column("normal_col")
        assert normal_results.count() == 3

        expon_results = results.for_column("expon_col")
        assert expon_results.count() == 3

    def test_fit_multiple_columns_best_per_column(self, spark_session):
        """Test best_per_column method."""
        np.random.seed(42)
        data1 = np.random.normal(50, 10, 5000)
        data2 = np.random.exponential(5, 5000)

        df = spark_session.createDataFrame(
            [(float(a), float(b)) for a, b in zip(data1, data2)],
            ["col1", "col2"]
        )

        fitter = DistributionFitter(spark_session)
        results = fitter.fit(df, columns=["col1", "col2"], max_distributions=5)

        best_per_col = results.best_per_column(n=1)

        assert "col1" in best_per_col
        assert "col2" in best_per_col
        assert len(best_per_col["col1"]) == 1
        assert len(best_per_col["col2"]) == 1
        assert best_per_col["col1"][0].column_name == "col1"
        assert best_per_col["col2"][0].column_name == "col2"

    def test_fit_backward_compatibility(self, spark_session, small_dataset):
        """Test that single column API still works with positional arg."""
        fitter = DistributionFitter(spark_session)
        # Using positional argument (backward compatible)
        results = fitter.fit(small_dataset, "value", max_distributions=3)

        assert results.count() == 3
        best = results.best(n=1)[0]
        assert best.column_name == "value"

    def test_fit_mutually_exclusive_params(self, spark_session, small_dataset):
        """Test error when both column and columns provided."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="Cannot provide both"):
            fitter.fit(small_dataset, column="value", columns=["value"])

    def test_fit_no_column_params(self, spark_session, small_dataset):
        """Test error when neither column nor columns provided."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="Must provide either"):
            fitter.fit(small_dataset)

    def test_fit_invalid_column_in_list(self, spark_session, small_dataset):
        """Test error when invalid column in columns list."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="not found"):
            fitter.fit(small_dataset, columns=["value", "nonexistent"])

    def test_fit_single_column_via_columns_param(self, spark_session, small_dataset):
        """Test fitting single column using columns parameter."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, columns=["value"], max_distributions=3)

        assert results.count() == 3
        assert results.column_names == ["value"]


class TestBroadcastCleanup:
    """Tests for broadcast variable cleanup.

    These tests verify that fit() properly cleans up broadcast variables by
    patching Broadcast.unpersist at the class level to track calls.
    """

    def test_broadcast_cleanup_on_success(self, spark_session, small_dataset):
        """Verify unpersist() is called on broadcast variables after successful fit."""
        from pyspark import Broadcast

        fitter = DistributionFitter(spark_session)

        # Track unpersist calls at Broadcast class level
        unpersist_calls = []
        original_unpersist = Broadcast.unpersist

        def tracked_unpersist(self, blocking=False):
            unpersist_calls.append(self._jbroadcast.id())
            return original_unpersist(self, blocking)

        with patch.object(Broadcast, "unpersist", tracked_unpersist):
            results = fitter.fit(small_dataset, column="value", max_distributions=3)
            assert results.count() > 0

        # Verify both broadcasts (histogram_bc and data_sample_bc) were unpersisted
        assert len(unpersist_calls) == 2, f"Expected 2 unpersist calls, got {len(unpersist_calls)}"

    def test_discrete_broadcast_cleanup_on_success(self, spark_session):
        """Verify unpersist() is called for discrete fitter broadcasts."""
        from pyspark import Broadcast

        # Create discrete data
        data = [(int(x),) for x in np.random.poisson(lam=5, size=1000)]
        df = spark_session.createDataFrame(data, ["value"])

        fitter = DiscreteDistributionFitter(spark_session)

        # Track unpersist calls at Broadcast class level
        unpersist_calls = []
        original_unpersist = Broadcast.unpersist

        def tracked_unpersist(self, blocking=False):
            unpersist_calls.append(self._jbroadcast.id())
            return original_unpersist(self, blocking)

        with patch.object(Broadcast, "unpersist", tracked_unpersist):
            results = fitter.fit(df, column="value", max_distributions=3)
            assert results.count() > 0

        # Verify both broadcasts were unpersisted
        assert len(unpersist_calls) == 2, f"Expected 2 unpersist calls, got {len(unpersist_calls)}"

    def test_broadcast_cleanup_on_get_distributions_exception(self, spark_session, small_dataset):
        """Verify no broadcasts leak when get_distributions() raises.

        With the refactored architecture, broadcasts are created inside
        _fit_single_column(), which is called AFTER get_distributions().
        If get_distributions() fails, no broadcasts have been created yet,
        so no cleanup is needed. This is actually better than the old design
        because it minimizes resource allocation before potential failures.
        """
        fitter = DistributionFitter(spark_session)

        with patch.object(
            fitter._registry, "get_distributions", side_effect=ValueError("Injected error")
        ):
            with pytest.raises(ValueError, match="Injected error"):
                fitter.fit(small_dataset, column="value", max_distributions=3)

        # No broadcasts should have been created, so no cleanup needed

    def test_discrete_broadcast_cleanup_on_get_distributions_exception(self, spark_session):
        """Verify no discrete broadcasts leak when get_distributions() raises.

        Broadcasts are created after get_distributions() is called, so if
        get_distributions() fails, no broadcasts have been created yet.
        """
        data = [(int(x),) for x in np.random.poisson(lam=5, size=1000)]
        df = spark_session.createDataFrame(data, ["value"])

        fitter = DiscreteDistributionFitter(spark_session)

        with patch.object(
            fitter._registry, "get_distributions", side_effect=ValueError("Injected error")
        ):
            with pytest.raises(ValueError, match="Injected error"):
                fitter.fit(df, column="value", max_distributions=3)

        # No broadcasts should have been created, so no cleanup needed


class TestDistributionFitterPlotting:
    """Tests for plotting functionality."""

    def test_plot_after_fit(self, spark_session, small_dataset):
        """Test plotting after fitting."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)
        best = results.best(n=1)[0]

        # Should not raise error (df and column are now required)
        fig, ax = fitter.plot(best, small_dataset, "value")

        assert fig is not None
        assert ax is not None

    def test_plot_with_title(self, spark_session, small_dataset):
        """Test plotting with title."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)
        best = results.best(n=1)[0]

        # Should work with explicit data and title
        fig, ax = fitter.plot(best, small_dataset, "value", title="Test Plot")

        assert fig is not None
        assert ax is not None

    def test_plot_with_custom_params(self, spark_session, small_dataset):
        """Test plotting with custom parameters."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)
        best = results.best(n=1)[0]

        # Should work with custom figsize, dpi, etc.
        fig, ax = fitter.plot(
            best, small_dataset, "value",
            figsize=(16, 10), dpi=150, title="Custom Plot"
        )

        assert fig is not None
        assert ax is not None

    def test_plot_qq_after_fit(self, spark_session, small_dataset):
        """Test Q-Q plotting after fitting."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)
        best = results.best(n=1)[0]

        fig, ax = fitter.plot_qq(best, small_dataset, "value")

        assert fig is not None
        assert ax is not None

    def test_plot_qq_with_max_points(self, spark_session, small_dataset):
        """Test Q-Q plotting with custom max_points."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=5)
        best = results.best(n=1)[0]

        fig, ax = fitter.plot_qq(best, small_dataset, "value", max_points=500, title="Q-Q Test")

        assert fig is not None
        assert ax is not None


class TestEdgeCases:
    """Tests for edge cases and error handling."""

    def test_very_small_dataset(self, spark_session):
        """Test with very small dataset."""
        data = np.array([1.0, 2.0, 3.0])
        df = spark_session.createDataFrame([(float(x),) for x in data], ["value"])

        fitter = DistributionFitter(spark_session)

        # Should handle gracefully without crashing
        results = fitter.fit(df, column="value", max_distributions=5)

        # Returns valid FitResults
        assert isinstance(results, FitResults)
        _ = results.df.toPandas()

    def test_single_value_dataset(self, spark_session):
        """Test with single value."""
        df = spark_session.createDataFrame([(42.0,)], ["value"])

        fitter = DistributionFitter(spark_session)

        # Should handle gracefully without crashing
        results = fitter.fit(df, column="value", max_distributions=5)

        # Returns valid FitResults
        assert isinstance(results, FitResults)
        _ = results.df.toPandas()

    def test_dataset_with_outliers(self, spark_session):
        """Test with dataset containing extreme outliers."""
        np.random.seed(42)
        normal_data = np.random.normal(50, 10, 9995)
        outliers = np.array([1000, -1000, 2000, -2000, 3000])
        data = np.concatenate([normal_data, outliers])

        df = spark_session.createDataFrame([(float(x),) for x in data], ["value"])

        fitter = DistributionFitter(spark_session)

        # Should handle outliers and fit all 5 requested distributions
        results = fitter.fit(df, column="value", max_distributions=5)

        assert results.count() == 5
        best = results.best(n=1)[0]
        assert best.sse < np.inf

    def test_apply_sampling_at_threshold(self, spark_session, small_dataset):
        """Test that data at threshold doesn't sample."""
        fitter = DistributionFitter(spark_session)
        df_result = fitter._apply_sampling(
            small_dataset, row_count=10_000, enable_sampling=True,
            sample_fraction=None, max_sample_size=1_000_000, sample_threshold=10_000
        )

        # At threshold should return original data (uses <=)
        assert df_result.count() == small_dataset.count()

    def test_fit_max_distributions_zero(self, spark_session, small_dataset):
        """Test fitting with max_distributions=0 raises error."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError):
            fitter.fit(small_dataset, column="value", max_distributions=0)

    def test_fit_with_different_columns(self, spark_session):
        """Test fitting on different column names."""
        np.random.seed(42)
        data = np.random.normal(50, 10, 1000)
        df = spark_session.createDataFrame([(float(x),) for x in data], ["custom_column_name"])

        fitter = DistributionFitter(spark_session)
        results = fitter.fit(df, column="custom_column_name", max_distributions=3)

        # Should fit all 3 requested distributions
        assert results.count() == 3

    def test_fit_invalid_bins(self, spark_session, small_dataset):
        """Test that invalid bins raises error."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="bins must be positive"):
            fitter.fit(small_dataset, column="value", bins=0)

    def test_fit_invalid_sample_fraction(self, spark_session, small_dataset):
        """Test that invalid sample_fraction raises error."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="sample_fraction must be in"):
            fitter.fit(small_dataset, column="value", sample_fraction=1.5)

class TestCoreNegativePaths:
    """Tests for negative/error paths in core module."""

    def test_fit_invalid_column(self, spark_session, small_dataset):
        """Test that fit raises error for invalid column."""
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="not found"):
            fitter.fit(small_dataset, column="nonexistent_column", max_distributions=3)

    def test_fit_non_numeric_column(self, spark_session):
        """Test that fit raises error for non-numeric column."""
        df = spark_session.createDataFrame([("a",), ("b",), ("c",)], ["value"])
        fitter = DistributionFitter(spark_session)

        with pytest.raises(TypeError, match="must be numeric"):
            fitter.fit(df, column="value", max_distributions=3)

    def test_fit_empty_dataframe(self, spark_session):
        """Test that fit raises error for empty DataFrame."""
        from pyspark.sql.types import DoubleType, StructField, StructType

        schema = StructType([StructField("value", DoubleType(), True)])
        df = spark_session.createDataFrame([], schema)
        fitter = DistributionFitter(spark_session)

        with pytest.raises(ValueError, match="empty"):
            fitter.fit(df, column="value", max_distributions=3)

    def test_plot_with_different_data(self, spark_session):
        """Test that plot works with different data than fit."""
        np.random.seed(42)
        data1 = np.random.normal(50, 10, 1000)
        data2 = np.random.normal(100, 20, 1000)

        df1 = spark_session.createDataFrame([(float(x),) for x in data1], ["value"])
        df2 = spark_session.createDataFrame([(float(x),) for x in data2], ["value"])

        fitter = DistributionFitter(spark_session)

        # Fit on first dataset
        results = fitter.fit(df1, column="value", max_distributions=3)
        best = results.best(n=1)[0]

        # Plot with different dataset should work
        fig, ax = fitter.plot(best, df=df2, column="value")
        assert fig is not None


class TestDiscreteDistributionFitter:
    """Tests for DiscreteDistributionFitter class."""

    def test_initialization(self, spark_session):
        """Test discrete fitter initialization with custom exclusions."""
        custom_exclusions = ("poisson", "geom")
        fitter = DiscreteDistributionFitter(
            spark_session, excluded_distributions=custom_exclusions, random_seed=123
        )

        assert fitter.excluded_distributions == custom_exclusions
        assert fitter.random_seed == 123

    def test_fit_identifies_poisson(self, spark_session, poisson_dataset):
        """Test that fitter identifies Poisson for Poisson data."""
        fitter = DiscreteDistributionFitter(spark_session)
        results = fitter.fit(poisson_dataset, column="counts")

        top_5 = [r.distribution for r in results.best(n=5)]
        assert "poisson" in top_5

    def test_fit_identifies_nbinom(self, spark_session, nbinom_dataset):
        """Test that fitter identifies negative binomial for nbinom data."""
        fitter = DiscreteDistributionFitter(spark_session)
        results = fitter.fit(nbinom_dataset, column="counts")

        top_5 = [r.distribution for r in results.best(n=5)]
        assert "nbinom" in top_5

    def test_fit_parameters_accuracy(self, spark_session, poisson_dataset, poisson_data):
        """Test that Poisson lambda is estimated accurately."""
        fitter = DiscreteDistributionFitter(spark_session)
        results = fitter.fit(poisson_dataset, column="counts")

        poisson_fit = next(r for r in results.best(n=10) if r.distribution == "poisson")
        fitted_lambda = poisson_fit.parameters[0]
        true_lambda = np.mean(poisson_data)

        assert np.isclose(fitted_lambda, true_lambda, rtol=0.05)

    def test_fit_excluded_distributions(self, spark_session, poisson_dataset):
        """Test that excluded distributions are not fitted."""
        fitter = DiscreteDistributionFitter(
            spark_session, excluded_distributions=("poisson", "nbinom")
        )
        results = fitter.fit(poisson_dataset, column="counts")

        all_dists = [r.distribution for r in results.best(n=20)]
        assert "poisson" not in all_dists
        assert "nbinom" not in all_dists

    def test_fit_empty_dataframe_raises(self, spark_session):
        """Test that fit raises error for empty DataFrame."""
        from pyspark.sql.types import IntegerType, StructField, StructType

        schema = StructType([StructField("counts", IntegerType(), True)])
        df = spark_session.createDataFrame([], schema)
        fitter = DiscreteDistributionFitter(spark_session)

        with pytest.raises(ValueError, match="empty"):
            fitter.fit(df, column="counts")

    def test_fit_invalid_column_raises(self, spark_session, poisson_dataset):
        """Test that fit raises error for invalid column."""
        fitter = DiscreteDistributionFitter(spark_session)

        with pytest.raises(ValueError, match="not found"):
            fitter.fit(poisson_dataset, column="nonexistent")

    def test_plot_produces_stem_plot(self, spark_session, poisson_dataset):
        """Test that discrete plot produces expected stem plot elements."""
        import matplotlib.pyplot as plt

        fitter = DiscreteDistributionFitter(spark_session)
        results = fitter.fit(poisson_dataset, column="counts", max_distributions=3)
        best = results.best(n=1)[0]

        fig, ax = fitter.plot(best, poisson_dataset, "counts", title="Test Plot")

        # Verify plot has expected elements
        assert ax.get_title().startswith("Test Plot")
        assert ax.get_xlabel() == "Value"
        assert ax.get_ylabel() == "Probability"
        # Should have bars (histogram) and stems (fitted PMF)
        assert len(ax.containers) > 0 or len(ax.collections) > 0

        plt.close(fig)


class TestPrefilter:
    """Tests for distribution pre-filtering feature (v1.6.0)."""

    def test_prefilter_false_no_filtering(self, spark_session, small_dataset):
        """Test that prefilter=False doesn't filter any distributions."""
        fitter = DistributionFitter(spark_session)
        results = fitter.fit(small_dataset, column="value", max_distributions=10, prefilter=False)
        assert results.count() == 10

    def test_prefilter_true_filters_incompatible(self, spark_session):
        """Test that prefilter=True filters incompatible distributions."""
        # Create left-skewed data with negative values
        np.random.seed(42)
        left_skewed = -np.abs(np.random.exponential(5, 5000))
        df = spark_session.createDataFrame([(float(x),) for x in left_skewed], ["value"])

        fitter = DistributionFitter(spark_session)

        # Without prefilter
        results_no_filter = fitter.fit(df, "value", max_distributions=20, prefilter=False, lazy_metrics=True)
        count_no_filter = results_no_filter.count()

        # With prefilter=True
        results_filter = fitter.fit(df, "value", max_distributions=20, prefilter=True, lazy_metrics=True)
        count_filter = results_filter.count()

        # Should filter out positive-skew-only and non-negative-support distributions
        assert count_filter < count_no_filter, "prefilter should reduce distribution count"

    def test_prefilter_discrete_warns(self, spark_session, poisson_dataset):
        """Test that prefilter on discrete fitter logs a warning."""
        fitter = DiscreteDistributionFitter(spark_session)

        with patch("spark_bestfit.core.logger") as mock_logger:
            results = fitter.fit(poisson_dataset, column="counts", max_distributions=3, prefilter=True)
            mock_logger.warning.assert_called_once()
            assert "not yet supported" in str(mock_logger.warning.call_args)

        # Should still return valid results
        assert results.count() > 0


class TestPrefilterUnit:
    """Unit tests for _prefilter_distributions() method directly."""

    @pytest.fixture
    def fitter(self, spark_session):
        """Create a fitter instance for testing."""
        return DistributionFitter(spark_session)

    def test_no_support_filtering_with_negative_data(self, fitter):
        """Test that negative data does NOT filter distributions (loc can shift them)."""
        # scipy.fit() uses loc parameter that can shift any distribution
        # to cover any data range, so we don't filter by support bounds
        data = np.array([-5.0, -2.0, 0.0, 1.0, 3.0])  # Has negative values
        distributions = ["norm", "expon", "gamma", "t", "lognorm", "laplace"]

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=True)

        # ALL distributions should remain (loc can shift them)
        # Only skewness/kurtosis filtering applies
        assert "norm" in compatible
        assert "expon" in compatible  # Can use loc=-10 to fit negative data
        assert "gamma" in compatible  # Can use loc=-10 to fit negative data
        assert "t" in compatible
        assert "lognorm" in compatible  # Can use loc=-10 to fit negative data
        assert "laplace" in compatible

        # No support-based filtering should occur
        support_filtered = [f for f in filtered if "support" in f[1]]
        assert len(support_filtered) == 0

    def test_no_filtering_for_symmetric_data(self, fitter):
        """Test that symmetric data around zero keeps all distributions."""
        data = np.array([-5.0, -2.0, 0.0, 2.0, 5.0])  # Symmetric
        distributions = ["norm", "expon", "gamma", "t", "uniform", "beta"]

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=True)

        # All should be compatible - no skewness or kurtosis filtering triggered
        assert len(compatible) == len(distributions)
        assert len(filtered) == 0

    def test_loc_scale_can_shift_any_distribution(self, fitter):
        """Test that distributions with default [0,1] bounds still pass (loc/scale shift them)."""
        # Data outside [0, 1] - but beta can be shifted via loc/scale
        data = np.array([50.0, 55.0, 60.0, 65.0, 70.0])
        distributions = ["beta", "uniform", "norm"]

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=True)

        # beta and uniform should NOT be filtered (loc/scale shift them)
        assert "beta" in compatible
        assert "uniform" in compatible
        assert "norm" in compatible
        assert len(filtered) == 0

    def test_skewness_filtering_left_skewed(self, fitter):
        """Test that left-skewed data filters positive-skew-only distributions."""
        # Create strongly left-skewed data (skewness < -1.0)
        np.random.seed(42)
        data = -np.random.exponential(5, 1000)  # Skewness ~ -2.0
        distributions = ["norm", "expon", "gamma", "t", "laplace"]

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=True)

        # Positive-skew-only distributions should be filtered by skewness
        # (skewness is a shape property that cannot be changed by loc/scale)
        assert "expon" not in compatible
        assert "gamma" not in compatible

        # Symmetric distributions should remain
        assert "norm" in compatible
        assert "t" in compatible
        assert "laplace" in compatible

    def test_skewness_filtering_threshold(self, fitter):
        """Test that skewness filter only triggers for |skew| > 1.0."""
        # Mildly left-skewed data (skewness ~ -0.5)
        np.random.seed(42)
        data = np.random.beta(3, 2, 1000) - 0.5  # Mild left skew, has negatives

        from scipy.stats import skew
        assert -1.0 < skew(data) < 0, "Test data should be mildly left-skewed"

        distributions = ["norm", "t", "laplace"]  # Only unbounded for this test

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=True)

        # All should remain (skewness not extreme enough to trigger filter)
        assert "norm" in compatible
        assert "t" in compatible
        assert "laplace" in compatible

    def test_aggressive_mode_kurtosis_filtering(self, fitter):
        """Test that aggressive mode filters by kurtosis for heavy-tailed data."""
        # Create heavy-tailed data WITHIN [0, 1] so uniform's support bounds pass
        # Use Pareto distribution, clipped and normalized to [0, 1]
        np.random.seed(42)
        raw = np.random.pareto(1.5, 10000)
        p99 = np.percentile(raw, 99)
        data = np.clip(raw, 0, p99) / p99  # Data in [0, 1] with heavy right tail

        from scipy.stats import kurtosis
        assert kurtosis(data) > 10, f"Test data should have high kurtosis, got {kurtosis(data)}"
        assert data.min() >= 0 and data.max() <= 1, "Data should be in [0, 1]"

        # uniform has support [0, 1], so it passes support bounds for this data
        # norm and beta also work with this range
        distributions = ["norm", "uniform", "beta"]

        # Safe mode should keep uniform (kurtosis filter not applied)
        compatible_safe, _ = fitter._prefilter_distributions(distributions, data, mode=True)
        assert "uniform" in compatible_safe, f"Safe mode should keep uniform, got {compatible_safe}"

        # Aggressive mode should filter uniform (low kurtosis dist for high kurtosis data)
        compatible_aggressive, filtered_aggressive = fitter._prefilter_distributions(
            distributions, data, mode="aggressive"
        )
        assert "uniform" not in compatible_aggressive, f"Aggressive mode should filter uniform, got {compatible_aggressive}"
        assert any(f[0] == "uniform" for f in filtered_aggressive), f"uniform should be in filtered list"

    def test_prefilter_mode_false_returns_all(self, fitter):
        """Test that mode=False returns all distributions unfiltered."""
        data = np.array([-100.0, -50.0, 0.0])  # Would normally filter many
        distributions = ["norm", "expon", "gamma", "t"]

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=False)

        # mode=False should return all (no filtering)
        assert compatible == distributions
        assert len(filtered) == 0

    def test_unknown_distribution_kept(self, fitter):
        """Test that unknown distributions are kept (conservative approach)."""
        data = np.array([-1.0, 0.0, 1.0])
        distributions = ["norm", "fake_distribution_xyz"]

        compatible, filtered = fitter._prefilter_distributions(distributions, data, mode=True)

        # Unknown distribution should be kept
        assert "fake_distribution_xyz" in compatible
        assert "norm" in compatible


class TestPrefilterIntegration:
    """Integration tests for prefilter with full fitting pipeline."""

    def test_prefilter_no_support_filtering_integration(self, spark_session):
        """Test that prefilter does NOT filter by support (loc/scale can shift distributions)."""
        np.random.seed(42)
        # Symmetric data around 0 - no skewness filtering triggered
        data = np.random.normal(0, 1, 5000)
        df = spark_session.createDataFrame([(float(x),) for x in data], ["value"])

        fitter = DistributionFitter(spark_session)
        results = fitter.fit(df, "value", max_distributions=30, prefilter=True, lazy_metrics=True)

        fitted_dists = {r.distribution for r in results.best(n=100)}

        # Distributions CAN be fit even with negative data (loc shifts them)
        # Only skewness/kurtosis filtering applies, not support bounds
        # Since data is symmetric, no skewness filtering should occur
        assert len(fitted_dists) > 0, "Should have fit some distributions"

    def test_prefilter_skewness_filtering_integration(self, spark_session):
        """Test that prefilter correctly filters by skewness in full pipeline."""
        np.random.seed(42)
        left_skewed = -np.random.exponential(5, 5000)
        from scipy.stats import skew
        assert skew(left_skewed) < -1.0, "Test data should be clearly left-skewed"

        df = spark_session.createDataFrame([(float(x),) for x in left_skewed], ["value"])

        fitter = DistributionFitter(spark_session)
        results = fitter.fit(df, "value", max_distributions=30, prefilter=True, lazy_metrics=True)

        fitted_dists = {r.distribution for r in results.best(n=100)}

        # Positive-skew-only distributions should be filtered
        positive_skew_only = {"expon", "gamma", "lognorm"}
        for dist in positive_skew_only:
            assert dist not in fitted_dists, f"{dist} should be filtered (positive-skew only)"

    def test_prefilter_aggressive_filters_more(self, spark_session):
        """Test that aggressive mode filters additional distributions."""
        np.random.seed(42)
        # Heavy-tailed data with very high kurtosis
        heavy_tailed = np.random.standard_t(2, 5000) * 10
        df = spark_session.createDataFrame([(float(x),) for x in heavy_tailed], ["value"])

        fitter = DistributionFitter(spark_session)

        # Get results with both modes
        results_safe = fitter.fit(df, "value", max_distributions=20, prefilter=True, lazy_metrics=True)
        results_aggressive = fitter.fit(df, "value", max_distributions=20, prefilter="aggressive", lazy_metrics=True)

        safe_dists = {r.distribution for r in results_safe.best(n=100)}
        aggressive_dists = {r.distribution for r in results_aggressive.best(n=100)}

        # Aggressive should filter at least as much as safe (usually more)
        assert len(aggressive_dists) <= len(safe_dists)

        # Specifically, uniform should be filtered in aggressive mode for heavy-tailed data
        # (if it was in the original distribution list)
        if "uniform" in safe_dists:
            assert "uniform" not in aggressive_dists, "uniform should be filtered in aggressive mode"

    def test_prefilter_multicolumn(self, spark_session):
        """Test that prefilter works correctly for multi-column fitting."""
        np.random.seed(42)
        # Column 1: left-skewed data (should filter positive-skew-only dists by skewness)
        col1 = -np.random.exponential(5, 1000)  # Left-skewed
        # Column 2: right-skewed data (should keep positive-skew-only dists)
        col2 = np.random.exponential(5, 1000)  # Right-skewed

        from scipy.stats import skew
        assert skew(col1) < -1.0, "col1 should be left-skewed"
        assert skew(col2) > 1.0, "col2 should be right-skewed"

        data = [(float(c1), float(c2)) for c1, c2 in zip(col1, col2)]
        df = spark_session.createDataFrame(data, ["col1", "col2"])

        fitter = DistributionFitter(spark_session)
        # Use max_distributions=30 to ensure expon/gamma are included
        results = fitter.fit(df, columns=["col1", "col2"], max_distributions=30, prefilter=True, lazy_metrics=True)

        # Get results per column
        col1_dists = {r.distribution for r in results.for_column("col1").best(n=100)}
        col2_dists = {r.distribution for r in results.for_column("col2").best(n=100)}

        # Col1 (left-skewed) should filter positive-skew-only distributions
        assert "expon" not in col1_dists, "expon should be filtered for left-skewed data"
        assert "gamma" not in col1_dists, "gamma should be filtered for left-skewed data"

        # Col2 (right-skewed) should keep positive-skew-only distributions
        assert "expon" in col2_dists, "expon should be kept for right-skewed data"
        assert "gamma" in col2_dists, "gamma should be kept for right-skewed data"

    def test_prefilter_fallback_logs_warning(self, spark_session):
        """Test that fallback when all filtered logs appropriate warning."""
        np.random.seed(42)
        negative_data = np.random.normal(-100, 10, 1000)
        df = spark_session.createDataFrame([(float(x),) for x in negative_data], ["value"])

        # Create fitter that only has non-negative distributions
        # by using a very restrictive exclusion list
        fitter = DistributionFitter(spark_session)

        with patch("spark_bestfit.core.logger") as mock_logger:
            results = fitter.fit(
                df, "value",
                max_distributions=5,
                prefilter=True,
                lazy_metrics=True
            )
            # Should still return results
            assert results.count() > 0

    def test_prefilter_logs_filtered_distributions(self, spark_session):
        """Test that prefilter logs which distributions were filtered."""
        np.random.seed(42)
        # Use left-skewed data to trigger skewness filtering
        left_skewed_data = -np.random.exponential(5, 1000)
        df = spark_session.createDataFrame([(float(x),) for x in left_skewed_data], ["value"])

        fitter = DistributionFitter(spark_session)

        with patch("spark_bestfit.core.logger") as mock_logger:
            results = fitter.fit(df, "value", max_distributions=30, prefilter=True, lazy_metrics=True)

            # Verify info log was called with prefilter message
            info_calls = [str(call) for call in mock_logger.info.call_args_list]
            prefilter_logged = any("Pre-filter: skipped" in call for call in info_calls)
            assert prefilter_logged, "Should log pre-filter info message (skewness filtering)"
