{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-bestfit API Demo\n",
    "\n",
    "This notebook demonstrates the complete API for the `spark-bestfit` library, including:\n",
    "\n",
    "1. **Distribution Fitting** - Using DistributionFitter with direct parameters\n",
    "2. **Progress Tracking** - Monitor long-running fits with callbacks\n",
    "3. **Working with Results** - FitResults and DistributionFitResult objects\n",
    "4. **Lazy Metrics** - Skip KS/AD computation for faster model selection (v1.5.0+)\n",
    "5. **Pre-filtering** - Skip incompatible distributions for faster fitting (v1.6.0+)\n",
    "6. **Confidence Intervals** - Bootstrap confidence intervals for fitted parameters\n",
    "7. **Plotting** - Visualization with customizable parameters\n",
    "8. **Excluding Distributions** - Customizing which distributions to fit\n",
    "9. **Serialization** - Save and load fitted distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's create a Spark session. Note: **You** are responsible for creating and configuring your SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:19.830791Z",
     "iopub.status.busy": "2026-01-01T14:36:19.830603Z",
     "iopub.status.idle": "2026-01-01T14:36:22.653099Z",
     "shell.execute_reply": "2026-01-01T14:36:22.652683Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session (your responsibility - configure as needed for your environment)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"API-Demo\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:22.654518Z",
     "iopub.status.busy": "2026-01-01T14:36:22.654376Z",
     "iopub.status.idle": "2026-01-01T14:36:22.968242Z",
     "shell.execute_reply": "2026-01-01T14:36:22.967849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import spark-bestfit components\n",
    "from spark_bestfit import (\n",
    "    DistributionFitter,\n",
    "    DEFAULT_EXCLUDED_DISTRIBUTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample data from known distributions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:22.969519Z",
     "iopub.status.busy": "2026-01-01T14:36:22.969431Z",
     "iopub.status.idle": "2026-01-01T14:36:25.100027Z",
     "shell.execute_reply": "2026-01-01T14:36:25.099307Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution data\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=50_000)\n",
    "df_normal = spark.createDataFrame([(float(x),) for x in normal_data], [\"value\"])\n",
    "\n",
    "# Exponential distribution data (non-negative)\n",
    "exp_data = np.random.exponential(scale=5, size=50_000)\n",
    "df_exp = spark.createDataFrame([(float(x),) for x in exp_data], [\"value\"])\n",
    "\n",
    "# Gamma distribution data\n",
    "gamma_data = np.random.gamma(shape=2.0, scale=2.0, size=50_000)\n",
    "df_gamma = spark.createDataFrame([(float(x),) for x in gamma_data], [\"value\"])\n",
    "\n",
    "print(f\"Normal data: {df_normal.count():,} rows, mean={normal_data.mean():.2f}, std={normal_data.std():.2f}\")\n",
    "print(f\"Exponential data: {df_exp.count():,} rows, mean={exp_data.mean():.2f}\")\n",
    "print(f\"Gamma data: {df_gamma.count():,} rows, mean={gamma_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Excluded Distributions\n",
    "\n",
    "spark-bestfit excludes some slow distributions by default. You can customize this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DEFAULT_EXCLUDED_DISTRIBUTIONS\n",
    "\n",
    "Some distributions are excluded by default because they are very slow to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:25.102379Z",
     "iopub.status.busy": "2026-01-01T14:36:25.102218Z",
     "iopub.status.idle": "2026-01-01T14:36:25.104935Z",
     "shell.execute_reply": "2026-01-01T14:36:25.104283Z"
    }
   },
   "outputs": [],
   "source": [
    "# View default excluded distributions\n",
    "print(f\"Default excluded distributions ({len(DEFAULT_EXCLUDED_DISTRIBUTIONS)}):\")\n",
    "for dist in sorted(DEFAULT_EXCLUDED_DISTRIBUTIONS):\n",
    "    print(f\"  - {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:25.105919Z",
     "iopub.status.busy": "2026-01-01T14:36:25.105855Z",
     "iopub.status.idle": "2026-01-01T14:36:25.107523Z",
     "shell.execute_reply": "2026-01-01T14:36:25.107144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Include a specific distribution that's excluded by default\n",
    "custom_exclusions = tuple(d for d in DEFAULT_EXCLUDED_DISTRIBUTIONS if d != \"wald\")\n",
    "\n",
    "fitter_with_wald = DistributionFitter(spark, excluded_distributions=custom_exclusions)\n",
    "print(f\"Now fitting 'wald' distribution (removed from exclusions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Distribution Fitting\n",
    "\n",
    "The `DistributionFitter` class is the main entry point for fitting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:25.108527Z",
     "iopub.status.busy": "2026-01-01T14:36:25.108471Z",
     "iopub.status.idle": "2026-01-01T14:36:29.414109Z",
     "shell.execute_reply": "2026-01-01T14:36:29.413508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create fitter with default config\n",
    "fitter = DistributionFitter(spark)\n",
    "\n",
    "# Fit distributions to normal data (limit to 20 for demo speed)\n",
    "print(\"Fitting distributions to normal data...\")\n",
    "results_normal = fitter.fit(df_normal, column=\"value\", max_distributions=20)\n",
    "\n",
    "print(f\"\\nFitted {results_normal.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fitting with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:29.416430Z",
     "iopub.status.busy": "2026-01-01T14:36:29.416291Z",
     "iopub.status.idle": "2026-01-01T14:36:31.733545Z",
     "shell.execute_reply": "2026-01-01T14:36:31.732873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit only non-negative distributions using support_at_zero=True\n",
    "fitter_nonneg = DistributionFitter(spark)\n",
    "\n",
    "print(\"Fitting non-negative distributions to exponential data...\")\n",
    "results_exp = fitter_nonneg.fit(\n",
    "    df_exp,\n",
    "    column=\"value\",\n",
    "    bins=100,\n",
    "    support_at_zero=True,  # Only fit non-negative distributions\n",
    "    enable_sampling=True,\n",
    "    max_distributions=15,\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results_exp.count()} non-negative distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Progress Tracking\n",
    "\n",
    "For long-running fits, you can monitor progress with a callback. The easiest way is to use the built-in `console_progress()` utility:\n",
    "\n",
    "```python\n",
    "from spark_bestfit.progress import console_progress\n",
    "\n",
    "results = fitter.fit(df, column=\"value\", progress_callback=console_progress())\n",
    "```\n",
    "\n",
    "For custom callbacks, pass any function matching `(completed: int, total: int, percent: float) -> None`.\n",
    "\n",
    "**Note**: Progress percentages may fluctuate as new Spark stages add tasks. This is normal - progress generally trends upward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:31.734640Z",
     "iopub.status.busy": "2026-01-01T14:36:31.734559Z",
     "iopub.status.idle": "2026-01-01T14:36:34.337098Z",
     "shell.execute_reply": "2026-01-01T14:36:34.336550Z"
    }
   },
   "outputs": [],
   "source": [
    "from spark_bestfit.progress import console_progress\n",
    "\n",
    "# Simple approach: use built-in console_progress()\n",
    "print(\"Fitting with console_progress()...\")\n",
    "fitter_progress = DistributionFitter(spark)\n",
    "results_progress = fitter_progress.fit(\n",
    "    df_normal,\n",
    "    column=\"value\",\n",
    "    max_distributions=25,\n",
    "    progress_callback=console_progress(\"Fitting\"),  # Built-in utility\n",
    ")\n",
    "print()  # Newline after progress\n",
    "print(f\"Fitted {results_progress.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Using Active SparkSession\n",
    "\n",
    "If a SparkSession is already active, you don't need to pass it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.338112Z",
     "iopub.status.busy": "2026-01-01T14:36:34.338049Z",
     "iopub.status.idle": "2026-01-01T14:36:34.340120Z",
     "shell.execute_reply": "2026-01-01T14:36:34.339777Z"
    }
   },
   "outputs": [],
   "source": [
    "# DistributionFitter can use the active session automatically\n",
    "fitter_active = DistributionFitter()  # No spark parameter needed\n",
    "print(f\"Using active session: {fitter_active.spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Working with Results\n",
    "\n",
    "The `fit()` method returns a `FitResults` object for easy result manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Getting Best Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.341002Z",
     "iopub.status.busy": "2026-01-01T14:36:34.340946Z",
     "iopub.status.idle": "2026-01-01T14:36:34.419767Z",
     "shell.execute_reply": "2026-01-01T14:36:34.419089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get best distribution (by K-S statistic, the default)\n",
    "best = results_normal.best(n=1)[0]\n",
    "print(f\"Best by K-S statistic: {best.distribution}\")\n",
    "print(f\"  K-S statistic: {best.ks_statistic:.6f}\")\n",
    "print(f\"  p-value: {best.pvalue:.4f}\")\n",
    "print(f\"  A-D statistic: {best.ad_statistic:.6f}\")\n",
    "print(f\"  A-D p-value: {best.ad_pvalue:.4f}\" if best.ad_pvalue else f\"  A-D p-value: N/A (not available for {best.distribution})\")\n",
    "print(f\"  SSE: {best.sse:.6f}\")\n",
    "print(f\"  AIC: {best.aic:.2f}\")\n",
    "print(f\"  BIC: {best.bic:.2f}\")\n",
    "print(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.422381Z",
     "iopub.status.busy": "2026-01-01T14:36:34.422211Z",
     "iopub.status.idle": "2026-01-01T14:36:34.584681Z",
     "shell.execute_reply": "2026-01-01T14:36:34.584369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get top 5 by different metrics\n",
    "print(\"Top 5 by K-S statistic (default):\")\n",
    "for i, r in enumerate(results_normal.best(n=5), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 by A-D statistic:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"ad_statistic\"), 1):\n",
    "    ad_p = f\"{r.ad_pvalue:.4f}\" if r.ad_pvalue else \"N/A\"\n",
    "    print(f\"  {i}. {r.distribution:20s} AD={r.ad_statistic:.6f} p={ad_p}\")\n",
    "\n",
    "print(\"\\nTop 5 by SSE:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"sse\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} SSE={r.sse:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 by AIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"aic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} AIC={r.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.585779Z",
     "iopub.status.busy": "2026-01-01T14:36:34.585708Z",
     "iopub.status.idle": "2026-01-01T14:36:34.858089Z",
     "shell.execute_reply": "2026-01-01T14:36:34.857684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter by K-S statistic threshold\n",
    "good_fits = results_normal.filter(ks_threshold=0.05)\n",
    "print(f\"Distributions with K-S statistic < 0.05: {good_fits.count()}\")\n",
    "\n",
    "for r in good_fits.best(n=10):\n",
    "    print(f\"  {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n",
    "\n",
    "# Filter by p-value threshold (keep distributions with p-value > 0.05)\n",
    "significant = results_normal.filter(pvalue_threshold=0.05)\n",
    "print(f\"\\nDistributions with p-value > 0.05: {significant.count()}\")\n",
    "\n",
    "# Filter by A-D statistic threshold\n",
    "good_ad = results_normal.filter(ad_threshold=2.0)\n",
    "print(f\"\\nDistributions with A-D statistic < 2.0: {good_ad.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.859747Z",
     "iopub.status.busy": "2026-01-01T14:36:34.859592Z",
     "iopub.status.idle": "2026-01-01T14:36:34.888481Z",
     "shell.execute_reply": "2026-01-01T14:36:34.888174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for further analysis\n",
    "df_results = results_normal.df.toPandas()\n",
    "print(\"Results as pandas DataFrame:\")\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Using Fitted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.889794Z",
     "iopub.status.busy": "2026-01-01T14:36:34.889734Z",
     "iopub.status.idle": "2026-01-01T14:36:34.932294Z",
     "shell.execute_reply": "2026-01-01T14:36:34.932004Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DistributionFitResult object wraps the scipy.stats distribution\n",
    "best = results_normal.best(n=1)[0]\n",
    "\n",
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "print(f\"Generated {len(samples)} samples from fitted {best.distribution}\")\n",
    "print(f\"  Sample mean: {samples.mean():.2f} (original: {normal_data.mean():.2f})\")\n",
    "print(f\"  Sample std: {samples.std():.2f} (original: {normal_data.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.933456Z",
     "iopub.status.busy": "2026-01-01T14:36:34.933387Z",
     "iopub.status.idle": "2026-01-01T14:36:34.935736Z",
     "shell.execute_reply": "2026-01-01T14:36:34.935407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"PDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Parameter Confidence Intervals\n",
    "\n",
    "Compute bootstrap confidence intervals for fitted distribution parameters. This is useful for understanding the uncertainty in your parameter estimates.\n",
    "\n",
    "**Note**: CI width depends on sample size and distribution identifiability. Highly flexible distributions (like beta with 4 parameters) may have wider CIs due to parameter identifiability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:34.936664Z",
     "iopub.status.busy": "2026-01-01T14:36:34.936610Z",
     "iopub.status.idle": "2026-01-01T14:36:54.522511Z",
     "shell.execute_reply": "2026-01-01T14:36:54.522126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use exponential fit for CI demo (simpler distribution = more stable CIs)\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "\n",
    "print(f\"Distribution: {best_exp.distribution}\")\n",
    "print(f\"Parameter names: {best_exp.get_param_names()}\")\n",
    "print(f\"Fitted values: {[f'{p:.4f}' for p in best_exp.parameters]}\")\n",
    "\n",
    "# Compute 95% bootstrap confidence intervals\n",
    "print(\"\\nComputing 95% confidence intervals (this may take a few seconds)...\")\n",
    "ci = best_exp.confidence_intervals(\n",
    "    df_exp,\n",
    "    column=\"value\",\n",
    "    alpha=0.05,           # 95% CI\n",
    "    n_bootstrap=500,      # Number of bootstrap samples (use 1000+ for production)\n",
    "    random_seed=42,       # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\nParameter confidence intervals:\")\n",
    "for param, (lower, upper) in ci.items():\n",
    "    print(f\"  {param}: [{lower:.4f}, {upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Lazy Metrics for Fast Model Selection (v1.5.0+)\n",
    "\n",
    "When fitting ~100 distributions, computing KS and AD statistics for all of them can be slow. With **lazy metrics**, these expensive computations are skipped during fitting and only computed on-demand when you actually need them.\n",
    "\n",
    "**Key benefits:**\n",
    "- Fast initial fitting (skip KS/AD computation)\n",
    "- On-demand computation only for distributions you access\n",
    "- Ideal for model selection workflows using AIC/BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:54.523666Z",
     "iopub.status.busy": "2026-01-01T14:36:54.523598Z",
     "iopub.status.idle": "2026-01-01T14:36:56.609466Z",
     "shell.execute_reply": "2026-01-01T14:36:56.609078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit with lazy metrics - KS/AD statistics are NOT computed during fitting\n",
    "print(\"Fitting with lazy_metrics=True (fast)...\")\n",
    "fitter_lazy = DistributionFitter(spark)\n",
    "results_lazy = fitter_lazy.fit(\n",
    "    df_normal,\n",
    "    column=\"value\",\n",
    "    max_distributions=20,\n",
    "    lazy_metrics=True,  # Skip KS/AD computation!\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results_lazy.count()} distributions\")\n",
    "print(f\"Is lazy: {results_lazy.is_lazy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:56.610347Z",
     "iopub.status.busy": "2026-01-01T14:36:56.610286Z",
     "iopub.status.idle": "2026-01-01T14:36:56.636944Z",
     "shell.execute_reply": "2026-01-01T14:36:56.636641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get best by AIC - fast! No KS/AD computation needed\n",
    "best_aic = results_lazy.best(n=1, metric=\"aic\")[0]\n",
    "\n",
    "print(f\"Best by AIC: {best_aic.distribution}\")\n",
    "print(f\"  AIC: {best_aic.aic:.2f}\")\n",
    "print(f\"  BIC: {best_aic.bic:.2f}\")\n",
    "print(f\"  KS statistic: {best_aic.ks_statistic}\")  # None - not computed yet!\n",
    "print(f\"  AD statistic: {best_aic.ad_statistic}\")  # None - not computed yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:56.638095Z",
     "iopub.status.busy": "2026-01-01T14:36:56.638038Z",
     "iopub.status.idle": "2026-01-01T14:36:56.889241Z",
     "shell.execute_reply": "2026-01-01T14:36:56.888831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get best by KS statistic - triggers ON-DEMAND computation!\n",
    "# Only computes KS/AD for top candidates, not all distributions\n",
    "best_ks = results_lazy.best(n=1, metric=\"ks_statistic\")[0]\n",
    "\n",
    "print(f\"Best by KS: {best_ks.distribution}\")\n",
    "print(f\"  KS statistic: {best_ks.ks_statistic:.6f}\")  # Computed on-demand!\n",
    "print(f\"  p-value: {best_ks.pvalue:.4f}\")\n",
    "print(f\"  AD statistic: {best_ks.ad_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:56.890286Z",
     "iopub.status.busy": "2026-01-01T14:36:56.890226Z",
     "iopub.status.idle": "2026-01-01T14:36:57.153895Z",
     "shell.execute_reply": "2026-01-01T14:36:57.153587Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you need all metrics computed (e.g., before unpersisting source data),\n",
    "# use materialize() to force computation of all KS/AD statistics\n",
    "materialized = results_lazy.materialize()\n",
    "\n",
    "print(f\"Is lazy after materialize: {materialized.is_lazy}\")  # False\n",
    "\n",
    "# Now all distributions have KS/AD computed\n",
    "top_3 = materialized.best(n=3, metric=\"ks_statistic\")\n",
    "print(\"\\nTop 3 distributions (all metrics available):\")\n",
    "for i, r in enumerate(top_3, 1):\n",
    "    print(f\"  {i}. {r.distribution:15} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Pre-filtering Distributions (v1.6.0+)\n",
    "\n",
    "When you know something about your data, you can skip distributions that are mathematically incompatible. Pre-filtering uses data characteristics (support bounds, skewness, kurtosis) to eliminate distributions before the expensive fitting step.\n",
    "\n",
    "**Filtering layers:**\n",
    "1. **Support bounds (100% reliable)**: Skips distributions whose support doesn't contain your data range\n",
    "2. **Skewness sign (95% reliable)**: Skips positive-skew-only distributions for left-skewed data  \n",
    "3. **Kurtosis (aggressive mode, ~80% reliable)**: Skips low-kurtosis distributions for heavy-tailed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative data (will filter out non-negative distributions like expon, gamma)\n",
    "np.random.seed(42)\n",
    "negative_data = np.random.normal(loc=-50, scale=10, size=10_000)\n",
    "df_negative = spark.createDataFrame([(float(x),) for x in negative_data], [\"value\"])\n",
    "\n",
    "print(f\"Data range: [{negative_data.min():.1f}, {negative_data.max():.1f}]\")\n",
    "print(f\"All values are negative - expon/gamma distributions cannot fit this data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WITHOUT prefilter (baseline)\n",
    "print(\"Fitting WITHOUT prefilter...\")\n",
    "fitter_no_prefilter = DistributionFitter(spark)\n",
    "results_no_prefilter = fitter_no_prefilter.fit(\n",
    "    df_negative, \n",
    "    column=\"value\", \n",
    "    max_distributions=20,\n",
    "    prefilter=False,  # Default - fit all distributions\n",
    ")\n",
    "print(f\"Fitted {results_no_prefilter.count()} distributions (no prefilter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WITH prefilter (safe mode) - skips incompatible distributions\n",
    "print(\"\\nFitting WITH prefilter=True (safe mode)...\")\n",
    "fitter_prefilter = DistributionFitter(spark)\n",
    "results_prefilter = fitter_prefilter.fit(\n",
    "    df_negative,\n",
    "    column=\"value\", \n",
    "    max_distributions=20,\n",
    "    prefilter=True,  # Enable pre-filtering!\n",
    ")\n",
    "print(f\"Fitted {results_prefilter.count()} distributions (with prefilter)\")\n",
    "print(\"\\n-> Pre-filter skipped distributions incompatible with negative data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best fits - both should find norm as best for normal data\n",
    "best_no_prefilter = results_no_prefilter.best(n=1)[0]\n",
    "best_prefilter = results_prefilter.best(n=1)[0]\n",
    "\n",
    "print(\"Best distribution comparison:\")\n",
    "print(f\"  Without prefilter: {best_no_prefilter.distribution} (KS={best_no_prefilter.ks_statistic:.6f})\")\n",
    "print(f\"  With prefilter:    {best_prefilter.distribution} (KS={best_prefilter.ks_statistic:.6f})\")\n",
    "print(\"\\n-> Same best fit, but prefilter was faster by skipping incompatible distributions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Plotting\n",
    "\n",
    "Visualize the fitted distribution with the data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:57.154900Z",
     "iopub.status.busy": "2026-01-01T14:36:57.154842Z",
     "iopub.status.idle": "2026-01-01T14:36:57.306547Z",
     "shell.execute_reply": "2026-01-01T14:36:57.306168Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:57.307896Z",
     "iopub.status.busy": "2026-01-01T14:36:57.307810Z",
     "iopub.status.idle": "2026-01-01T14:36:57.554369Z",
     "shell.execute_reply": "2026-01-01T14:36:57.554035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plot with default config\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    title=\"Best Fit Distribution (Normal Data)\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Plot with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:57.555458Z",
     "iopub.status.busy": "2026-01-01T14:36:57.555380Z",
     "iopub.status.idle": "2026-01-01T14:36:57.769289Z",
     "shell.execute_reply": "2026-01-01T14:36:57.768942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom plot with direct parameters\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=14,\n",
    "    legend_fontsize=12,\n",
    "    grid_alpha=0.4,\n",
    "    title=\"Distribution Fit with Custom Styling\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Plot Non-Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:57.770288Z",
     "iopub.status.busy": "2026-01-01T14:36:57.770228Z",
     "iopub.status.idle": "2026-01-01T14:36:58.028763Z",
     "shell.execute_reply": "2026-01-01T14:36:58.028348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Best fit for exponential data\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "print(f\"Best fit for exponential data: {best_exp.distribution}\")\n",
    "print(f\"  K-S statistic: {best_exp.ks_statistic:.6f}\")\n",
    "print(f\"  p-value: {best_exp.pvalue:.4f}\")\n",
    "\n",
    "fig, ax = fitter_nonneg.plot(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    title=f\"Best Fit: {best_exp.distribution.capitalize()}\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Q-Q Plots for Goodness-of-Fit Assessment\n",
    "\n",
    "A Q-Q (quantile-quantile) plot is a powerful visual tool for assessing how well a distribution fits your data. It plots sample quantiles against theoretical quantiles from the fitted distribution. If the fit is good, points will fall approximately along the diagonal reference line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:58.030092Z",
     "iopub.status.busy": "2026-01-01T14:36:58.030013Z",
     "iopub.status.idle": "2026-01-01T14:36:58.259381Z",
     "shell.execute_reply": "2026-01-01T14:36:58.259004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Q plot for the best fit on normal data\n",
    "fig, ax = fitter.plot_qq(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n",
    "    figsize=(10, 10),\n",
    "    title=\"Q-Q Plot: Normal Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare: Q-Q plot for exponential data\n",
    "fig, ax = fitter_nonneg.plot_qq(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    max_points=1000,\n",
    "    figsize=(10, 10),\n",
    "    title=\"Q-Q Plot: Exponential Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 P-P Plots for Goodness-of-Fit Assessment\n",
    "\n",
    "A P-P (probability-probability) plot compares the empirical cumulative distribution function (CDF) of the sample data against the theoretical CDF of the fitted distribution. It is particularly useful for assessing the fit in the center of the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:58.260716Z",
     "iopub.status.busy": "2026-01-01T14:36:58.260626Z",
     "iopub.status.idle": "2026-01-01T14:36:58.469950Z",
     "shell.execute_reply": "2026-01-01T14:36:58.469628Z"
    }
   },
   "outputs": [],
   "source": [
    "# P-P plot for the best fit on normal data\n",
    "fig, ax = fitter.plot_pp(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n",
    "    figsize=(10, 10),\n",
    "    title=\"P-P Plot: Normal Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare: P-P plot for exponential data\n",
    "fig, ax = fitter_nonneg.plot_pp(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    max_points=1000,\n",
    "    figsize=(10, 10),\n",
    "    title=\"P-P Plot: Exponential Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Multi-Column Fitting\n",
    "\n",
    "Fit multiple columns efficiently in a single operation. This shares Spark overhead (count, sampling, broadcasting) across all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create Multi-Column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:58.471078Z",
     "iopub.status.busy": "2026-01-01T14:36:58.471018Z",
     "iopub.status.idle": "2026-01-01T14:36:58.640827Z",
     "shell.execute_reply": "2026-01-01T14:36:58.640422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with multiple columns (different distributions)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data from different distributions\n",
    "n_rows = 20_000\n",
    "multi_data = [\n",
    "    (\n",
    "        float(np.random.normal(50, 10)),      # normal_col\n",
    "        float(np.random.exponential(5)),      # exp_col\n",
    "        float(np.random.gamma(2.0, 2.0)),     # gamma_col\n",
    "    )\n",
    "    for _ in range(n_rows)\n",
    "]\n",
    "\n",
    "df_multi = spark.createDataFrame(multi_data, [\"normal_col\", \"exp_col\", \"gamma_col\"])\n",
    "print(f\"Created DataFrame with {df_multi.count():,} rows and columns: {df_multi.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fit Multiple Columns in One Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:36:58.642514Z",
     "iopub.status.busy": "2026-01-01T14:36:58.642385Z",
     "iopub.status.idle": "2026-01-01T14:37:01.576368Z",
     "shell.execute_reply": "2026-01-01T14:37:01.575987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit distributions to all columns in a single operation\n",
    "# This is more efficient than fitting each column separately\n",
    "print(\"Fitting distributions to 3 columns simultaneously...\")\n",
    "\n",
    "fitter_multi = DistributionFitter(spark)\n",
    "results_multi = fitter_multi.fit(\n",
    "    df_multi,\n",
    "    columns=[\"normal_col\", \"exp_col\", \"gamma_col\"],  # Multi-column fitting!\n",
    "    max_distributions=15,\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal results: {results_multi.count()}\")\n",
    "print(f\"Columns fitted: {results_multi.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Get Best Distribution Per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:01.577455Z",
     "iopub.status.busy": "2026-01-01T14:37:01.577390Z",
     "iopub.status.idle": "2026-01-01T14:37:01.894325Z",
     "shell.execute_reply": "2026-01-01T14:37:01.893856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the best distribution for each column\n",
    "best_per_col = results_multi.best_per_column(n=1)\n",
    "\n",
    "print(\"Best distribution per column:\")\n",
    "for col_name, fits in best_per_col.items():\n",
    "    best = fits[0]\n",
    "    print(f\"\\n{col_name}:\")\n",
    "    print(f\"  Distribution: {best.distribution}\")\n",
    "    print(f\"  K-S statistic: {best.ks_statistic:.6f}\")\n",
    "    print(f\"  p-value: {best.pvalue:.4f}\")\n",
    "    print(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Filter Results by Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:01.895506Z",
     "iopub.status.busy": "2026-01-01T14:37:01.895446Z",
     "iopub.status.idle": "2026-01-01T14:37:02.039530Z",
     "shell.execute_reply": "2026-01-01T14:37:02.038874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get results for a specific column\n",
    "exp_results = results_multi.for_column(\"exp_col\")\n",
    "\n",
    "print(f\"Results for 'exp_col': {exp_results.count()} distributions\")\n",
    "print(\"\\nTop 5 by K-S statistic:\")\n",
    "for i, r in enumerate(exp_results.best(n=5), 1):\n",
    "    print(f\"  {i}. {r.distribution:15} KS={r.ks_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Plot Results for Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:02.040929Z",
     "iopub.status.busy": "2026-01-01T14:37:02.040832Z",
     "iopub.status.idle": "2026-01-01T14:37:02.630269Z",
     "shell.execute_reply": "2026-01-01T14:37:02.629839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the best fit for each column\n",
    "for col_name, fits in best_per_col.items():\n",
    "    best = fits[0]\n",
    "    fig, ax = fitter_multi.plot(\n",
    "        best,\n",
    "        df_multi,\n",
    "        col_name,\n",
    "        title=f\"{col_name}: {best.distribution} (KS={best.ks_statistic:.4f})\",\n",
    "        xlabel=\"Value\",\n",
    "        ylabel=\"Density\",\n",
    "        figsize=(10, 6),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Complete Workflow Example\n",
    "\n",
    "Putting it all together - a complete production-style workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:02.631499Z",
     "iopub.status.busy": "2026-01-01T14:37:02.631424Z",
     "iopub.status.idle": "2026-01-01T14:37:04.987987Z",
     "shell.execute_reply": "2026-01-01T14:37:04.987376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Complete workflow with all parameters\n",
    "fitter_gamma = DistributionFitter(spark, random_seed=42)\n",
    "\n",
    "# Fit distributions\n",
    "print(\"Fitting gamma distribution data...\")\n",
    "results = fitter_gamma.fit(\n",
    "    df_gamma,\n",
    "    column=\"value\",\n",
    "    bins=100,\n",
    "    use_rice_rule=False,\n",
    "    enable_sampling=True,\n",
    "    max_sample_size=1_000_000,\n",
    "    max_distributions=25,\n",
    ")\n",
    "\n",
    "# Get best result\n",
    "best = results.best(n=1)[0]\n",
    "print(f\"\\nBest distribution: {best.distribution}\")\n",
    "print(f\"K-S statistic: {best.ks_statistic:.6f}\")\n",
    "print(f\"p-value: {best.pvalue:.4f}\")\n",
    "print(f\"SSE: {best.sse:.6f}\")\n",
    "print(f\"Parameters: {[f'{p:.4f}' for p in best.parameters]}\")\n",
    "\n",
    "# Plot with custom parameters\n",
    "fig, ax = fitter_gamma.plot(\n",
    "    best,\n",
    "    df_gamma,\n",
    "    \"value\",\n",
    "    figsize=(14, 9),\n",
    "    dpi=150,\n",
    "    histogram_alpha=0.6,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=16,\n",
    "    title=f\"Gamma Data - Best Fit: {best.distribution.capitalize()}\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Show top 5 results (sorted by K-S statistic for meaningful ranking)\n",
    "print(\"\\nTop 5 distributions:\")\n",
    "df_top5 = results.df.toPandas().sort_values(\"ks_statistic\").head(5)\n",
    "df_top5[[\"distribution\", \"ks_statistic\", \"pvalue\", \"sse\", \"aic\", \"bic\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Serialization\n",
    "\n",
    "Save fitted distributions to disk and reload them later for inference without re-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Save and Load\n",
    "\n",
    "Save a fitted distribution to JSON (default) or pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:04.989135Z",
     "iopub.status.busy": "2026-01-01T14:37:04.989064Z",
     "iopub.status.idle": "2026-01-01T14:37:04.991805Z",
     "shell.execute_reply": "2026-01-01T14:37:04.991441Z"
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the best fit from Part 6\n",
    "print(f\"Saving distribution: {best.distribution}\")\n",
    "print(f\"Parameters: {best.parameters}\")\n",
    "\n",
    "# Save to a temporary directory\n",
    "model_dir = Path(tempfile.mkdtemp())\n",
    "json_path = model_dir / \"best_model.json\"\n",
    "pkl_path = model_dir / \"best_model.pkl\"\n",
    "\n",
    "# Save as JSON (human-readable, default)\n",
    "best.save(json_path)\n",
    "print(f\"\\nSaved to JSON: {json_path}\")\n",
    "print(f\"File size: {json_path.stat().st_size:,} bytes\")\n",
    "\n",
    "# Save as pickle (binary, faster)\n",
    "best.save(pkl_path, format=\"pickle\")\n",
    "print(f\"\\nSaved to pickle: {pkl_path}\")\n",
    "print(f\"File size: {pkl_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:04.992821Z",
     "iopub.status.busy": "2026-01-01T14:37:04.992744Z",
     "iopub.status.idle": "2026-01-01T14:37:04.995405Z",
     "shell.execute_reply": "2026-01-01T14:37:04.995133Z"
    }
   },
   "outputs": [],
   "source": [
    "from spark_bestfit import DistributionFitResult\n",
    "\n",
    "# Load the saved model\n",
    "loaded = DistributionFitResult.load(json_path)\n",
    "\n",
    "print(f\"Loaded distribution: {loaded.distribution}\")\n",
    "print(f\"Parameters: {loaded.parameters}\")\n",
    "print(f\"K-S statistic: {loaded.ks_statistic:.6f}\")\n",
    "print(f\"p-value: {loaded.pvalue:.4f}\")\n",
    "\n",
    "# Verify loaded model works\n",
    "samples = loaded.sample(size=1000, random_state=42)\n",
    "print(f\"\\nGenerated {len(samples)} samples from loaded model\")\n",
    "print(f\"Sample mean: {samples.mean():.2f}\")\n",
    "print(f\"Sample std: {samples.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Data Summary\n",
    "\n",
    "When fitting with `DistributionFitter`, the result includes a `data_summary` field\n",
    "with statistics about the fitted data. This provides lightweight provenance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:04.996432Z",
     "iopub.status.busy": "2026-01-01T14:37:04.996359Z",
     "iopub.status.idle": "2026-01-01T14:37:04.998005Z",
     "shell.execute_reply": "2026-01-01T14:37:04.997735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Access data summary from the loaded model\n",
    "if loaded.data_summary:\n",
    "    summary = loaded.data_summary\n",
    "    print(\"Data Summary (from fitting):\")\n",
    "    print(f\"  Sample size: {summary['sample_size']:,.0f}\")\n",
    "    print(f\"  Min: {summary['min']:.4f}\")\n",
    "    print(f\"  Max: {summary['max']:.4f}\")\n",
    "    print(f\"  Mean: {summary['mean']:.4f}\")\n",
    "    print(f\"  Std: {summary['std']:.4f}\")\n",
    "else:\n",
    "    print(\"No data summary available (result may have been created manually)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 JSON Format\n",
    "\n",
    "The JSON format is human-readable and includes version metadata for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:04.998932Z",
     "iopub.status.busy": "2026-01-01T14:37:04.998880Z",
     "iopub.status.idle": "2026-01-01T14:37:05.000548Z",
     "shell.execute_reply": "2026-01-01T14:37:05.000287Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the JSON content\n",
    "with open(json_path) as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"JSON file content:\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:05.001398Z",
     "iopub.status.busy": "2026-01-01T14:37:05.001347Z",
     "iopub.status.idle": "2026-01-01T14:37:05.002971Z",
     "shell.execute_reply": "2026-01-01T14:37:05.002721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "shutil.rmtree(model_dir)\n",
    "print(f\"Cleaned up temporary directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Excluded Distributions**:\n",
    "   - `DEFAULT_EXCLUDED_DISTRIBUTIONS` - Slow distributions excluded by default\n",
    "   - Pass custom `excluded_distributions` to `DistributionFitter()` to include/exclude\n",
    "\n",
    "2. **SparkSession Management**:\n",
    "   - You create and configure your own SparkSession\n",
    "   - Pass it to `DistributionFitter(spark)` or use active session\n",
    "\n",
    "3. **Fitting**:\n",
    "   - `DistributionFitter.fit()` - Fit distributions to data\n",
    "   - Parameters: `bins`, `use_rice_rule`, `support_at_zero`, `enable_sampling`, etc.\n",
    "   - `max_distributions` parameter to limit fitting scope\n",
    "   - `progress_callback` parameter to monitor long-running fits\n",
    "\n",
    "4. **Progress Tracking**:\n",
    "   - Pass `progress_callback=fn` to `fit()` to receive progress updates\n",
    "   - Callback receives `(completed_tasks, total_tasks, percent_complete)`\n",
    "   - Works with both `DistributionFitter` and `DiscreteDistributionFitter`\n",
    "\n",
    "5. **Results**:\n",
    "   - `results.best(n, metric)` - Get top N by K-S statistic (default), A-D statistic, SSE, AIC, or BIC\n",
    "   - `results.filter(ks_threshold, pvalue_threshold, ad_threshold)` - Filter by goodness-of-fit\n",
    "   - `results.df.toPandas()` - Convert to pandas DataFrame\n",
    "   - `DistributionFitResult.sample()`, `.pdf()`, `.cdf()` - Use fitted distribution\n",
    "   - `DistributionFitResult.get_param_names()` - Get parameter names\n",
    "   - `DistributionFitResult.confidence_intervals()` - Bootstrap confidence intervals\n",
    "\n",
    "6. **Lazy Metrics (v1.5.0+)**:\n",
    "   - `lazy_metrics=True` - Skip KS/AD computation during fitting for faster iteration\n",
    "   - `results.is_lazy` - Check if results have lazy metrics\n",
    "   - `results.best(metric=\"ks_statistic\")` - Triggers on-demand computation for top candidates only\n",
    "   - `results.materialize()` - Force computation of all KS/AD statistics\n",
    "\n",
    "7. **Pre-filtering (v1.6.0+)**:\n",
    "   - `prefilter=True` - Skip distributions incompatible with your data (safe mode)\n",
    "   - `prefilter=\"aggressive\"` - Also filter by kurtosis for heavy-tailed data\n",
    "   - Uses support bounds, skewness sign, and kurtosis to eliminate distributions\n",
    "   - 30-70% fewer distributions to fit with automatic fallback\n",
    "\n",
    "8. **Multi-Column Fitting**:\n",
    "   - `fitter.fit(df, columns=[...])` - Fit multiple columns in one call\n",
    "   - `results.column_names` - List all fitted columns\n",
    "   - `results.for_column(name)` - Filter results to one column\n",
    "   - `results.best_per_column(n, metric)` - Get top N per column\n",
    "\n",
    "9. **Plotting**:\n",
    "   - `fitter.plot()` - Visualize fitted distribution with data histogram\n",
    "   - `fitter.plot_qq()` - Q-Q plot for visual goodness-of-fit assessment\n",
    "   - `fitter.plot_pp()` - P-P plot for assessing fit in the center of distribution\n",
    "   - Customizable with `figsize`, `dpi`, `histogram_alpha`, `pdf_linewidth`, etc.\n",
    "\n",
    "10. **Serialization**:\n",
    "    - `result.save(path)` - Save to JSON (default) or pickle format\n",
    "    - `DistributionFitResult.load(path)` - Load a saved result\n",
    "    - `result.data_summary` - Access fitting statistics for provenance\n",
    "    - JSON format includes version metadata for compatibility\n",
    "\n",
    "11. **Goodness-of-Fit Metrics**:\n",
    "    - **K-S statistic** (default) - Lower is better, measures max distance from empirical CDF\n",
    "    - **A-D statistic** - Lower is better, more sensitive to tails than K-S\n",
    "    - **p-value** - Higher is better (>0.05 suggests good fit)\n",
    "    - **A-D p-value** - Only available for norm, expon, logistic, gumbel_r, gumbel_l\n",
    "    - **SSE** - Sum of squared errors between histogram and fitted PDF\n",
    "    - **AIC/BIC** - Information criteria for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T14:37:05.003781Z",
     "iopub.status.busy": "2026-01-01T14:37:05.003732Z",
     "iopub.status.idle": "2026-01-01T14:37:05.989200Z",
     "shell.execute_reply": "2026-01-01T14:37:05.988245Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
