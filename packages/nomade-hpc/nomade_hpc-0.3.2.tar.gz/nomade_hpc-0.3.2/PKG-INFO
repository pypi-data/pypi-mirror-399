Metadata-Version: 2.4
Name: nomade-hpc
Version: 0.3.2
Summary: A lightweight HPC monitoring and predictive analytics tool
Author-email: Joao Tonini <jtonini@richmond.edu>
Maintainer-email: Joao Tonini <jtonini@richmond.edu>
License-Expression: AGPL-3.0-or-later
Project-URL: Homepage, https://github.com/jtonini/nomade
Project-URL: Documentation, https://github.com/jtonini/nomade#readme
Project-URL: Repository, https://github.com/jtonini/nomade
Project-URL: Issues, https://github.com/jtonini/nomade/issues
Keywords: hpc,monitoring,slurm,cluster,predictive-analytics,machine-learning,anomaly-detection,graph-neural-network
Classifier: Development Status :: 4 - Beta
Classifier: Environment :: Console
Classifier: Intended Audience :: System Administrators
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: POSIX :: Linux
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: System :: Monitoring
Classifier: Topic :: System :: Systems Administration
Classifier: Topic :: Scientific/Engineering
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: click>=8.0
Requires-Dist: toml>=0.10
Requires-Dist: numpy>=1.21
Requires-Dist: pandas>=1.3
Requires-Dist: scipy>=1.7
Provides-Extra: ml
Requires-Dist: scikit-learn>=1.0; extra == "ml"
Requires-Dist: torch>=2.0; extra == "ml"
Requires-Dist: torch-geometric>=2.0; extra == "ml"
Provides-Extra: dashboard
Requires-Dist: jinja2>=3.0; extra == "dashboard"
Provides-Extra: alerts
Provides-Extra: all
Requires-Dist: nomade[dashboard,ml]; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: ruff>=0.1; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Requires-Dist: pre-commit>=3.0; extra == "dev"
Dynamic: license-file

# NÃ˜MADE

**NÃ˜de MAnagement DEvice** â€” A lightweight HPC monitoring and predictive analytics tool.

> *"Travels light, adapts to its environment, and doesn't need permanent infrastructure."*

[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

---

## Overview

NÃ˜MADE is a lightweight, self-contained monitoring and prediction system for HPC clusters. Unlike heavyweight monitoring solutions that require complex infrastructure, NÃ˜MADE is designed to be deployed quickly, run with minimal resources, and provide actionable insights through both real-time alerts and predictive analytics.

### Key Features

- **Real-time Monitoring**: Track disk usage, SLURM queues, node health, license servers, and job metrics
- **Derivative Analysis**: Detect accelerating trends before they become critical (not just threshold alerts)
- **Predictive Analytics**: ML-based job health prediction using similarity networks
- **Actionable Recommendations**: Data-driven defaults and user-specific suggestions
- **3D Visualization**: Interactive network visualization with safe/danger zones
- **Lightweight**: SQLite database, minimal dependencies, no external services required

### Philosophy

NÃ˜MADE is inspired by nomadic principles:
- **Travels light**: Minimal dependencies, single SQLite database, no complex infrastructure
- **Adapts to its environment**: Configurable collectors, flexible alert rules, cluster-agnostic
- **Leaves no trace**: Clean uninstall, no system modifications required (except optional SLURM hooks)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              NÃ˜MADE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      ALERT DISPATCHER                           â”‚    â”‚
â”‚  â”‚             Email Â· Slack Â· Webhook Â· Dashboard                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      ALERT ENGINE                               â”‚    â”‚
â”‚  â”‚       Rules Â· Derivatives Â· Deduplication Â· Cooldowns           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â–¼                                             â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  MONITORING ENGINE  â”‚                â”‚   PREDICTION ENGINE     â”‚     â”‚
â”‚  â”‚  Threshold-based    â”‚                â”‚   Similarity networks   â”‚     â”‚
â”‚  â”‚  Immediate alerts   â”‚                â”‚   17-dim feature space  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚            â”‚                                          â”‚                 â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                               â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         DATA LAYER                              â”‚    â”‚
â”‚  â”‚            SQLite Â· Time-series Â· Job History Â· I/O Samples     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                               â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        COLLECTORS                                â”‚   â”‚
â”‚  â”‚  diskâ”‚slurmâ”‚job_metricsâ”‚iostatâ”‚mpstatâ”‚vmstatâ”‚node_stateâ”‚gpuâ”‚nfs  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Collection Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         NÃ˜MADE Data Collection                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  SYSTEM COLLECTORS (every 60s):                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ disk         â”‚ Filesystem usage (total, used, free, projections)       â”‚  â”‚
â”‚  â”‚ iostat       â”‚ Device I/O: %iowait, utilization, latency               â”‚  â”‚
â”‚  â”‚ mpstat       â”‚ Per-core CPU: utilization, imbalance detection          â”‚  â”‚
â”‚  â”‚ vmstat       â”‚ Memory pressure, swap activity, blocked processes       â”‚  â”‚
â”‚  â”‚ nfs          â”‚ NFS I/O: ops/sec, throughput, RTT, retransmissions      â”‚  â”‚
â”‚  â”‚ gpu          â”‚ NVIDIA GPU: utilization, memory, temperature, power     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  SLURM COLLECTORS (every 60s):                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ slurm        â”‚ Queue state: pending, running, partition stats          â”‚  â”‚
â”‚  â”‚ job_metrics  â”‚ sacct data: CPU/mem efficiency, health scores           â”‚  â”‚
â”‚  â”‚ node_state   â”‚ Node allocation, drain reasons, CPU load, memory        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  JOB MONITOR (every 30s):                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ job_monitor  â”‚ Per-job I/O: NFS vs local writes from /proc/[pid]/io    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  FEATURE VECTOR (17 dimensions for similarity analysis):                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  From sacct:              From iostat:           From vmstat:          â”‚  â”‚
â”‚  â”‚   1. health_score          11. avg_iowait         17. memory_pressure  â”‚  â”‚
â”‚  â”‚   2. cpu_efficiency        12. peak_iowait        18. swap_activity    â”‚  â”‚
â”‚  â”‚   3. memory_efficiency     13. device_util        19. procs_blocked    â”‚  â”‚
â”‚  â”‚   4. used_gpu                                                          â”‚  â”‚
â”‚  â”‚   5. had_swap             From mpstat:                                 â”‚  â”‚
â”‚  â”‚                            14. avg_core_busy                           â”‚  â”‚
â”‚  â”‚  From job_monitor:         15. imbalance_ratio                         â”‚  â”‚
â”‚  â”‚   6. total_write_gb        16. max_core_busy                           â”‚  â”‚
â”‚  â”‚   7. write_rate_mbps                                                   â”‚  â”‚
â”‚  â”‚   8. nfs_ratio                                                         â”‚  â”‚
â”‚  â”‚   9. runtime_minutes                                                   â”‚  â”‚
â”‚  â”‚  10. write_intensity                                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Collector Details

| Collector | Source | Data Collected | Graceful Skip |
|-----------|--------|----------------|---------------|
| `disk` | `shutil.disk_usage` | Filesystem total/used/free, projections | No |
| `slurm` | `squeue`, `sinfo` | Queue depth, partition stats, wait times | No |
| `job_metrics` | `sacct` | Job history, CPU/mem efficiency, health scores | No |
| `iostat` | `iostat -x` | %iowait, device utilization, latency | No |
| `mpstat` | `mpstat -P ALL` | Per-core CPU, imbalance ratio, saturation | No |
| `vmstat` | `vmstat` | Memory pressure, swap, blocked processes | No |
| `node_state` | `scontrol show node` | Node allocation, drain reasons, CPU load | No |
| `gpu` | `nvidia-smi` | GPU util, memory, temp, power | Yes (if no GPU) |
| `nfs` | `nfsiostat` | NFS ops/sec, throughput, RTT | Yes (if no NFS) |
| `job_monitor` | `/proc/[pid]/io` | Per-job NFS vs local I/O attribution | No |

### Two Engines, One System

1. **Monitoring Engine**: Real-time threshold and derivative-based alerts
   - Catches immediate issues (disk full, node down, stuck jobs)
   - Uses first and second derivatives for early warning
   - "Your disk fill rate is *accelerating* â€” full in 3 days, not 10"

2. **Prediction Engine**: Pattern-based ML analytics
   - Catches patterns before they become issues
   - Uses job similarity networks and health prediction
   - "Jobs with your I/O pattern have 72% failure rate"

---

## Monitoring Capabilities

### Disk Storage
- Filesystem usage monitoring (/, /home, /scratch, /project)
- Per-user and per-group quota tracking
- Fill rate calculation and projection
- **Derivative analysis**: Detect accelerating growth before thresholds trigger
- Orphan file and stale data detection
- Localscratch cleanup verification

### SLURM Queue
- Queue depth and wait time tracking
- Stuck and zombie job detection
- Node drain status monitoring
- Fairshare imbalance alerts
- Pending job analysis (why is my job waiting?)
- Job array health monitoring

### Node Health
- Node up/down/drain status
- Hardware error detection (ECC, GPU, disk)
- Temperature monitoring (CPU, GPU)
- NFS mount health
- Service status (slurmctld, slurmd, munge)
- Network connectivity checks

### License Servers
- FlexLM and RLM license tracking
- Real-time availability monitoring
- Usage pattern analysis
- Server connectivity alerts
- Expiration warnings

### Job Metrics
- Per-job resource usage (CPU, memory, GPU)
- I/O patterns (NFS vs local storage)
- Runtime and efficiency metrics
- Collected via SLURM prolog/epilog hooks

---

## Prediction Capabilities

### 17-Dimension Feature Vector

NÃ˜MADE builds job similarity networks using a comprehensive feature vector that captures multiple aspects of job behavior:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Feature Vector Architecture                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  JOB OUTCOME (from sacct):                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  health_score      â”‚ 0.0 (catastrophic) â†’ 1.0 (perfect)             â”‚    â”‚
â”‚  â”‚  cpu_efficiency    â”‚ actual/requested CPU utilization               â”‚    â”‚
â”‚  â”‚  memory_efficiency â”‚ actual/requested memory utilization            â”‚    â”‚
â”‚  â”‚  used_gpu          â”‚ job utilized GPU resources                     â”‚    â”‚
â”‚  â”‚  had_swap          â”‚ job triggered swap usage                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚  I/O BEHAVIOR (from job_monitor):                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  total_write_gb    â”‚ total data written during job                  â”‚    â”‚
â”‚  â”‚  write_rate_mbps   â”‚ peak write throughput                          â”‚    â”‚
â”‚  â”‚  nfs_ratio         â”‚ NFS writes / total writes (0-1)                â”‚    â”‚
â”‚  â”‚  runtime_minutes   â”‚ job duration                                   â”‚    â”‚
â”‚  â”‚  write_intensity   â”‚ GB written per minute                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚  SYSTEM I/O STATE (from iostat, correlated to job runtime):                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  avg_iowait        â”‚ average %iowait during job                     â”‚    â”‚
â”‚  â”‚  peak_iowait       â”‚ maximum %iowait spike                          â”‚    â”‚
â”‚  â”‚  device_util       â”‚ average device utilization                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚  CPU DISTRIBUTION (from mpstat, correlated to job runtime):                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  avg_core_busy     â”‚ average CPU utilization across cores           â”‚    â”‚
â”‚  â”‚  imbalance_ratio   â”‚ std/avg busy (higher = more imbalance)         â”‚    â”‚
â”‚  â”‚  max_core_busy     â”‚ hottest core utilization                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚  MEMORY PRESSURE (from vmstat, correlated to job runtime):                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  memory_pressure   â”‚ composite pressure indicator (0-1)             â”‚    â”‚
â”‚  â”‚  swap_activity     â”‚ peak swap in+out (KB/s)                        â”‚    â”‚
â”‚  â”‚  procs_blocked     â”‚ avg processes blocked on I/O                   â”‚    â”‚ 
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quantitative Similarity Network

- **Raw quantitative metrics**: No arbitrary thresholds or binary labels
- **Non-redundant features**: `vram_gb > 0` implies GPU used (no separate flag)
- **Simpson similarity**: Biogeographical approach on discretized feature bins
- **Continuous health score**: 0 (catastrophic) â†’ 1 (perfect), not binary
- **Time-correlated system state**: iostat/mpstat/vmstat data aligned to job runtime

### Simulation & Validation

- **Generative model**: Learn distributions from empirical data
- **Simulation cloud**: Thousands of synthetic jobs for coverage validation
- **Anomaly detection**: Real jobs outside simulation bounds
- **Temporal drift**: Monitor for model staleness

### Error Analysis & Defaults

- **Type 1 errors** (false alarms): Predicted failure, actually succeeded
- **Type 2 errors** (missed failures): Predicted success, actually failed
- **Threshold optimization**: Balance alert fatigue vs missed problems
- **Data-driven defaults**: "Use localscratch â†’ +23% success rate"

### Visualization

- **3D network visualization**: Three.js interactive display
- **Axes**: NFS Write / Local Write / I/O Wait
- **Safe zone**: Low NFS, high local, low I/O wait (green region)
- **Danger zone**: High NFS, low local, high I/O wait (red region)
- **Real-time tracking**: Watch jobs move through feature space

---

## Derivative Analysis

A key innovation in NÃ˜MADE is the use of first and second derivatives for early warning:

```
VALUE (0th derivative):     "Disk is at 850 GB"
FIRST DERIVATIVE:           "Disk is filling at 15 GB/day"  
SECOND DERIVATIVE:          "Fill rate is ACCELERATING at 3 GB/dayÂ²"
```

### Why Second Derivatives Matter

Traditional threshold alerts only trigger when a value crosses a limit. By monitoring the second derivative (acceleration), NÃ˜MADE can detect:

- **Exponential growth**: Before linear projections underestimate
- **Sudden changes**: Spikes in usage patterns
- **Developing problems**: I/O storms, memory leaks, cascading failures

### Applications

| Metric | Accelerating (dÂ²>0) | Decelerating (dÂ²<0) |
|--------|---------------------|---------------------|
| Disk usage | ! Exponential fill | OK Cleanup in progress |
| Queue depth | ! System issue | OK Draining normally |
| Failure rate | ðŸ”´ Cascading problem | OK Issue resolving |
| NFS latency | ! I/O storm developing | OK Load decreasing |
| Job memory | ! Memory leak / OOM | OK Normal variation |
| GPU temp | ! Cooling issue | OK Throttling working |

---

## Installation

### Requirements

- Python 3.9+
- SQLite 3.35+
- SLURM (for queue and job monitoring)
- sysstat package (iostat, mpstat)
- procps package (vmstat) - usually pre-installed

Optional:
- nvidia-smi (for GPU monitoring)
- nfs-common with nfsiostat (for NFS monitoring)
- Root access (for cgroup metrics)

### System Check

After installation, verify all requirements:

```bash
nomade syscheck
```

Expected output:
```
NÃ˜MADE System Check
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Python:
  OK Version 3.10.12 (requires >=3.9)
  OK Required packages installed
SLURM:
  OK sinfo available
  OK squeue available
  OK sacct available
  OK sstat available
  OK slurmdbd enabled
  OK JobAcctGather configured
System Tools:
  OK iostat available
  OK mpstat available
  OK vmstat available
  â—‹ nvidia-smi not found (no GPU monitoring)
  â—‹ nfsiostat not found (no NFS monitoring)
  OK /proc/[pid]/io accessible
Database:
  OK SQLite available
  OK Database: /var/lib/nomade/nomade.db
  OK Schema version: 2
Config:
  OK Config: /etc/nomade/nomade.toml
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OK All checks passed!
```

### Quick Start

**Try it now (no HPC required):**
```bash
pip install nomade-hpc
nomade demo
```

This generates synthetic data and launches the dashboard at http://localhost:5000

**For production HPC deployment:**
```bash
pip install nomade-hpc
nomade init
nomade collect    # Start data collection
nomade dashboard  # Launch web interface
```

**Or install from source:**
```bash
git clone https://github.com/jtonini/nomade.git
cd nomade
pip install -e .
nomade demo  # Test with synthetic data
```
```

### SLURM Integration (Optional)

For per-job metrics collection, install prolog/epilog hooks:

```bash
# Copy hooks to SLURM configuration
sudo cp scripts/prolog.sh /etc/slurm/prolog.d/nomade.sh
sudo cp scripts/epilog.sh /etc/slurm/epilog.d/nomade.sh

# Update slurm.conf
# Prolog=/etc/slurm/prolog.d/*
# Epilog=/etc/slurm/epilog.d/*

# Restart SLURM
sudo systemctl restart slurmctld
```

---

## Configuration

NÃ˜MADE uses a TOML configuration file:

```toml
# nomade.toml

[general]
cluster_name = "mycluster"
data_dir = "/var/lib/nomade"
log_level = "INFO"

[collectors]
# All collectors enabled by default
# Set enabled = false to disable specific collectors

[collectors.disk]
enabled = true
filesystems = ["/", "/home", "/scratch", "/localscratch"]

[collectors.slurm]
enabled = true
partitions = ["standard", "debug", "gpu", "highmem"]

[collectors.job_metrics]
enabled = true
lookback_hours = 24
min_runtime_seconds = 10

[collectors.iostat]
enabled = true
# devices = ["sda", "nvme0n1"]  # Optional: specific devices only

[collectors.mpstat]
enabled = true
store_per_core = true
store_summary = true

[collectors.vmstat]
enabled = true

[collectors.node_state]
enabled = true
# nodes = ["node001", "node002"]  # Optional: specific nodes only

[collectors.gpu]
enabled = true  # Gracefully skipped if no nvidia-smi

[collectors.nfs]
enabled = true  # Gracefully skipped if no nfsiostat

[monitor]
# Job I/O monitor settings
sample_interval = 30
nfs_paths = ["/home", "/scratch", "/project"]
local_paths = ["/localscratch", "/tmp", "/dev/shm"]
port = 27001

[alerts]
# Alert dispatch configuration
email_enabled = true
email_to = ["admin@example.edu"]
email_from = "nomade@cluster.example.edu"
smtp_host = "smtp.example.edu"

slack_enabled = false
slack_webhook = ""

# Alert thresholds
disk_warning_percent = 85
disk_critical_percent = 95
queue_stuck_days = 7
gpu_temp_warning = 83

[alerts.derivatives]
# Second derivative thresholds
disk_acceleration_warning = 1.0  # GB/dayÂ²
queue_acceleration_warning = 5   # jobs/hourÂ²

[prediction]
# Prediction engine settings
enabled = true
similarity_threshold = 0.85
health_threshold = 0.5
retrain_interval_days = 7

[dashboard]
host = "0.0.0.0"
port = 8080
```

---

## Usage

### Command Line Interface

```bash
# System status overview
nomade status              # Full system status with all metrics
nomade syscheck            # Verify system requirements

# Data collection
nomade collect --once      # Single collection cycle
nomade collect --interval 60   # Continuous collection
nomade collect -C disk,slurm   # Specific collectors only

# Job I/O monitoring
nomade monitor             # Monitor running jobs for I/O
nomade monitor --once      # Single snapshot
nomade monitor -i 30       # 30-second interval

# Analysis
nomade disk /home --hours 24   # Filesystem trend analysis
nomade jobs --user jsmith      # Recent job history
nomade similarity              # Job similarity analysis
nomade similarity --find-similar 12345  # Find similar jobs
nomade similarity --export viz.json     # Export for visualization

# Alerts
nomade alerts              # View recent alerts
nomade alerts --unresolved # Only unresolved alerts
```

### Bash Helper Functions

Source the helper script for convenient shortcuts:

```bash
source ~/nomade/scripts/nomade.sh
nhelp      # Show all commands
```

| Command | Description |
|---------|-------------|
| `nstatus` | Quick status overview |
| `nwatch [s]` | Live status updates (every s seconds) |
| `ndisk PATH` | Filesystem trend analysis |
| `njobs` | Recent job history |
| `nsimilarity` | Job similarity analysis |
| `nalerts` | View alerts |
| `ncollect` | Run data collection |
| `nmonitor` | Job I/O monitoring |
| `nsyscheck` | System requirements check |
| `nlog` | Tail collection log |

### Status Output

```
â•â•â• NÃ˜MADE Status â•â•â•

Filesystems:
  /                    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 51.4% (34.02/66.26 GB)
  /home                [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 51.4% (34.02/66.26 GB)
Queue:
  standard        Running:   4  Pending:  12
  gpu             Running:   2  Pending:   3
I/O:
  CPU iowait:    2.3%
  CPU user/sys:  45.2% / 3.1%
  vda          util: 15.2% write: 1240 KB/s  latency: 4.2ms
CPU Cores:
  Cores:         32
  Avg busy:      48.2%
  Range:         12.0% - 98.5% (spread: 86.5%)
  Imbalance:     0.42 (std/avg)
  Saturated:     4 (>95% busy)
Memory:
  Free:          12.45 GB
  Cache:         48.23 GB
  Swap used:     128 MB
  Pressure:      0.15
Nodes:
  node001         MIXED        CPU: 28/32 (88%)  Mem: 92%  Load: 27.4
  node002         ALLOCATED    CPU: 32/32 (100%) Mem: 98%  Load: 31.2
  node003         DRAIN        CPU: 0/32 (0%)    Mem: 0%   Load: 0.01
    â””â”€ Reason: GPU memory errors - investigating
Collection:
  disk            1440 runs  100% success
  iostat          1440 runs  100% success
  mpstat          1440 runs  100% success
  vmstat          1440 runs  100% success
  slurm           1440 runs  100% success
  job_metrics     1440 runs  100% success
  node_state      1440 runs  100% success
```

### Python API

```python
from nomade import Nomade

# Initialize
nm = Nomade(config_path='nomade.toml')

# Get current disk status
disk_status = nm.collectors.disk.get_status()
for fs in disk_status:
    print(f"{fs['path']}: {fs['used_pct']:.1f}%")
    
# Analyze trends
analysis = nm.analysis.analyze_disk('/scratch')
print(f"Fill rate: {analysis['first_derivative']:.1f} GB/day")
print(f"Acceleration: {analysis['second_derivative']:.2f} GB/dayÂ²")
print(f"Trend: {analysis['trend']}")

# Predict job health
prediction = nm.prediction.predict_job(job_metrics)
print(f"Predicted health: {prediction['health']:.2f}")
print(f"Risk level: {prediction['risk_level']}")
print(f"Recommendations: {prediction['recommendations']}")

# Get recommendations for a user
recs = nm.prediction.recommend_for_user('alice')
for rec in recs:
    print(f"- {rec['message']}")
```

---

## Repository Structure

```
nomade/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ LICENSE                   # AGPL v3
â”œâ”€â”€ pyproject.toml           # Package configuration
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ nomade.toml.example      # Example configuration
â”‚
â”œâ”€â”€ nomade/                  # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py               # Command-line interface
â”‚   â”œâ”€â”€ daemon.py            # Main monitoring daemon
â”‚   â”œâ”€â”€ config.py            # Configuration handling
â”‚   â”‚
â”‚   â”œâ”€â”€ collectors/          # Data collectors
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py          # Base collector class
â”‚   â”‚   â”œâ”€â”€ disk.py          # Disk & quota monitoring
â”‚   â”‚   â”œâ”€â”€ slurm.py         # SLURM queue & jobs
â”‚   â”‚   â”œâ”€â”€ nodes.py         # Node health
â”‚   â”‚   â”œâ”€â”€ licenses.py      # License servers
â”‚   â”‚   â”œâ”€â”€ jobs.py          # Per-job metrics
â”‚   â”‚   â””â”€â”€ network.py       # Network monitoring
â”‚   â”‚
â”‚   â”œâ”€â”€ db/                  # Database layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema.sql       # SQLite schema
â”‚   â”‚   â”œâ”€â”€ models.py        # Data models
â”‚   â”‚   â””â”€â”€ queries.py       # Common queries
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/            # Analysis utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ derivatives.py   # Derivative calculations
â”‚   â”‚   â”œâ”€â”€ projections.py   # Trend projections
â”‚   â”‚   â””â”€â”€ timeseries.py    # Time-series utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ alerts/              # Alert system
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engine.py        # Alert evaluation
â”‚   â”‚   â”œâ”€â”€ rules.py         # Alert rule definitions
â”‚   â”‚   â””â”€â”€ dispatch.py      # Email/Slack/webhook
â”‚   â”‚
â”‚   â”œâ”€â”€ prediction/          # ML prediction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ similarity.py    # Cosine similarity
â”‚   â”‚   â”œâ”€â”€ network.py       # Similarity network
â”‚   â”‚   â”œâ”€â”€ health.py        # Health score prediction
â”‚   â”‚   â”œâ”€â”€ simulation.py    # Simulation model
â”‚   â”‚   â”œâ”€â”€ errors.py        # Type 1/2 error analysis
â”‚   â”‚   â””â”€â”€ recommendations.py  # Defaults generation
â”‚   â”‚
â”‚   â””â”€â”€ viz/                 # Visualization
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ dashboard.py     # Web dashboard
â”‚       â””â”€â”€ static/          # React frontend
â”‚           â”œâ”€â”€ index.html
â”‚           â””â”€â”€ components/
â”‚               â”œâ”€â”€ Network3D.jsx
â”‚               â”œâ”€â”€ DiskStatus.jsx
â”‚               â”œâ”€â”€ QueueStatus.jsx
â”‚               â””â”€â”€ Alerts.jsx
â”‚
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ prolog.sh           # SLURM prolog hook
â”‚   â”œâ”€â”€ epilog.sh           # SLURM epilog hook
â”‚   â””â”€â”€ install_hooks.sh    # Hook installer
â”‚
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_collectors.py
â”‚   â”œâ”€â”€ test_analysis.py
â”‚   â”œâ”€â”€ test_alerts.py
â”‚   â””â”€â”€ test_prediction.py
â”‚
â””â”€â”€ docs/                    # Documentation
    â”œâ”€â”€ installation.md
    â”œâ”€â”€ configuration.md
    â”œâ”€â”€ collectors.md
    â”œâ”€â”€ alerts.md
    â”œâ”€â”€ prediction.md
    â””â”€â”€ api.md
```

---

## Theoretical Background

NÃ˜MADE's prediction engine is inspired by biogeographical network analysis, particularly the work of Vilhena & Antonelli (2015) on mapping biomes using species occurrence data.

### Biogeography â†’ HPC Analogy

| Biogeography | HPC Infrastructure |
|--------------|-------------------|
| Species | Jobs |
| Geographic regions | Resources (nodes, storage) |
| Biomes | Emergent behavior clusters |
| Species ranges | Job resource usage patterns |
| Transition zones | Domain boundaries (CPUâ†”GPU, NFSâ†”local) |

### Key Insight

Just as biogeographical regions emerge from species distribution data rather than being predefined, NÃ˜MADE allows behavior patterns to emerge from job metrics rather than imposing arbitrary categories.

### Dual-View Analysis

1. **Data space**: Jobs as points in feature space, clustered by similarity
2. **Real space**: Jobs mapped to physical resources, showing actual infrastructure usage

---

## Roadmap

### Phase 1: Monitoring Foundation âœ“
- [x] Design architecture
- [x] Define data model
- [x] Implement collectors (disk, SLURM, GPU, NFS, iostat, vmstat, mpstat)
- [x] Implement alert engine
- [x] Basic dashboard

### Phase 2: Prediction Engine âœ“
- [x] Simpson similarity network (Vilhena & Antonelli biogeography method)
- [x] Failure classification (8 classes: SUCCESS, TIMEOUT, FAILED, OOM, etc.)
- [x] Simulation framework (VM-based SLURM simulation)
- [x] Clustering analysis (assortativity, SES.MNTD, neighborhood purity)
- [x] Hotspot detection (failure-correlated feature bins)

### Phase 3: Visualization âœ“
- [x] 3D network visualization (Three.js force-directed layout)
- [x] Interactive dashboard with cluster/network views
- [x] PCA view for emergent patterns
- [x] Clustering quality panel
- [x] ML Risk panel with high-risk job display

### Phase 4: Advanced ML âœ“
- [x] GNN for network-aware prediction (PyTorch Geometric)
- [x] LSTM for temporal pattern detection
- [x] Autoencoder for anomaly detection (100% precision)
- [x] Ensemble methods (weighted voting)
- [x] Model persistence (save/load from database)
- [x] CLI commands (train, predict, report)
- [ ] Real-time scoring hook (SLURM prolog)
- [ ] Continuous learning pipeline

### Phase 5: Community
- [ ] Multi-cluster federation
- [ ] Anonymized data sharing
- [ ] Community benchmarks
- [ ] JOSS/SoftwareX paper submission

---

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Clone and install in development mode
git clone https://github.com/jtonini/nomade.git
cd nomade
python -m venv venv
source venv/bin/activate
pip install -e ".[dev]"

# Run tests
pytest

# Run linting
ruff check .

# Build documentation
cd docs && make html
```

---

## License

NOMADE is dual-licensed:

- **AGPL v3**: Free for academic, educational, and open-source use
- **Commercial License**: Available for proprietary/commercial deployments

See [LICENSE](LICENSE) for details.

---

## Citation

If you use NOMADE in your research, please cite:

```bibtex
@software{nomade2026,
  author = {Tonini, Joao},
  title = {NOMADE: A Lightweight HPC Monitoring and Prediction Tool},
  year = {2026},
  url = {https://github.com/jtonini/nomade}
}
```

---

## Acknowledgments

- Biogeographical network analysis inspired by Vilhena & Antonelli (2015)

---

## Contact

- **Author**: JoÃ£o Tonini
- **Email**: jtonini@richmond.edu
- **Issues**: [GitHub Issues](https://github.com/jtonini/nomade/issues)
