"""
Bootstrap Poisoning Detector — SENTINEL Phase 3 Tier 2

Detects self-reinforcing contamination loops.
Philosophy: Agent ingests own outputs → contamination compounds.

Features:
- Output-to-input chain tracking
- Self-reference detection
- Contamination loop breaking
- Clean data verification

Author: Dmitry Labintsev
Contact: chg@live.ru | @DmLabincev
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
import hashlib


@dataclass
class DataRecord:
    """A data record with provenance"""

    record_id: str
    content_hash: str
    source: str  # "external", "self", "agent"
    timestamp: datetime
    parent_ids: List[str]


@dataclass
class ContaminationChain:
    """A chain of self-referential data"""

    chain_id: str
    records: List[str]
    loop_detected: bool
    contamination_ratio: float


@dataclass
class BootstrapPoisoningResult:
    """Result of bootstrap poisoning detection"""

    poisoning_detected: bool
    contamination_chains: List[ContaminationChain]
    self_reference_ratio: float
    affected_records: Set[str]
    recommendations: List[str]


class BootstrapPoisoningDetector:
    """
    Detects self-reinforcing contamination loops.

    Attack pattern:
    Agent output → stored as training data
    Agent later reads that data
    Uses it to generate more output
    Loop compounds errors/poison

    Usage:
        detector = BootstrapPoisoningDetector()
        detector.record_data(record)
        result = detector.analyze()
    """

    ENGINE_NAME = "bootstrap_poisoning"
    ENGINE_VERSION = "1.0.0"
    IS_PROACTIVE = True

    SELF_REF_THRESHOLD = 0.3  # 30% self-reference is suspicious

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.records: Dict[str, DataRecord] = {}
        self.agent_outputs: Set[str] = set()

    def record_data(self, record: DataRecord):
        """Record a data item with provenance"""
        self.records[record.record_id] = record
        if record.source in ["self", "agent"]:
            self.agent_outputs.add(record.content_hash)

    def is_self_generated(self, content: str) -> bool:
        """Check if content was generated by the agent"""
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
        return content_hash in self.agent_outputs

    def analyze(self) -> BootstrapPoisoningResult:
        """Analyze for bootstrap poisoning"""
        chains = []
        affected = set()

        # Find records that reference agent outputs
        for rec_id, record in self.records.items():
            if record.source == "external":
                continue

            # Trace ancestry
            chain_records = self._trace_chain(rec_id)

            # Check for loops
            loop = len(chain_records) != len(set(chain_records))

            # Check self-reference ratio
            self_refs = sum(
                1
                for r in chain_records
                if self.records.get(
                    r, DataRecord("", "", "", datetime.now(), [])
                ).source
                in ["self", "agent"]
            )
            ratio = self_refs / max(len(chain_records), 1)

            if ratio > self.SELF_REF_THRESHOLD or loop:
                chains.append(
                    ContaminationChain(
                        chain_id=f"chain-{len(chains)}",
                        records=chain_records,
                        loop_detected=loop,
                        contamination_ratio=ratio,
                    )
                )
                affected.update(chain_records)

        # Overall self-reference ratio
        agent_count = sum(
            1 for r in self.records.values() if r.source in ["self", "agent"]
        )
        overall_ratio = agent_count / max(len(self.records), 1)

        poisoning_detected = len(chains) > 0 or overall_ratio > self.SELF_REF_THRESHOLD

        recommendations = []
        if poisoning_detected:
            recommendations.append("Inject verified external data")
            recommendations.append("Break contamination chains")
            recommendations.append("Mark agent outputs to prevent re-ingestion")

        return BootstrapPoisoningResult(
            poisoning_detected=poisoning_detected,
            contamination_chains=chains,
            self_reference_ratio=overall_ratio,
            affected_records=affected,
            recommendations=recommendations,
        )

    def _trace_chain(self, record_id: str, visited: Set[str] = None) -> List[str]:
        """Trace ancestry chain of a record"""
        if visited is None:
            visited = set()

        if record_id in visited:
            return [record_id]  # Loop detected

        visited.add(record_id)
        chain = [record_id]

        record = self.records.get(record_id)
        if record:
            for parent_id in record.parent_ids:
                chain.extend(self._trace_chain(parent_id, visited))

        return chain

    def get_statistics(self) -> Dict[str, Any]:
        return {
            "records_tracked": len(self.records),
            "agent_outputs": len(self.agent_outputs),
            "self_ref_threshold": self.SELF_REF_THRESHOLD,
        }


def create_engine(
    config: Optional[Dict[str, Any]] = None,
) -> BootstrapPoisoningDetector:
    return BootstrapPoisoningDetector(config)
