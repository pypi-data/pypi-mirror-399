"""
Detects tokenizer exploit attacks using malformed tokens

Auto-generated from: strike/attacks/tokenizer_exploit.py
Generated: 2025-12-29T21:24:05.505332
"""

import re
import logging
from typing import Dict, List, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class TokenizerExploitDetectorResult:
    """Detection result."""
    detected: bool
    confidence: float
    matched_patterns: List[str]
    risk_score: float
    explanation: str


class TokenizerExploitDetector:
    """
    Detects tokenizer exploit attacks using malformed tokens
    
    Synced from attack module: strike/attacks/tokenizer_exploit.py
    """
    
    PATTERNS = ["[^\\x00-\\x7F]{5,}", "\\\\u[0-9a-fA-F]{4}", "[\\uFE00-\\uFE0F]"]
    KEYWORDS = ["unicode", "token", "encode", "decode"]
    
    def __init__(self):
        self._compiled = [re.compile(p, re.IGNORECASE) for p in self.PATTERNS]
    
    def analyze(self, text: str) -> TokenizerExploitDetectorResult:
        """Analyze text for tokenizer_exploit attack patterns."""
        text_lower = text.lower()
        matched = []
        
        # Check regex patterns
        for i, pattern in enumerate(self._compiled):
            try:
                if pattern.search(text):
                    matched.append(f"pattern_{i}")
            except re.error:
                pass
        
        # Check keywords
        for keyword in self.KEYWORDS:
            if keyword.lower() in text_lower:
                matched.append(f"keyword:{keyword}")
        
        confidence = min(0.95, 0.3 + len(matched) * 0.15)
        detected = len(matched) >= 2
        
        return TokenizerExploitDetectorResult(
            detected=detected,
            confidence=confidence,
            matched_patterns=matched[:5],
            risk_score=confidence if detected else confidence * 0.5,
            explanation=f"Matched {len(matched)} indicators" if matched else "Clean",
        )


# Singleton
_detector = None

def get_detector() -> TokenizerExploitDetector:
    global _detector
    if _detector is None:
        _detector = TokenizerExploitDetector()
    return _detector

def detect(text: str) -> TokenizerExploitDetectorResult:
    return get_detector().analyze(text)
