import requests 
import pandas as pd 
import numpy as np
import plotly.graph_objects as go
import requests
import yaml
from datetime import datetime
import pandas as pd
import os


# --- INICIO DE TU L√ìGICA DE IMPORTACI√ìN ---
try:
    # Intenta importar desde dentro del paquete (uso normal de librer√≠a)
    from .settings import config
    print("‚úÖ [CuponCero.py] Configuraci√≥n cargada desde settings.py")
except ImportError:
    # Fallback: Esto se ejecuta si corres este archivo solo, o si falta settings.py
    print("‚ö†Ô∏è [CuponCero.py] settings.py no encontrado. Usando MockConfig.")
    
    class MockConfig:
        CACHE_DIR = "./cache_temporal"
        TIMEOUT = 10
        DB_HOST = "localhost"
    
    config = MockConfig()

def obtener_datos_sbs():
    """
    Funci√≥n principal que simula traer datos usando la config.
    """
    print(f"   -> Conectando a: {config.DB_HOST}")
    print(f"   -> Guardando en: {config.CACHE_DIR}")
    print("   -> Datos obtenidos exitosamente.")
    return True







def pivot_curva_cupon_cero_historico(df):
    # 1. Pivotar el DataFrame
    # IMPORTANTE: Agregamos "Fecha Visual" al √≠ndice para no perderla
    pivot_df = df.pivot(index=["Fecha de Proceso (d/m/Y)", "Fecha de Proceso"], 
                        columns="Periodo (d√≠as)", 
                        values="Tasas (%)").reset_index()

    # 2. Renombrar las columnas (quita el nombre de la jerarqu√≠a)
    pivot_df.columns.name = None

    # 3. Definir columnas de metadatos
    non_numeric_columns = ["Fecha de Proceso (d/m/Y)", "Fecha de Proceso"]

    # 4. Identificar columnas num√©ricas (los plazos)
    numeric_columns = [col for col in pivot_df.columns if col not in non_numeric_columns]

    # --- CORRECCI√ìN PRINCIPAL ---
    # Como las columnas ya son n√∫meros (0, 90, 180...), Python sabe ordenarlas directo.
    # No usamos lambda ni split.
    numeric_columns.sort() 
    # ----------------------------

    # 5. Reordenar columnas: Info primero, Plazos despu√©s
    new_columns_order = non_numeric_columns + numeric_columns
    pivot_df = pivot_df[new_columns_order]

    # 6. Convertir las columnas de tasas a num√©ricas (float)
    pivot_df[numeric_columns] = pivot_df[numeric_columns].apply(pd.to_numeric, errors='coerce')

    pivot_df = pivot_df.sort_values(by=["Fecha de Proceso"], ascending=[False])

    return pivot_df




def get_curva_cupon_cero_historico(fechaInicio=None, fechaFin=None, tipoCurva=None, cache=True):
    """
    Obtiene datos de la curva SBS con opci√≥n de cach√© local.
    
    Args:
        fechaInicio (str): Fecha inicial en formato dd/mm/yyyy.
        fechaFin (str): Fecha final en formato dd/mm/yyyy.
        tipoCurva (str): Nombre de la curva (ej. 'TIP_PR_PEN').
        use_cache (bool): Si es True, guarda y lee a√±os pasados localmente.
    """
    
    # 1. CONFIGURACI√ìN DE CACH√â
    CACHE_DIR = config.CACHE_DIR
    if cache and not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

    # 2. VALIDACI√ìN DE FECHAS
    if not all([fechaInicio, fechaFin, tipoCurva]):
        raise TypeError("‚ùå Error: 'fechaInicio', 'fechaFin' y 'tipoCurva' son obligatorios.")

    formato = "%d/%m/%Y"
    try:
        inicio_obj = datetime.strptime(fechaInicio.strip(), formato)
        fin_obj = datetime.strptime(fechaFin.strip(), formato)
        if inicio_obj > fin_obj:
            raise ValueError("La fecha de inicio no puede ser posterior a la fecha de fin.")
    except ValueError as e:
        raise TypeError(f"‚ùå Error en formato de fecha: {str(e)}")

    # 3. IDENTIFICAR A√ëOS Y L√çMITES
    anios_a_procesar = range(inicio_obj.year, fin_obj.year + 1)
    anio_actual = datetime.now().year
    
    dfs_acumulados = []
    base_url = f"https://raw.githubusercontent.com/ecandela/pysbs-peru-data/refs/heads/main/curva/anual/{tipoCurva}"

    # 4. PROCESAMIENTO POR A√ëO
    for anio in anios_a_procesar:
        nombre_archivo = f"{tipoCurva}_{anio}.parquet"
        ruta_local = os.path.join(CACHE_DIR, nombre_archivo)
        df_temp = None

        # Determinar si podemos usar el archivo local
        # Condiciones: Cache activado AND el a√±o es pasado AND el archivo existe
        usar_local = cache and anio < anio_actual and os.path.exists(ruta_local)

        if usar_local:
            print(f"üìÇ [Local] Cargando a√±o {anio}...")
            df_temp = pd.read_parquet(ruta_local)
        else:
            # Descarga de GitHub
            url = f"{base_url}/{nombre_archivo}"
            try:
                msg = "Actualizando a√±o actual" if anio == anio_actual else f"Descargando a√±o {anio}"
                print(f"üåê [GitHub] {msg}...")
                df_temp = pd.read_parquet(url)
                
                # Guardar en cach√© si est√° habilitado y es un a√±o pasado
                if cache and anio < anio_actual:
                    df_temp.to_parquet(ruta_local)
                    print(f"üíæ [Cach√©] A√±o {anio} guardado en disco.")
            except Exception as e:
                print(f"‚ö†Ô∏è No se pudo obtener el a√±o {anio} de GitHub: {e}")

        if df_temp is not None:
            dfs_acumulados.append(df_temp)

    if not dfs_acumulados:
        raise RuntimeError("No se recuperaron datos. Verifique la conexi√≥n o el nombre de la curva.")

    # 5. CONSOLIDACI√ìN Y FILTRADO FINAL
    df_total = pd.concat(dfs_acumulados, ignore_index=True)
    df_total['Fecha de Proceso'] = pd.to_datetime(df_total['Fecha de Proceso'])
    
    mask = (df_total['Fecha de Proceso'] >= inicio_obj) & (df_total['Fecha de Proceso'] <= fin_obj)
    df_final = df_total.loc[mask].copy()
    
    # Formatear la fecha para facilitar la lectura al usuario
    df_final['Fecha de Proceso (d/m/Y)'] = df_final['Fecha de Proceso'].dt.strftime('%d/%m/%Y')

    df_final = df_final.sort_values(by=['Fecha de Proceso','Periodo (d√≠as)'], ascending=[False, True])

    return df_final.reset_index(drop=True)


def get_curva_cupon_cero(fechaProceso=None, tipoCurva=None):
    """
    Descarga el Vector de Precios de Renta Fija de la SBS.
    Realiza el mapeo autom√°tico de c√≥digos (inputs) a descripciones reales (Excel).
    """
    
    # Mapeo de Rating (Imagen image_fda477.png)
    # En este caso el c√≥digo y la descripci√≥n parecen iguales, pero normalizamos por si acaso.
    # Si el usuario manda "A " con espacio, esto ayuda a limpiar.
    # Como la imagen muestra que el valor ES el texto, no necesitamos traducci√≥n compleja,
    # pero s√≠ asegurarnos que coincida exactamente.

    # 1. VALIDACI√ìN OBLIGATORIA DE FECHA
    if not fechaProceso or not isinstance(fechaProceso, str):
        raise TypeError("‚ùå Error: 'fechaProceso' es obligatorio y debe ser texto (formato dd/mm/yyyy).")
    
    if not tipoCurva or not isinstance(tipoCurva, str):
        raise TypeError("‚ùå Error: 'tipoCurva' es obligatorio.")

    # Limpieza de la fecha
    fecha_clean = fechaProceso.strip()

    formato = "%d/%m/%Y"

    try:
        # Intenta convertir la cadena a objeto datetime
        fecha_obj = datetime.strptime(fecha_clean, formato)
        
        # Si pasa la l√≠nea anterior, es v√°lida. Obtenemos el a√±o:
        anio = fecha_obj.year
        print(f"La fecha es v√°lida. El a√±o es: {anio}")
    except ValueError:
        raise TypeError("‚ùå Error: la fecha no es v√°lida o no cumple el formato d/m/Y.")

    # 2. CONSTRUCCI√ìN DE LA URL

    base_url = f"https://raw.githubusercontent.com/ecandela/pysbs-peru-data/refs/heads/main/curva/anual/{tipoCurva}"    
    nombre_archivo = f"{tipoCurva}_{anio}.parquet"
    url = f"{base_url}/{nombre_archivo}"
  
    try:
        print(f"1. Descargando: {nombre_archivo}...")

        df = pd.read_parquet(url)      
        df['Fecha de Proceso (d/m/Y)'] = df['Fecha de Proceso'].dt.strftime('%d/%m/%Y')
        # --- 4. L√ìGICA DE FILTRADO CON TRADUCCI√ìN (MAPEO) ---
        df = df[df['Fecha de Proceso (d/m/Y)'] == fecha_clean].copy()        
        
        # 5. VERIFICACI√ìN FINAL
        if df.empty:
            print("‚ö†Ô∏è Aviso: La consulta devolvi√≥ 0 filas. Verifique si los c√≥digos de filtro son correctos para esta fecha.")
            
        return df.reset_index(drop=True)

    except Exception as e:
        raise RuntimeError(f"Error procesando el vector de precios: {str(e)}") from e


def plot_curva(df):

    df_cup_por_anio = df[df['Periodo (d√≠as)'] % 360 == 0].copy()

    df_cup_por_anio["anio"] = df_cup_por_anio['Periodo (d√≠as)'] / 360 

    fig = go.Figure(data=go.Scatter(x=df_cup_por_anio.anio, y=df_cup_por_anio["Tasas (%)"], mode='lines+markers',    name='lines+markers'))

    # Obtener los l√≠mites m√≠nimo y m√°ximo de los datos en el eje Y
    y_min = df_cup_por_anio["Tasas (%)"].min()
    y_max = df_cup_por_anio["Tasas (%)"].max()

    # Calcular los 8 valores equidistantes entre el l√≠mite m√≠nimo y m√°ximo
    y_values = [y_min + (i * (y_max - y_min) / 7) for i in range(8)]

    fig.update_layout(
                    xaxis_title='A√±os',
                    yaxis_title='Tasas',       
                    yaxis=dict(tickmode='array',  tickvals=y_values, nticks=8, tickfont=dict(size=12), hoverformat='.2f'),         
                    xaxis=dict(type='category', tickfont=dict(size=12), tickangle=90),
                    margin=dict(l=20, r=10, t=20, b=10)
                    )

    fig.show()



def get_pronostico_lineal(conjunto_x,conjunto_y, var_indep ):
    x = var_indep
    f = np.polyfit(conjunto_x, conjunto_y, 1)
    a = f[0]
    b = f[1]
    pronostico = a * x + b

    return pronostico


def get_tasa_interes_por_dias(dias ,df_tasas):

    lb_dias = "Periodo (d√≠as)"
    ld_tasas = "Tasas (%)"

    # Valor "x" que deseas buscar
    x = dias
    y = None
    # Si "x" coincide con uno de los valores en la columna "d√≠as", entonces se muestra el registro correspondiente
    matching_record = df_tasas[df_tasas[lb_dias] == x]
    if len(matching_record)>0:

        #print(matching_record)
        y = matching_record.loc[: , ld_tasas].values[0]

    else:        
        # Encontrar el registro inferior y el registro superior m√°s pr√≥ximos a "x" en la columna "d√≠as"
        lower_record = df_tasas[df_tasas[lb_dias] <= x].tail(1)
        upper_record = df_tasas[df_tasas[lb_dias] >= x].head(1)

        result = pd.concat([lower_record, upper_record])

        conjunto_x = result[lb_dias]
        conjunto_y = result[ld_tasas]
        
        y = get_pronostico_lineal(conjunto_x,conjunto_y,x)

        #print(result)
    return y        



############# DEPRECATED ###############

def es_cache_curva_historica_desactualizada(tipoCurva,maxima_fehca_cache_local):
    # Tu URL raw de GitHub
    url = "https://raw.githubusercontent.com/ecandela/pysbs-peru-data/refs/heads/main/config.yaml"

    try:
        # 1. Hacemos la petici√≥n GET para obtener el contenido texto del archivo
        response = requests.get(url)
        response.raise_for_status() # Lanza un error si la descarga falla (ej. 404)

        # 2. Parseamos el contenido de texto a un diccionario de Python
        # Usamos safe_load para mayor seguridad
        config = yaml.safe_load(response.text)

        str_fecha_max_nube = config['curva_historica'][tipoCurva]
        fecha_max_nube = datetime.strptime(str_fecha_max_nube, '%d/%m/%Y')

        es_superior = fecha_max_nube.date() > maxima_fehca_cache_local.date()
        return es_superior
        
    except requests.exceptions.RequestException as e:
        print(f"Error al descargar el archivo: {e}")
    except yaml.YAMLError as e:
        print(f"Error al procesar el archivo YAML: {e}")
    except KeyError as e:
        print(f"Error: La clave {e} no existe en el archivo YAML.")

#https://www.sbs.gob.pe/app/pp/n_CurvaSoberana/CurvaSoberana/ConsultaHistorica
def get_curva_cupon_cero_historico_deprecated(fechaInicio=None, fechaFin=None, tipoCurva=None, 
                                    cache=True, ruta_personalizada=None):
    """
    Descarga, procesa y filtra la curva cup√≥n cero.
    - cache (bool): Si es True, busca primero en disco local.
    - ruta_personalizada (str): Ruta opcional para guardar/leer este archivo espec√≠fico.
      Si es None, usa la ruta definida en la configuraci√≥n global (config.CACHE_DIR).
    """
    
    # 1. LISTA BLANCA DE C√ìDIGOS V√ÅLIDOS
    codigos_permitidos = [
        "CBCRPS", "CBCRS", "CCCLD", "CCINFS", 
        "CCPEDS", "CCPSS", "CCPVS", "CCSDF", "CSBCRD"
    ]
    
    # 2. VALIDACI√ìN Y LIMPIEZA DE PARAMETRO
    if not isinstance(tipoCurva, str):
        raise TypeError(f"‚ùå Error: El TipoCurva debe ser texto. Recibido: {tipoCurva}")
        
    codigo_clean = tipoCurva.upper().strip()
    
    if codigo_clean not in codigos_permitidos:
        raise ValueError(f"‚ùå Error: El tipo '{tipoCurva}' no existe.\n   Opciones v√°lidas: {codigos_permitidos}")

    # --- CONFIGURACI√ìN DE CARPETA CACH√â (JERARQU√çA) ---
    # Prioridad 1: Argumento de la funci√≥n
    if ruta_personalizada:
        carpeta_cache = ruta_personalizada
    # Prioridad 2: Configuraci√≥n Global (settings.py / variables de entorno)
    else:
        carpeta_cache = config.CACHE_DIR
    
    # Crea la carpeta si no existe
    # Nota: Si config.CACHE_DIR usa la ruta del usuario (~/.sbs_helper), esto funcionar√° sin permisos de admin.
    os.makedirs(carpeta_cache, exist_ok=True)

    # Nombre del archivo y ruta completa
    nombre_archivo = f"resultado_curva_cupon_cero_historico_{codigo_clean}.parquet"
    ruta_cache = os.path.join(carpeta_cache, nombre_archivo)

    # --- INTENTO DE CARGA DESDE CACH√â ---
    if cache:
        if os.path.exists(ruta_cache):
            print(f"‚ö° CACH√â DETECTADO (Parquet): Cargando datos desde '{ruta_cache}'...")
            try:
                # read_parquet mantiene los tipos de datos intactos
                df_cache = pd.read_parquet(ruta_cache)

                maxima_fehca_cache_local = df_cache['Fecha de Proceso'].max()
                esta_desactualizado = es_cache_curva_historica_desactualizada(tipoCurva,maxima_fehca_cache_local)
                if esta_desactualizado ==False:
                    print(f"‚ö†Ô∏è cach√© actualizada...")
                    # --- FILTRADO ---
                    if fechaInicio:
                        fi = pd.to_datetime(fechaInicio, dayfirst=True)
                        print(f"   -> Filtrando desde: {fi.date()}")
                        df_cache = df_cache[df_cache['Fecha de Proceso'] >= fi]

                    if fechaFin:
                        ff = pd.to_datetime(fechaFin, dayfirst=True)
                        print(f"   -> Filtrando hasta: {ff.date()}")
                        df_cache = df_cache[df_cache['Fecha de Proceso'] <= ff]

                    return df_cache
                else:
                    print(f"‚ö†Ô∏è La cach√© para el tipo de curva {tipoCurva} est√° desactualizada. Se proceder√° a descargar la versi√≥n m√°s reciente.")
            except Exception as e:
                print(f"‚ö†Ô∏è Error leyendo cach√© (se proceder√° a descargar): {e}")
        else:
            print(f"‚ÑπÔ∏è Modo cach√© activado, pero el archivo no existe en: {ruta_cache}")
            print("   -> Se proceder√° a descargar.")

    # 3. CONSTRUCCI√ìN DE LA URL DIN√ÅMICA
    base_url = "https://raw.githubusercontent.com/ecandela/pysbs-peru-data/main/curva_historica"
    nombre_archivo_web = f"curva_historica_{codigo_clean}.xlsx"
    url = f"{base_url}/{nombre_archivo_web}"

    try:
        print("1. Descargando archivo desde GitHub...")
        df_raw = pd.read_excel(url, engine='openpyxl') 
        
        # --- LIMPIEZA ---
        df = df_raw.iloc[1:].reset_index(drop=True)
        df.columns = df_raw.iloc[0]
        df.columns = df.columns.astype(str).str.strip()
        
        print(f"2. Filas totales descargadas: {len(df)}")
        
        if 'Fecha de Proceso' not in df.columns:
             raise KeyError(f"‚ùå Error: No existe la columna 'Fecha de Proceso'. Columnas: {df.columns.tolist()}")

        df['Fecha de Proceso'] = pd.to_datetime(df['Fecha de Proceso'], dayfirst=True, errors='coerce')
        df = df.dropna(subset=['Fecha de Proceso'])
        
        if df.empty:
            raise ValueError("‚ö†Ô∏è ALERTA: El DataFrame se qued√≥ vac√≠o despu√©s de limpiar fechas incorrectas.")


            
        print(f"6. Filas finales despu√©s del filtro: {len(df)}")
        
        # --- FINALIZAR ---
        df = df.sort_values('Fecha de Proceso').reset_index(drop=True)
        df['Fecha Visual'] = df['Fecha de Proceso'].dt.strftime('%d/%m/%Y')
        
        cols = ['Fecha Visual', 'Fecha de Proceso'] + [c for c in df.columns if c not in ['Fecha Visual', 'Fecha de Proceso']]
        
        df['Tasas (%)'] = pd.to_numeric(df['Tasas (%)'], errors='coerce')
        df['Plazo (DIAS)'] = pd.to_numeric(df['Plazo (DIAS)'], errors='coerce')
        
        df = df.sort_values(by=['Fecha de Proceso','Plazo (DIAS)'], ascending=[True, True])
        df_final = df[cols]

        # --- GUARDADO AUTOM√ÅTICO (PERSISTENCIA PARQUET) ---
        print(f"üíæ Guardando resultado (Parquet) en: {ruta_cache}")
        df_final.to_parquet(ruta_cache, index=False)

        # --- FILTRADO ---
        if fechaInicio:
            fi = pd.to_datetime(fechaInicio, dayfirst=True)
            print(f"   -> Filtrando desde: {fi.date()}")
            df_final = df_final[df_final['Fecha de Proceso'] >= fi]

        if fechaFin:
            ff = pd.to_datetime(fechaFin, dayfirst=True)
            print(f"   -> Filtrando hasta: {ff.date()}")
            df_final = df_final[df_final['Fecha de Proceso'] <= ff]


        return df_final

    except Exception as e:
        raise RuntimeError(f"Error en el proceso: {str(e)}") from e