Metadata-Version: 2.4
Name: enkaliprime-cli
Version: 0.6.1
Summary: Command-line interface for EnkaliPrime Chat API
Author-email: EnkaliPrime <support@enkaliprime.com>
Maintainer-email: EnkaliPrime <support@enkaliprime.com>
License-Expression: MIT
Project-URL: Homepage, https://enkaliprime.com
Project-URL: Documentation, https://api.enkaliprime.com/docs
Project-URL: Repository, https://github.com/enkaliprime/python-sdk
Project-URL: Issues, https://github.com/enkaliprime/python-sdk/issues
Keywords: cli,chat,ai,enkaliprime,chatbot,command-line
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: End Users/Desktop
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Communications :: Chat
Classifier: Topic :: Utilities
Classifier: Typing :: Typed
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: enkaliprime>=1.2.0
Requires-Dist: typer>=0.9.0
Requires-Dist: rich>=13.0.0
Requires-Dist: keyring>=24.0.0
Requires-Dist: click>=8.0.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: requests>=2.25.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: ruff>=0.0.280; extra == "dev"
Dynamic: license-file

<div align="center">

# üñ•Ô∏è EnkaliPrime CLI

<p align="center">
  <strong>Beautiful Command-Line Interface for AI Chat</strong>
</p>

<p align="center">
  <em>Chat with AI directly from your terminal with rich formatting and beautiful animations</em>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/">
    <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.8+">
  </a>
  <a href="https://pypi.org/project/enkaliprime-cli/">
    <img src="https://img.shields.io/pypi/v/enkaliprime-cli?style=for-the-badge&logo=pypi&logoColor=white" alt="PyPI Version">
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-22C55E?style=for-the-badge" alt="MIT License">
  </a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/interactive_chat-supported-00D4AA?style=flat-square" alt="Interactive Chat">
  <img src="https://img.shields.io/badge/local_llm-ollama-FF6B35?style=flat-square" alt="Local LLM">
  <img src="https://img.shields.io/badge/rich_formatting-enabled-00D4AA?style=flat-square" alt="Rich Formatting">
  <img src="https://img.shields.io/badge/secure_keyring-encrypted-00D4AA?style=flat-square" alt="Secure Keyring">
</p>

---

**[Installation](#-installation) ‚Ä¢ [Quick Start](#-quick-start) ‚Ä¢ [Commands](#-commands) ‚Ä¢ [Examples](#-examples)**

</div>

---

## ‚ú® Features

<table>
<tr>
<td width="50%">

### üí¨ Interactive AI Chat
- **Real-time conversation** with AI assistants
- **Rich markdown formatting** for responses
- **Beautiful Unicode animations** while thinking
- **Session persistence** across chats

</td>
<td width="50%">

### üè† Local LLM Support
- **Ollama integration** for local AI models
- **Zero API costs** for local inference
- **Privacy-focused** - conversations stay local
- **Multiple model support** (Llama2, CodeLlama, Mistral, etc.)

</td>
</tr>
<tr>
<td width="50%">

### üé® Beautiful Terminal UI
- **Rich console output** with colors and formatting
- **Loading animations** with brain emoji sequences
- **Interactive prompts** with validation
- **Progress indicators** and status displays

</td>
<td width="50%">

### üîê Secure Configuration
- **Encrypted API key storage** using system keyring
- **Automatic validation** of API keys
- **Connection testing** to verify setup
- **Ollama model configuration** and management

</td>
</tr>
<tr>
<td width="50%">

### üìù Session Management
- **Create named sessions** for different contexts
- **Session history** and conversation tracking
- **Multiple agents** support
- **Clean session lifecycle** management

</td>
<td width="50%">

### üéØ Smart Provider Switching
- **Automatic provider detection** and switching
- **Fallback mechanisms** for reliability
- **Unified interface** for local and remote AI
- **Simple --local flag** for easy local LLM access

</td>
</tr>
</table>

---

## üèóÔ∏è Architecture

```mermaid
graph TD
    A[üñ•Ô∏è Terminal] --> B[enkaliprime CLI]
    B --> C{Commands}
    C --> D[üí¨ chat]
    C --> E[‚öôÔ∏è config]
    C --> F[üìù session]

    D --> G[Interactive Chat]
    D --> H[Single Message]
    D --> I[History View]

    E --> J[API Key Config]
    E --> K[Ollama Config]
    E --> L[Test Connection]
    E --> M[Show Config]

    F --> N[Create Session]
    F --> O[End Session]
    F --> P[Session Info]

    B --> Q[üîê Keyring]
    B --> R[üì¶ SDK]
    R --> S{Provider Router}
    S --> T[ü§ñ EnkaliPrime API]
    S --> U[üè† Local Ollama]

    Q --> V[(API Keys)]
    Q --> W[(Ollama Models)]
```

---

## üì¶ Installation

### From PyPI (Recommended)

```bash
pip install enkaliprime-cli
```

### From Source

```bash
git clone https://github.com/enkaliprime/enkaliprime-cli.git
cd enkaliprime-cli
pip install -e .
```

### Requirements

- Python 3.8+
- `enkaliprime` SDK (automatically installed)
- System keyring support (built-in on most systems)
- **Optional**: [Ollama](https://ollama.ai/) for local LLM support

---

## üè† Local LLM Support (Ollama)

EnkaliPrime CLI supports **local AI models** through Ollama integration, allowing you to run AI conversations **without API costs** and with **complete privacy**.

### Quick Local Setup

1. **Install Ollama:**
   ```bash
   # Download from https://ollama.ai/
   # Or use package manager:
   # Linux: curl -fsSL https://ollama.ai/install.sh | sh
   # macOS: brew install ollama
   # Windows: winget install Ollama.Ollama
   ```

2. **Start Ollama:**
   ```bash
   ollama serve
   ```

3. **Pull a model:**
   ```bash
   ollama pull llama2      # General purpose
   ollama pull codellama   # Code assistant
   ollama pull mistral     # Fast and capable
   ```

4. **Configure CLI:**
   ```bash
   enkaliprime config set-ollama-model
   # Interactive model selection
   ```

5. **Start using local AI:**
   ```bash
   enkaliprime chat interactive --local
   enkaliprime chat ask "Hello local AI!" --local
   ```

### Local vs Remote Comparison

| Feature | Remote (EnkaliPrime) | Local (Ollama) |
|---------|---------------------|----------------|
| **Cost** | API usage fees | Free (one-time setup) |
| **Privacy** | Cloud processing | Local processing |
| **Speed** | Network dependent | Local inference |
| **Models** | Curated selection | Any Ollama model |
| **Setup** | API key only | Ollama installation |
| **Offline** | ‚ùå Requires internet | ‚úÖ Works offline |

---

## üöÄ Quick Start

### Option A: Remote AI (EnkaliPrime API)

#### 1. Configure API Key

```bash
enkaliprime config set-api-key
# Follow the secure prompt to enter your API key
```

#### 2. Test Connection

```bash
enkaliprime config test-connection
# Verify everything is working
```

#### 3. Start Chatting

```bash
enkaliprime chat interactive
# Begin your AI conversation!
```

### Option B: Local AI (Ollama)

#### 1. Install and Setup Ollama

```bash
# Install Ollama (visit https://ollama.ai/)
ollama serve
ollama pull llama2
```

#### 2. Configure Default Model

```bash
enkaliprime config set-ollama-model
# Interactive model selection from available Ollama models
```

#### 3. Start Local Chat

```bash
enkaliprime chat interactive --local
# Chat with your local AI model!
```

### Hybrid Usage

You can use both remote and local AI:

```bash
# Remote AI (default)
enkaliprime chat interactive

# Local AI (when configured)
enkaliprime chat interactive --local

# Check available providers
enkaliprime chat providers
```

---

## üìñ Commands

### Main Commands

| Command | Description |
|---------|-------------|
| `enkaliprime --help` | Show help and available commands |
| `enkaliprime info` | Show CLI information and features |
| `enkaliprime --version` | Show version information |

### üí¨ Chat Commands

| Command | Description |
|---------|-------------|
| `enkaliprime chat interactive` | Start interactive chat session (remote AI) |
| `enkaliprime chat interactive --local` | Start interactive chat with local Ollama model |
| `enkaliprime chat ask "message"` | Send single message and get response |
| `enkaliprime chat ask "message" --local` | Send message to local Ollama model |
| `enkaliprime chat providers` | List available AI providers and models |
| `enkaliprime chat history` | Show conversation history |

### ‚öôÔ∏è Configuration Commands

| Command | Description |
|---------|-------------|
| `enkaliprime config set-api-key` | Set your EnkaliPrime API key securely |
| `enkaliprime config get-api-key` | Show masked API key |
| `enkaliprime config remove-api-key` | Remove stored API key |
| `enkaliprime config test-connection` | Test EnkaliPrime API connection |
| `enkaliprime config set-ollama-model` | Configure default Ollama model |
| `enkaliprime config get-ollama-model` | Show current Ollama model setting |
| `enkaliprime config remove-ollama-model` | Remove Ollama model configuration |
| `enkaliprime config show-ollama` | Show Ollama status and available models |
| `enkaliprime config show` | Show current configuration |

### üìù Session Commands

| Command | Description |
|---------|-------------|
| `enkaliprime session create` | Create new chat session |
| `enkaliprime session current` | Show current session info |
| `enkaliprime session end` | End current session |
| `enkaliprime session list` | List all sessions |
| `enkaliprime session clear-history` | Clear conversation history |

---

## üí° Examples

### Interactive Chat Session

```bash
# Remote AI with custom agent
enkaliprime chat interactive --agent "Code Assistant"

# Local AI with configured Ollama model
enkaliprime chat interactive --local

# Local AI with specific model override
enkaliprime chat interactive --local --model codellama

# Remote AI without loading animations
enkaliprime chat interactive --no-loading
```

### Single Message Queries

```bash
# Remote AI - quick question
enkaliprime chat ask "What is the capital of France?"

# Local AI - private, no-cost query
enkaliprime chat ask "Explain quantum computing" --local

# Local AI with specific model
enkaliprime chat ask "Write a Python function" --local --model codellama

# Remote AI with custom agent
enkaliprime chat ask "Explain recursion" --agent "Programming Tutor"
```

### Configuration Management

```bash
# EnkaliPrime API Configuration
enkaliprime config set-api-key
enkaliprime config test-connection
enkaliprime config show

# Ollama Local AI Configuration
enkaliprime config set-ollama-model     # Interactive model selection
enkaliprime config get-ollama-model     # Show current model
enkaliprime config show-ollama          # Show Ollama status & models
enkaliprime config remove-ollama-model  # Remove model config

# Provider Management
enkaliprime chat providers              # List available AI providers
```

### Session Management

```bash
# Create a new session with custom agent
enkaliprime session create --name "Math Tutor" --avatar "üéì"

# Check current session
enkaliprime session current

# End the current session
enkaliprime session end

# Clear conversation history
enkaliprime session clear-history
```

---

## üîß Troubleshooting

### Command Not Found Error

If you get `"enkaliprime" command not found` after installation:

#### Windows (PowerShell/Command Prompt)

**Check if Python Scripts directory is in PATH:**
```cmd
where python
# This should show your Python installation directory
```

**Add Python Scripts to PATH (run as Administrator):**
```cmd
setx PATH "%PATH%;C:\Python313\Scripts"  # Replace with your Python version
```

**Or use full path:**
```cmd
& "C:\Users\%USERNAME%\AppData\Roaming\Python\Python313\Scripts\enkaliprime.exe" --help
```

#### Linux/macOS

**Check PATH:**
```bash
echo $PATH
which python3
```

**Add to PATH in ~/.bashrc or ~/.zshrc:**
```bash
export PATH="$HOME/.local/bin:$PATH"  # For user installations
# OR
export PATH="/usr/local/bin:$PATH"     # For system installations
```

#### Alternative Usage

**Use Python module directly (recommended for PowerShell):**
```bash
python -m enkaliprime_cli info
python -c "from enkaliprime_cli.main import app; import sys; sys.argv = ['enkaliprime', 'chat', 'interactive']; app()"
```

**Use pipx for isolated installation:**
```bash
pip install pipx
pipx install enkaliprime-cli
# Now enkaliprime command works globally
```

### PowerShell Compatibility

If you experience glitches with loading animations in PowerShell:

1. **Disable progress display:**
   ```powershell
   $ProgressPreference = "SilentlyContinue"
   enkaliprime chat ask "Hello"
   ```

2. **Use Python directly:**
   ```powershell
   python -c "from enkaliprime_cli.main import app; import sys; sys.argv = ['enkaliprime', 'chat', 'ask', 'Hello']; app()"
   ```

3. **Loading animations are disabled by default** in CLI commands to avoid PowerShell conflicts. The SDK still shows its internal brain animation.

### Ollama Setup Issues

**"Ollama not available" error:**

1. **Install Ollama:**
   ```bash
   # Visit https://ollama.ai/ for installation instructions
   # Windows: winget install Ollama.Ollama
   # macOS: brew install ollama
   # Linux: curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Start Ollama service:**
   ```bash
   ollama serve
   # Keep this running in a separate terminal
   ```

3. **Pull a model:**
   ```bash
   ollama pull llama2  # General purpose model
   ollama pull codellama  # Code-focused model
   ```

4. **Configure CLI:**
   ```bash
   enkaliprime config set-ollama-model
   # Select your preferred model
   ```

**Test Ollama connection:**
```bash
enkaliprime config show-ollama
# Should show available models and ‚úÖ status
```

**Common Ollama issues:**
- **Port conflict**: Default port 11434 may be in use
- **Memory**: Large models need sufficient RAM
- **Storage**: Models require disk space (2-7GB each)

### API Key Issues

**"No API key configured" error:**
```bash
enkaliprime config set-api-key
# Enter your API key when prompted
```

**Test connection:**
```bash
enkaliprime config test-connection
```

---

## üîê Security

- **Encrypted Storage**: API keys are stored securely using your system's keyring
- **No Plain Text**: Keys are never displayed in full, only masked
- **Secure Prompts**: Password-style input for sensitive information
- **Validation**: Automatic format checking for API keys
- **Local AI Privacy**: Ollama models process conversations locally with zero data transmission

### Best Practices

```bash
# ‚úÖ Good: Use interactive setup
enkaliprime config set-api-key

# ‚ùå Bad: Expose key in command history
enkaliprime config set-api-key --key ek_bridge_secret_key
```

---

## üé® Terminal Features

### Rich Formatting
- **Markdown rendering** for AI responses
- **Syntax highlighting** for code blocks
- **Colored output** for better readability
- **Unicode emojis** for visual appeal

### Loading Animations
```
üß† Thinking... 1.2s
üß† Thinking... 2.8s
üí≠ Thinking... 3.1s
üí° Thinking... 3.7s
‚ú® Thinking... 4.2s
```

### Interactive Prompts
- **Auto-completion** for commands and options
- **Input validation** with helpful error messages
- **Confirmation prompts** for destructive operations

---

## üõ†Ô∏è Development

### Setup for Development

```bash
git clone https://github.com/enkaliprime/enkaliprime-cli.git
cd enkaliprime-cli
pip install -e ".[dev]"
```

### Running Tests

```bash
pytest
```

### Code Quality

```bash
# Format code
black enkaliprime_cli

# Sort imports
isort enkaliprime_cli

# Type checking
mypy enkaliprime_cli

# Linting
ruff enkaliprime_cli
```

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äî see the [LICENSE](LICENSE) file for details.

---

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

---

## üîó Links

<div align="center">

| Resource | Link |
|----------|------|
| üìñ **Documentation** | [api.enkaliprime.com/docs](https://api.enkaliprime.com/docs) |
| üè† **Website** | [api.enkaliprime.com](https://api.enkaliprime.com) |
| üì¶ **PyPI** | [pypi.org/project/enkaliprime-cli](https://pypi.org/project/enkaliprime-cli/) |
| üêô **GitHub** | [github.com/enkaliprime/enkaliprime-cli](https://github.com/enkaliprime/enkaliprime-cli) |

</div>

---

<div align="center">

### Built with ‚ù§Ô∏è by the EnkaliPrime Team

<sub>¬© 2024 EnkaliPrime. All rights reserved.</sub>

</div>
