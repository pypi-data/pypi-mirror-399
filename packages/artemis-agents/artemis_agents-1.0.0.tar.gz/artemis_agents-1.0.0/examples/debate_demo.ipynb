{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Display turning points if any\nif analytics.turning_points:\n    print(\"\\nðŸ”„ Turning Points Detected:\")\n    print(\"-\" * 50)\n    for tp in analytics.turning_points:\n        print(f\"\\nRound {tp.round} - {tp.agent}\")\n        print(f\"  Significance: {tp.significance:.2f}\")\n        print(f\"  {tp.analysis}\")\nelse:\n    print(\"\\nâœ“ No major turning points detected - consistent performance throughout.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export analytics HTML report\nfrom artemis.analytics import export_analytics_report\n\nanalytics_report_path = output_dir / \"analytics_report.html\"\nexport_analytics_report(result, analytics_report_path)\n\nprint(f\"\\nâœ“ Analytics report exported to: {analytics_report_path}\")\ndisplay(IFrame(src=str(analytics_report_path), width='100%', height=600))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Final scores bar chart\nif analytics.round_metrics:\n    jury_chart = JuryVoteChart(width=500, height=200)\n    final_scores = analytics.round_metrics[-1].agent_scores\n    svg = jury_chart.render_bar(final_scores)\n    print(\"\\nFinal Scores:\")\n    display(SVG(svg))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Momentum chart\nif analytics.momentum_history:\n    momentum_chart = MomentumChart(width=700, height=350)\n    svg = momentum_chart.render(analytics.momentum_history, analytics.agents)\n    print(\"\\nMomentum Over Time:\")\n    display(SVG(svg))\nelse:\n    print(\"No momentum data available.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate SVG visualizations\nfrom artemis.analytics.visualizations import ScoreProgressionChart, MomentumChart, JuryVoteChart\nfrom IPython.display import SVG\n\n# Score progression chart\nscore_chart = ScoreProgressionChart(width=700, height=350)\nround_scores = [rm.agent_scores for rm in analytics.round_metrics]\nturning_points = [tp.round for tp in analytics.turning_points]\n\nif round_scores:\n    svg = score_chart.render(round_scores, analytics.agents, turning_points)\n    print(\"Score Progression Over Rounds:\")\n    display(SVG(svg))\nelse:\n    print(\"No round scores available for visualization.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display key metrics\nmetrics_md = f\"\"\"\n### Analytics Metrics\n\n| Metric | {analytics.agents[0] if analytics.agents else 'Agent 1'} | {analytics.agents[1] if len(analytics.agents) > 1 else 'Agent 2'} |\n|--------|---------|---------|\n| **Rebuttal Effectiveness** | {analytics.rebuttal_effectiveness_overall.get(analytics.agents[0], 0):.2f} | {analytics.rebuttal_effectiveness_overall.get(analytics.agents[1] if len(analytics.agents) > 1 else '', 0):.2f} |\n| **Evidence Utilization** | {analytics.evidence_utilization_overall.get(analytics.agents[0], 0):.2f} | {analytics.evidence_utilization_overall.get(analytics.agents[1] if len(analytics.agents) > 1 else '', 0):.2f} |\n| **Argument Diversity** | {analytics.argument_diversity_index.get(analytics.agents[0], 0):.2f} | {analytics.argument_diversity_index.get(analytics.agents[1] if len(analytics.agents) > 1 else '', 0):.2f} |\n| **Final Momentum** | {analytics.final_momentum.get(analytics.agents[0], 0):.2f} | {analytics.final_momentum.get(analytics.agents[1] if len(analytics.agents) > 1 else '', 0):.2f} |\n\"\"\"\ndisplay(Markdown(metrics_md))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display momentum trajectory\nprint(\"\\nMomentum Trajectory by Round:\")\nprint(\"-\" * 50)\n\nfor rm in analytics.round_metrics:\n    print(f\"\\nRound {rm.round}:\")\n    for agent, score in rm.agent_scores.items():\n        delta = rm.score_delta.get(agent, 0)\n        delta_str = f\"+{delta:.3f}\" if delta > 0 else f\"{delta:.3f}\"\n        print(f\"  {agent}: {score:.3f} ({delta_str})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compute analytics from the debate result\nfrom artemis.analytics import analyze_debate, DebateAnalyzer\n\nanalytics = analyze_debate(result)\n\nprint(\"Debate Analytics Summary\")\nprint(\"=\" * 40)\nprint(f\"Rounds: {analytics.rounds}\")\nprint(f\"Agents: {', '.join(analytics.agents)}\")\nprint(f\"Turning points detected: {len(analytics.turning_points)}\")\nprint(f\"Lead changes: {analytics.count_lead_changes()}\")\nprint(f\"Sway events: {len(analytics.sway_events)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTEMIS Debate Demo\n",
    "\n",
    "This notebook demonstrates a complete end-to-end debate using ARTEMIS, including:\n",
    "- Setting up debate agents\n",
    "- Running a multi-round debate\n",
    "- Viewing the transcript and verdict\n",
    "- Exporting audit logs (JSON, Markdown, HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "print(\"API Keys configured:\")\n",
    "print(f\"  OpenAI: {'âœ“' if os.environ.get('OPENAI_API_KEY') else 'âœ—'}\")\n",
    "print(f\"  Anthropic: {'âœ“' if os.environ.get('ANTHROPIC_API_KEY') else 'âœ—'}\")\n",
    "print(f\"  Google AI Studio: {'âœ“' if os.environ.get('GOOGLE_API_KEY') else 'âœ—'}\")\n",
    "print(f\"  Vertex AI: {'âœ“' if os.environ.get('GOOGLE_CLOUD_PROJECT') else 'âœ—'}\")\n",
    "print(f\"  DeepSeek: {'âœ“' if os.environ.get('DEEPSEEK_API_KEY') else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Debate Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from artemis.core import Debate, Agent\n",
    "from artemis.core.jury import JuryPanel\n",
    "from artemis.core.types import JuryPerspective, JurorConfig\n",
    "\n",
    "# Choose model based on available API keys\n",
    "if os.environ.get('GOOGLE_CLOUD_PROJECT'):\n",
    "    MODEL = \"gemini-2.0-flash\"\n",
    "    print(f\"Using Vertex AI: {MODEL}\")\n",
    "elif os.environ.get('OPENAI_API_KEY'):\n",
    "    MODEL = \"gpt-4o-mini\"  # Cost-effective for demos\n",
    "    print(f\"Using OpenAI: {MODEL}\")\n",
    "else:\n",
    "    raise ValueError(\"No API key configured! Set OPENAI_API_KEY or GOOGLE_CLOUD_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create debate agents with different positions\n",
    "pro_agent = Agent(\n",
    "    name=\"Advocate\",\n",
    "    role=\"Proponent\",\n",
    "    position=\"in favor of remote work as the default\",\n",
    "    model=MODEL,\n",
    "    persona=\"You are a forward-thinking workplace strategist who believes in flexibility and work-life balance.\"\n",
    ")\n",
    "\n",
    "con_agent = Agent(\n",
    "    name=\"Traditionalist\",\n",
    "    role=\"Opponent\",\n",
    "    position=\"against remote work as the default\",\n",
    "    model=MODEL,\n",
    "    persona=\"You are a pragmatic business leader who values in-person collaboration and company culture.\"\n",
    ")\n",
    "\n",
    "print(f\"Created agents: {pro_agent.name} vs {con_agent.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diverse jury panel\n",
    "jury = JuryPanel(\n",
    "    evaluators=3,\n",
    "    model=MODEL,\n",
    "    consensus_threshold=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Jury panel: {len(jury)} jurors\")\n",
    "print(f\"Perspectives: {[j.perspective.value for j in jury.jurors]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the debate\n",
    "debate = Debate(\n",
    "    topic=\"Should remote work be the default for knowledge workers?\",\n",
    "    agents=[pro_agent, con_agent],\n",
    "    rounds=2,  # Keep short for demo\n",
    "    jury=jury,\n",
    ")\n",
    "\n",
    "print(f\"Debate topic: {debate.topic}\")\n",
    "print(f\"Rounds: {debate.rounds}\")\n",
    "print(f\"Agents: {[a.name for a in debate.agents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the debate (this may take a minute)\n",
    "print(\"Running debate...\")\n",
    "result = await debate.run()\n",
    "print(f\"\\nâœ“ Debate complete! {len(result.transcript)} turns recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display verdict\n",
    "from IPython.display import Markdown, HTML\n",
    "\n",
    "verdict_md = f\"\"\"\n",
    "## Verdict\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Winner** | {result.verdict.decision} |\n",
    "| **Confidence** | {result.verdict.confidence:.0%} |\n",
    "| **Unanimous** | {'Yes' if result.verdict.unanimous else 'No'} |\n",
    "\n",
    "### Reasoning\n",
    "{result.verdict.reasoning}\n",
    "\n",
    "### Score Breakdown\n",
    "\"\"\"\n",
    "for agent, score in result.verdict.score_breakdown.items():\n",
    "    verdict_md += f\"- **{agent}**: {score:.2f}\\n\"\n",
    "\n",
    "display(Markdown(verdict_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transcript\n",
    "transcript_md = \"## Debate Transcript\\n\\n\"\n",
    "\n",
    "for turn in result.transcript:\n",
    "    transcript_md += f\"### Round {turn.round} - {turn.agent} ({turn.argument.level.value})\\n\\n\"\n",
    "    transcript_md += f\"{turn.argument.content}\\n\\n\"\n",
    "    \n",
    "    if turn.argument.evidence:\n",
    "        transcript_md += \"**Evidence:**\\n\"\n",
    "        for ev in turn.argument.evidence:\n",
    "            transcript_md += f\"- [{ev.type}] {ev.content[:100]}...\\n\"\n",
    "        transcript_md += \"\\n\"\n",
    "    \n",
    "    transcript_md += \"---\\n\\n\"\n",
    "\n",
    "display(Markdown(transcript_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Audit Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from artemis.utils.audit import export_debate_audit, AuditLog\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"./audit_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export in all formats\n",
    "paths = export_debate_audit(\n",
    "    result,\n",
    "    output_dir=output_dir,\n",
    "    formats=[\"json\", \"markdown\", \"html\"]\n",
    ")\n",
    "\n",
    "print(\"Exported audit logs:\")\n",
    "for fmt, path in paths.items():\n",
    "    print(f\"  {fmt.upper()}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display JSON structure\n",
    "import json\n",
    "\n",
    "with open(paths['json']) as f:\n",
    "    audit_json = json.load(f)\n",
    "\n",
    "print(\"JSON Audit Log Structure:\")\n",
    "print(f\"  - debate_id: {audit_json['debate_id'][:8]}...\")\n",
    "print(f\"  - topic: {audit_json['topic'][:50]}...\")\n",
    "print(f\"  - entries: {len(audit_json['entries'])} events\")\n",
    "print(f\"  - verdict: {audit_json['verdict']['decision']}\")\n",
    "print(f\"  - metadata: {list(audit_json['metadata'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML report in notebook\n",
    "with open(paths['html']) as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Show HTML report (in iframe to contain styles)\n",
    "from IPython.display import IFrame\n",
    "display(IFrame(src=str(paths['html']), width='100%', height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Markdown report\n",
    "with open(paths['markdown']) as f:\n",
    "    md_content = f.read()\n",
    "\n",
    "display(Markdown(md_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Individual Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evaluation scores for each turn\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for turn in result.transcript:\n",
    "    if turn.evaluation:\n",
    "        eval_data.append({\n",
    "            'Round': turn.round,\n",
    "            'Agent': turn.agent,\n",
    "            'Level': turn.argument.level.value,\n",
    "            'Total Score': turn.evaluation.total_score,\n",
    "            'Causal Score': turn.evaluation.causal_score,\n",
    "            'Evidence Count': len(turn.argument.evidence),\n",
    "        })\n",
    "\n",
    "if eval_data:\n",
    "    df = pd.DataFrame(eval_data)\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No evaluation data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nDebate Statistics:\")\n",
    "print(f\"  Total turns: {len(result.transcript)}\")\n",
    "print(f\"  Rounds completed: {result.metadata.total_rounds}\")\n",
    "print(f\"  Safety alerts: {len(result.safety_alerts)}\")\n",
    "print(f\"  Final state: {result.final_state.value}\")\n",
    "\n",
    "if result.metadata.model_usage:\n",
    "    print(\"\\nToken Usage:\")\n",
    "    for model, usage in result.metadata.model_usage.items():\n",
    "        print(f\"  {model}: {usage.get('prompt_tokens', 0)} prompt, {usage.get('completion_tokens', 0)} completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Debate Analytics\n\nAnalyze momentum, turning points, and metrics using the new analytics module.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}