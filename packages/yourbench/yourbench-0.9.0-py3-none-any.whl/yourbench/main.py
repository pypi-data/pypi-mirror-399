#!/usr/bin/env python3
"""YourBench CLI - Dynamic Evaluation Set Generation with Large Language Models.

A modern CLI using Rich for beautiful output, progress tracking, and useful commands.
"""

import os
import sys
import atexit
from pathlib import Path
from datetime import datetime

import typer
from dotenv import load_dotenv
from loguru import logger
from rich.panel import Panel
from rich.table import Table
from rich.console import Console


load_dotenv()
console = Console()


def configure_logging(debug: bool = False, log_dir: Path = None, quiet: bool = False):
    """Configure structured logging with file output."""
    logger.remove()  # Remove default handler

    log_level = "DEBUG" if debug else os.getenv("YOURBENCH_LOG_LEVEL", "INFO")

    if not quiet:
        # Console handler with structured format for logs with stage
        console_format = (
            "<green>{time:HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{extra[stage]: <16}</cyan> | "
            "<level>{message}</level>"
        )
        logger.add(
            sys.stderr,
            format=console_format,
            level=log_level,
            filter=lambda record: "stage" in record["extra"],
            enqueue=True,
        )

        # Fallback console handler for logs without stage
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            level=log_level,
            filter=lambda record: "stage" not in record["extra"],
            enqueue=True,
        )

    # File handler - JSON structured logs
    if log_dir is None:
        log_dir = Path(os.getenv("YOURBENCH_LOG_DIR", "logs"))
    log_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"yourbench_{timestamp}.jsonl"

    logger.add(
        str(log_file),
        format="{message}",
        level="DEBUG",
        serialize=True,
        enqueue=True,
        rotation="100 MB",
    )

    # Summary log file (INFO and above only)
    summary_file = log_dir / f"yourbench_{timestamp}_summary.log"
    logger.add(
        str(summary_file),
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {extra} | {message}",
        level="INFO",
        filter=lambda record: record["level"].no >= 20,
        enqueue=True,
    )

    if not quiet:
        logger.info(f"Logging configured. JSON logs: {log_file}, Summary: {summary_file}")
    return log_file, summary_file


def cleanup_logging():
    """Ensure all logs are flushed and closed."""
    logger.complete()


atexit.register(cleanup_logging)

# Initialize logging with default configuration (quiet mode for Rich output)
configure_logging(quiet=True)

app = typer.Typer(
    name="yourbench",
    help="YourBench - Dynamic Evaluation Set Generation with Large Language Models.",
    pretty_exceptions_show_locals=False,
    add_completion=False,
)


def _print_banner():
    """Print YourBench banner."""
    banner = """
[bold blue]\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e[/bold blue]
[bold blue]\u2502[/bold blue]  [bold cyan]YourBench[/bold cyan] - Dynamic Evaluation Set Generation           [bold blue]\u2502[/bold blue]
[bold blue]\u2502[/bold blue]  [dim]Build domain-specific benchmarks from your documents[/dim]     [bold blue]\u2502[/bold blue]
[bold blue]\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f[/bold blue]
"""
    console.print(banner)


def _print_config_summary(config) -> None:
    """Print a summary of the configuration."""
    from yourbench.conf.loader import get_enabled_stages

    table = Table(title="Configuration Summary", show_header=True, header_style="bold magenta")
    table.add_column("Setting", style="cyan")
    table.add_column("Value", style="green")

    # Dataset info
    hf = config.hf_configuration
    if hf.hf_dataset_name:
        org_prefix = f"{hf.hf_organization}/" if hf.hf_organization else ""
        table.add_row("Dataset", f"{org_prefix}{hf.hf_dataset_name}")
    table.add_row("Push to Hub", "\u2713" if hf.push_to_hub else "\u2717")
    table.add_row("Private", "\u2713" if hf.private else "\u2717")

    # Models
    if config.model_list:
        models = ", ".join(m.model_name for m in config.model_list)
        table.add_row("Models", models)

    # Enabled stages
    stages = get_enabled_stages(config)
    if stages:
        table.add_row("Stages", ", ".join(stages))

    console.print(table)
    console.print()


@app.command()
def run(
    config_path: str = typer.Argument(..., help="Path to YAML config file"),
    debug: bool = typer.Option(False, "--debug", "-d", help="Enable debug logging"),
    quiet: bool = typer.Option(False, "--quiet", "-q", help="Minimal output (only errors)"),
    no_banner: bool = typer.Option(False, "--no-banner", help="Hide the banner"),
) -> None:
    """Run YourBench pipeline with a config file."""
    configure_logging(debug=debug, quiet=quiet)

    if not quiet and not no_banner:
        _print_banner()

    config_file = Path(config_path)
    if not config_file.exists():
        console.print(f"[bold red]\u2717[/bold red] Config file not found: {config_path}")
        raise typer.Exit(1)

    if config_file.suffix not in {".yaml", ".yml"}:
        console.print(f"[bold red]\u2717[/bold red] Config must be a YAML file (.yaml or .yml): {config_path}")
        raise typer.Exit(1)

    from yourbench.conf.loader import load_config, get_enabled_stages
    from yourbench.pipeline.handler import run_pipeline_with_progress

    try:
        with console.status("[bold cyan]Loading configuration..."):
            config = load_config(config_file)
            if debug:
                config.debug = True

        if not quiet:
            _print_config_summary(config)

        stages = get_enabled_stages(config)
        if not stages:
            console.print("[yellow]\u26a0[/yellow] No pipeline stages enabled")
            raise typer.Exit(0)

        run_pipeline_with_progress(config, debug=debug, quiet=quiet, console=console)

        if not quiet:
            console.print()
            console.print(
                Panel.fit("[bold green]\u2713 Pipeline completed successfully![/bold green]", border_style="green")
            )

    except Exception as e:
        logger.exception(f"Pipeline failed: {e}")
        console.print(f"[bold red]\u2717[/bold red] Pipeline failed: {e}")
        raise typer.Exit(1)


@app.command()
def validate(
    config_path: str = typer.Argument(..., help="Path to YAML config file to validate"),
) -> None:
    """Validate a configuration file without running the pipeline."""
    _print_banner()

    config_file = Path(config_path)
    if not config_file.exists():
        console.print(f"[bold red]\u2717[/bold red] Config file not found: {config_path}")
        raise typer.Exit(1)

    from yourbench.conf.loader import load_config, get_enabled_stages
    from yourbench.conf.schema import ConfigValidationError

    try:
        with console.status("[bold cyan]Validating configuration..."):
            config = load_config(config_file)

        console.print("[bold green]\u2713[/bold green] Configuration is valid!")
        console.print()
        _print_config_summary(config)

        # Show detailed stage information
        stages = get_enabled_stages(config)
        if stages:
            console.print(f"[cyan]Enabled stages ({len(stages)}):[/cyan]")
            for i, stage in enumerate(stages, 1):
                console.print(f"  {i}. {stage}")
        else:
            console.print("[yellow]\u26a0[/yellow] No pipeline stages enabled")

    except ConfigValidationError as e:
        console.print(f"[bold red]\u2717[/bold red] Validation failed: {e}")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[bold red]\u2717[/bold red] Error loading config: {e}")
        raise typer.Exit(1)


@app.command()
def init(
    output: str = typer.Option("config.yaml", "--output", "-o", help="Output file path"),
    force: bool = typer.Option(False, "--force", "-f", help="Overwrite existing file"),
) -> None:
    """Generate a starter configuration file interactively."""
    from rich.prompt import Prompt, Confirm

    _print_banner()

    output_path = Path(output)
    if output_path.exists() and not force:
        console.print(f"[yellow]\u26a0[/yellow] File already exists: {output}")
        if not Confirm.ask("Overwrite?"):
            raise typer.Exit(0)

    console.print("[cyan]Let's create your YourBench configuration![/cyan]")
    console.print()

    # Basic setup
    dataset_name = Prompt.ask("Dataset name", default="my-yourbench-dataset")

    hf_org = Prompt.ask(
        "HuggingFace organization (leave empty for personal)", default=os.getenv("HF_ORGANIZATION", "")
    )

    source_dir = Prompt.ask("Source documents directory", default="data/raw")

    output_dir = Prompt.ask("Processed output directory", default="data/processed")

    # Model configuration
    console.print()
    console.print("[cyan]Model Configuration[/cyan]")

    use_env = Confirm.ask("Use model from environment variables (OPENAI_*)", default=True)

    model_name = ""
    if not use_env:
        model_name = Prompt.ask("Model name", default="gpt-4")

    # Pipeline stages
    console.print()
    console.print("[cyan]Pipeline Stages[/cyan]")

    stages = []
    if Confirm.ask("Enable ingestion (document processing)", default=True):
        stages.append("ingestion")
    if Confirm.ask("Enable single-hop question generation", default=True):
        stages.append("single_hop_question_generation")
    if Confirm.ask("Enable multi-hop question generation", default=False):
        stages.append("multi_hop_question_generation")
    if Confirm.ask("Enable cross-document question generation", default=False):
        stages.append("cross_document_question_generation")
    if Confirm.ask("Enable lighteval preparation", default=True):
        stages.append("prepare_lighteval")

    private = Confirm.ask("Make dataset private", default=True)

    # Generate config
    config_lines = [
        "# YourBench Configuration",
        f"# Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "hf_configuration:",
        f"  hf_dataset_name: {dataset_name}",
    ]

    if hf_org:
        config_lines.append(f"  hf_organization: {hf_org}")

    config_lines.extend([
        "  hf_token: $HF_TOKEN",
        f"  private: {str(private).lower()}",
        "  push_to_hub: true",
        "",
    ])

    if use_env:
        config_lines.extend([
            "model_list:",
            "  - model_name: $OPENAI_MODEL",
            "    base_url: $OPENAI_BASE_URL",
            "    api_key: $OPENAI_API_KEY",
            "    max_concurrent_requests: 128",
            "",
        ])
    elif model_name:
        config_lines.extend([
            "model_list:",
            f"  - model_name: {model_name}",
            "    api_key: $OPENAI_API_KEY",
            "    max_concurrent_requests: 128",
            "",
        ])

    config_lines.append("pipeline:")

    if "ingestion" in stages:
        config_lines.extend([
            "  ingestion:",
            f"    source_documents_dir: {source_dir}",
            f"    output_dir: {output_dir}",
            "",
        ])

    if "single_hop_question_generation" in stages:
        config_lines.extend([
            "  single_hop_question_generation:",
            "    question_mode: open-ended",
            "",
        ])

    if "multi_hop_question_generation" in stages:
        config_lines.extend([
            "  multi_hop_question_generation:",
            "    question_mode: open-ended",
            "",
        ])

    if "cross_document_question_generation" in stages:
        config_lines.extend([
            "  cross_document_question_generation:",
            "    question_mode: open-ended",
            "    max_combinations: 50",
            "",
        ])

    if "prepare_lighteval" in stages:
        config_lines.extend([
            "  prepare_lighteval:",
            "",
        ])

    config_content = "\n".join(config_lines)

    # Write file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(config_content)

    console.print()
    console.print(
        Panel.fit(
            f"[bold green]\u2713 Configuration saved to {output}[/bold green]\n\n"
            f"Run with: [cyan]yourbench run {output}[/cyan]",
            title="Success",
            border_style="green",
        )
    )


@app.command()
def stages() -> None:
    """Show available pipeline stages and their descriptions."""
    _print_banner()

    table = Table(title="Pipeline Stages", show_header=True, header_style="bold magenta")
    table.add_column("#", style="dim", width=3)
    table.add_column("Stage", style="cyan")
    table.add_column("Description", style="white")

    stage_info = [
        ("ingestion", "Process source documents (PDF, Markdown, text) into structured format"),
        ("summarization", "Generate summaries of document content"),
        ("chunking", "Split documents into smaller chunks for question generation"),
        ("single_hop_question_generation", "Generate standalone Q&A pairs from chunks"),
        ("multi_hop_question_generation", "Generate questions requiring multiple chunks to answer"),
        ("cross_document_question_generation", "Generate questions spanning multiple documents"),
        ("question_rewriting", "Rewrite questions for clarity and consistency"),
        ("prepare_lighteval", "Format dataset for LightEval evaluation framework"),
        ("citation_score_filtering", "Filter questions based on citation quality scores"),
    ]

    for i, (stage, desc) in enumerate(stage_info, 1):
        table.add_row(str(i), stage, desc)

    console.print(table)
    console.print()
    console.print("[dim]Enable stages by adding them to your config's pipeline section.[/dim]")


@app.command()
def estimate(
    config_path: str = typer.Argument(..., help="Path to YAML config file"),
) -> None:
    """Estimate token usage for a pipeline run."""
    _print_banner()

    config_file = Path(config_path)
    if not config_file.exists():
        console.print(f"[bold red]✗[/bold red] Config file not found: {config_path}")
        raise typer.Exit(1)

    from yourbench.conf.loader import load_config
    from yourbench.utils.token_estimation import format_token_count, estimate_pipeline_tokens

    try:
        with console.status("[bold cyan]Analyzing configuration..."):
            config = load_config(config_file)
            estimates = estimate_pipeline_tokens(config)

        # Source info
        console.print("[bold]Source Documents:[/bold]")
        console.print(f"  Files: {estimates.get('source_file_count', 0)}")
        console.print(f"  Estimated tokens: {format_token_count(estimates['source_tokens'])}")
        console.print()

        # Stage breakdown
        table = Table(title="Token Estimation by Stage", show_header=True, header_style="bold magenta")
        table.add_column("Stage", style="cyan")
        table.add_column("Input Tokens", style="green", justify="right")
        table.add_column("Output Tokens", style="yellow", justify="right")
        table.add_column("API Calls", style="blue", justify="right")
        table.add_column("Notes", style="dim")

        for stage, info in estimates["stages"].items():
            input_tok = format_token_count(info.get("input_tokens", 0)) if info.get("input_tokens") else "-"
            output_tok = format_token_count(info.get("output_tokens", 0)) if info.get("output_tokens") else "-"
            calls = str(info.get("calls", "-")) if info.get("calls") else "-"
            note = info.get("note", "")
            table.add_row(stage.replace("_", " ").title(), input_tok, output_tok, calls, note)

        console.print(table)
        console.print()

        # Totals
        console.print(
            Panel.fit(
                f"[bold]Total Estimated Usage:[/bold]\n"
                f"  Input tokens:  [green]{format_token_count(estimates['total_input_tokens'])}[/green]\n"
                f"  Output tokens: [yellow]{format_token_count(estimates['total_output_tokens'])}[/yellow]\n"
                f"  Total:         [bold cyan]{format_token_count(estimates['total_tokens'])}[/bold cyan]",
                title="Summary",
                border_style="blue",
            )
        )

        console.print()
        console.print(
            "[dim]Note: These are rough estimates. Actual usage may vary based on document content and model responses.[/dim]"
        )

    except Exception as e:
        console.print(f"[bold red]✗[/bold red] Error: {e}")
        raise typer.Exit(1)


@app.command("version")
def version_command() -> None:
    """Show YourBench version."""
    from importlib.metadata import version as get_version

    try:
        v = get_version("yourbench")
    except Exception:
        v = "development"

    console.print(
        Panel.fit(f"[bold cyan]YourBench[/bold cyan] version [bold green]{v}[/bold green]", border_style="blue")
    )


def main() -> None:
    """Entry point for the CLI."""
    # Handle version flag
    if "--version" in sys.argv or "-v" in sys.argv:
        version_command()
        return

    # If no arguments, show help
    if len(sys.argv) == 1:
        _print_banner()
        app()
        return

    # If first arg looks like a path (not a command), assume it's 'run'
    if len(sys.argv) > 1:
        first_arg = sys.argv[1]
        commands = ["run", "version", "validate", "init", "stages", "estimate"]
        if not first_arg.startswith("-") and first_arg not in commands:
            sys.argv = [sys.argv[0], "run"] + sys.argv[1:]

    app()


if __name__ == "__main__":
    main()
