# Default configuration for iris_rag package
database:
  iris:
    driver: "intersystems_iris.dbapi._DBAPI"
    host: "localhost"
    port: "1974"
    namespace: "USER"
    username: "_SYSTEM"
    password: "SYS"

storage:
  iris:
    table_name: "RAG.SourceDocuments"
    vector_dimension: 384
    # Custom metadata filter keys for multi-tenant isolation (User Story 1 - P1)
    # These fields can be used in metadata_filter parameter for queries
    # Default filter keys (always enabled): document_id, source, title, author,
    #   created_at, updated_at, content_type, file_type, page_number, section,
    #   chunk_id, chunk_index, language, collection_id, metadata_hash,
    #   parent_document_id, version
    custom_filter_keys:
      - "tenant_id"              # Tenant identifier for multi-tenant deployments
      - "security_level"         # Security clearance level (e.g., public, confidential, secret)
      - "department"             # Department or organizational unit

# Vector index configuration for optimal HNSW performance
vector_index:
  type: "HNSW"                    # Index type: HNSW for high-performance vector search
  M: 16                          # Number of bi-directional links for each node (higher = better recall, more memory)
  efConstruction: 200            # Size of dynamic candidate list (higher = better quality, slower build)
  Distance: "COSINE"             # Distance metric: COSINE, EUCLIDEAN, or DOT

pipelines:
  basic:
    chunk_size: 1000
    chunk_overlap: 200
    default_top_k: 5
    embedding_batch_size: 32
  vector_query_mode: "safe_single_param"
  vector_hnsw:
    ensure_on_start: true
    try_acorn: true

# Pipeline validation configuration (Feature 047)
validation:
  enabled: true                          # Enable pipeline contract validation
  strict_mode: false                     # If true, treat warnings as errors
  validate_on_register: true             # Validate pipelines when registering handlers
  validate_on_query: false               # Validate responses after each query (performance impact)
  log_violations: true                   # Log contract violations
  raise_on_error: true                   # Raise exceptions for validation errors

  # Contract requirements (should match PipelineValidator)
  contract:
    required_methods: ["query", "load_documents"]
    required_response_fields:
      - "answer"
      - "retrieved_documents"
      - "contexts"
      - "sources"
      - "execution_time"
      - "metadata"
    required_metadata_fields:
      - "num_retrieved"
      - "pipeline_type"
      - "generated_answer"
      - "processing_time"
      - "retrieval_method"
      - "context_count"

embeddings:
  default_provider: "sentence_transformers"
  sentence_transformers:
    model_name: "all-MiniLM-L6-v2"
    device: "cpu"

# IRIS EMBEDDING Configuration (Feature 051)
iris_embedding:
  enabled: false                             # Enable IRIS EMBEDDING auto-vectorization

  # Default configurations (can be overridden per-table)
  default_config:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    hf_cache_path: "/var/lib/huggingface"   # HuggingFace model cache directory
    python_path: "/usr/bin/python3"          # Python interpreter path
    batch_size: 32                           # Batch size for vectorization
    device_preference: "auto"                # Device: auto, cuda, mps, cpu
    enable_entity_extraction: false          # Extract entities during vectorization
    entity_types: []                         # Entity types to extract (if enabled)

  # Model cache configuration
  cache:
    enabled: true                            # Enable in-memory model caching
    max_models: 5                            # Maximum number of cached models
    eviction_policy: "lru"                   # Cache eviction: lru, lfu, ttl
    ttl_seconds: 3600                        # Time-to-live for cached models (1 hour)
    warmup_on_start: false                   # Preload models at startup

  # Performance targets (Feature 050 pattern)
  performance:
    cache_hit_target_ms: 50                  # Target: <50ms for cache hits
    cache_miss_target_ms: 5000               # Target: <5000ms for cache misses (includes model load)
    min_cache_hit_rate: 0.95                 # Target: >=95% cache hit rate after warmup
    gpu_oom_fallback: true                   # Fallback to CPU on GPU OOM

  # Table-specific configurations (example)
  # configurations:
  #   medical_embeddings_v1:
  #     model_name: "sentence-transformers/all-MiniLM-L6-v2"
  #     enable_entity_extraction: true
  #     entity_types: ["Disease", "Symptom", "Medication"]
  #
  #   paper_embeddings:
  #     model_name: "sentence-transformers/allenai-specter"
  #     device_preference: "cuda"
  #     batch_size: 64

  # Multi-field vectorization examples
  # multi_field_configs:
  #   research_papers:
  #     table_name: "RAG.ResearchPapers"
  #     embedding_column: "metadata_vector"
  #     source_columns:                      # Concatenate multiple fields
  #       - "title"
  #       - "abstract"
  #       - "conclusions"
  #     config_name: "paper_embeddings"
  #
  #   medical_records:
  #     table_name: "RAG.MedicalRecords"
  #     embedding_column: "clinical_vector"
  #     source_columns:
  #       - "chief_complaint"
  #       - "history_of_present_illness"
  #       - "assessment"
  #     config_name: "medical_embeddings_v1"

# Entity extraction configuration
entity_extraction:
  enabled: true
  method: "llm_basic"                    # ontology_hybrid, llm_basic, pattern_only, hybrid
  confidence_threshold: 0.7              # Minimum confidence score for entities
  entity_types:                         # Enabled entity types (matches pattern definitions)
    - "DRUG"
    - "DISEASE"
    - "GENE"
    - "ANATOMY"
    - "PROCEDURE"
    - "SYMPTOM"
    - "TREATMENT"
    - "PERSON"
    - "ORGANIZATION"
  llm:
    model: "openai/gpt-oss-120b"         # LLM model: GPT-OSS 120B (OpenAI-compatible)
    api_base: "http://apps-llm-4.iscinternal.com:8000/v1"  # GPT-OSS endpoint
    api_key: "dummy"                     # API key (not required for internal endpoints)
    api_type: "openai"                   # OpenAI-compatible API
    temperature: 0.1                     # Low temperature for consistent extraction
    max_tokens: 2000                     # Max tokens for LLM response
    supports_response_format: false      # GPT-OSS doesn't support JSON mode
    use_json_mode: false                 # Disable JSON mode
  batch_processing:
    enabled: false                       # DISABLED: GPT-OSS doesn't support JSON mode
  storage:
    entities_table: "RAG.Entities"      # Database table for entities
    relationships_table: "RAG.EntityRelationships"  # Table for relationships
    embeddings_table: "RAG.EntityEmbeddings"        # Table for entity embeddings

# General-purpose ontology configuration for ANY domain
ontology:
  enabled: false                         # Enable/disable ontology support
  type: "general"                        # Single general-purpose type (replaces domain-specific plugins)
  auto_detect_domain: true               # Auto-detect domain from ontology metadata
  
  # Ontology data sources - works with ANY valid ontology
  sources:
    - type: "owl"                        # Ontology format: owl, rdf, skos, xml, ttl, n3
      path: "path/to/any/ontology.owl"   # Path to ontology file (ANY domain)
      # Optional: override auto-detection
      # domain: "custom_domain_name"
    
    # Examples for different domains:
    # - type: "owl"
    #   path: "ontologies/medical/umls.owl"        # Medical domain
    # - type: "skos" 
    #   path: "ontologies/legal/legal_terms.skos"  # Legal domain
    # - type: "rdf"
    #   path: "ontologies/financial/fibo.rdf"      # Financial domain
    # - type: "owl"
    #   path: "ontologies/scientific/research.owl" # Scientific domain
  
  # Custom domain definitions (optional)
  custom_domains:
    enabled: false                       # Enable custom domain definitions
    definition_path: null                # Path to JSON file with domain configuration
    # Example JSON structure:
    # {
    #   "domain_name": "biomedical",
    #   "description": "Biomedical research domain",
    #   "entity_types": {
    #     "PROTEIN": ["protein", "enzyme", "antibody"],
    #     "GENE": ["gene", "allele", "locus"]
    #   },
    #   "extraction_patterns": {
    #     "PROTEIN": ["\\b[A-Z][a-z]+\\d+\\b"]
    #   },
    #   "synonyms": {
    #     "protein": ["enzyme", "polypeptide"]
    #   }
    # }
  
  # Reasoning engine configuration
  reasoning:
    enable_inference: true               # Enable ontology-based reasoning
    max_inference_depth: 3               # Maximum depth for concept traversal
    confidence_threshold: 0.6            # Minimum confidence for inferred relationships
    reasoning_strategies:                # Active reasoning strategies
      - "subsumption"                    # Hierarchical concept reasoning
      - "property_based"                 # Property-based inference
      - "rule_based"                     # Custom rule application
  
  # Entity mapping configuration
  entity_mapping:
    auto_map: true                       # Automatically map concepts to entity types
    auto_domain_detection: true          # Auto-detect relevant domains from text
    expansion_strategy: "synonyms"       # Query expansion: synonyms, hierarchical, semantic
    # Note: custom_mappings are now auto-generated from loaded ontologies
    
  # Performance and optimization settings
  performance:
    max_concepts_per_domain: 10000       # Limit concepts loaded per domain
    cache_ontology_queries: true         # Cache frequently used ontology queries
    lazy_load_concepts: true             # Load concepts on-demand
    concept_similarity_threshold: 0.8    # Minimum similarity for concept matching

# GraphRAG pipeline configuration with general ontology support
graphrag:
  enabled: true                          # Enable GraphRAG pipeline
  default_top_k: 10                      # Default number of results to return
  max_depth: 2                           # Maximum graph traversal depth
  max_entities: 50                       # Maximum entities per traversal
  enable_vector_fallback: false          # Fallback to vector search if graph fails

# IRIS Graph Core Hybrid Search Configuration
iris_graph_core:
  enabled: true                          # Enable iris_graph_core hybrid search capabilities

  # RRF (Reciprocal Rank Fusion) Configuration
  rrf:
    c_parameter: 60                      # RRF constant (typically 60)
    default_vector_k: 30                 # Default number of vector candidates
    default_text_k: 30                   # Default number of text candidates
    default_weights: [0.5, 0.3, 0.2]    # Default fusion weights [vector, text, graph]

  # HNSW Vector Search Optimization
  hnsw:
    enabled: true                        # Enable HNSW-optimized vector search
    fallback_to_csv: true                # Fall back to CSV parsing if HNSW unavailable
    M: 16                                # HNSW M parameter (bi-directional links)
    efConstruction: 200                  # HNSW efConstruction parameter
    distance: "COSINE"                   # Distance metric: COSINE, EUCLIDEAN, DOT
    try_acorn: false                     # Try ACORN=1 optimization (requires licensed IRIS)
    community_edition_compatible: true   # Ensure compatibility with IRIS Community Edition
    # For licensed IRIS users: Set try_acorn: true and use intersystemsdc/iris-ml:latest
    # For community users: Keep try_acorn: false and use intersystemsdc/iris-community:latest

  # Text Search Configuration
  text_search:
    enable_ifind: true                   # Use IRIS iFind for enhanced text search
    fallback_to_like: true               # Fall back to LIKE patterns if iFind unavailable
    min_confidence_threshold: 0          # Minimum confidence for text results (0-1000)
    use_stemming: true                   # Enable stemming in iFind
    use_stopwords: true                  # Enable stopword filtering in iFind

  # Graph Traversal Configuration
  graph_traversal:
    max_expansion_depth: 2               # Maximum depth for neighborhood expansion
    default_confidence_threshold: 500   # Default confidence threshold (0-1000)
    max_entities_per_expansion: 50       # Maximum entities per expansion step
    use_json_table_filtering: true       # Use JSON_TABLE for confidence filtering

  # Hybrid Search Method Configuration
  hybrid_search:
    default_method: "rrf"                # Default method: hybrid, rrf, vector, text, kg
    adaptive_routing: true               # Enable adaptive query routing
    enable_multi_modal: true             # Enable multi-modal search fusion

  # Schema Management for Graph Core Tables
  schema:
    auto_create_tables: true             # Automatically create iris_graph_core tables
    auto_migrate_schema: true            # Automatically migrate schema changes
    required_tables:                     # Tables required for iris_graph_core
      - "rdf_labels"
      - "rdf_props"
      - "rdf_edges"
      - "kg_NodeEmbeddings_optimized"

  # Performance and Optimization
  performance:
    cache_vector_queries: true           # Cache frequently used vector queries
    cache_text_queries: true             # Cache frequently used text queries
    max_cache_size: 1000                 # Maximum number of cached queries
    query_timeout_ms: 30000              # Query timeout in milliseconds
  
  # General ontology integration for GraphRAG
  ontology_integration:
    query_expansion: true                # Expand queries using ontology concepts
    entity_enrichment: true              # Enrich extracted entities with ontology data
    relationship_inference: true         # Infer relationships using ontology reasoning
    semantic_similarity: true            # Use semantic similarity for entity matching
    
  # Visualization settings
  visualization:
    enabled: true                        # Enable graph visualization
    default_type: "plotly"               # Default visualization: plotly, d3, traversal
    highlight_ontology_concepts: true    # Highlight ontology-derived concepts
    show_inference_paths: true           # Show reasoning inference paths

# Example configurations for different domains:

# Medical/Healthcare Domain Example:
# ontology:
#   enabled: true
#   sources:
#     - type: "owl"
#       path: "ontologies/umls/umls_core.owl"
#     - type: "skos"
#       path: "ontologies/snomed/snomed_ct.skos"

# Legal Domain Example:
# ontology:
#   enabled: true
#   sources:
#     - type: "owl"
#       path: "ontologies/legal/lkif.owl"
#     - type: "rdf"
#       path: "ontologies/legal/legislation.rdf"

# Financial Domain Example:
# ontology:
#   enabled: true
#   sources:
#     - type: "owl"
#       path: "ontologies/fibo/fibo_core.owl"
#     - type: "skos"
#       path: "ontologies/finance/financial_terms.skos"

# Scientific Research Domain Example:
# ontology:
#   enabled: true
#   sources:
#     - type: "owl"
#       path: "ontologies/science/research_methods.owl"
#     - type: "rdf"
#       path: "ontologies/science/publications.rdf"

# Multi-Domain Example:
# ontology:
#   enabled: true
#   sources:
#     - type: "owl"
#       path: "ontologies/medical.owl"
#     - type: "owl"
#       path: "ontologies/legal.owl"
#     - type: "skos"
#       path: "ontologies/general_terms.skos"

# =============================================================================
# Enterprise Enhancements (v0.6.0)
# =============================================================================

# Role-Based Access Control (User Story 3 - P1)
security:
  rbac:
    enabled: false               # Enable RBAC policy enforcement
    policy_class: null           # Full import path to RBAC policy class (e.g., "myapp.security.MyRBACPolicy")
    # Example policy classes:
    # - "iris_vector_rag.security.examples.LDAPRBACPolicy"
    # - "iris_vector_rag.security.examples.ClearanceLevelPolicy"
    # - "mycompany.security.CustomRBACPolicy"

# OpenTelemetry Monitoring (User Story 4 - P2)
telemetry:
  enabled: false                 # Enable OpenTelemetry instrumentation (zero overhead when disabled)
  service_name: "iris-vector-rag"
  endpoint: "http://localhost:4318"  # OTLP HTTP endpoint
  sampling_ratio: 1.0            # Sampling rate (1.0 = 100%, 0.1 = 10%)
  export_protocol: "http"        # Export protocol: http or grpc
  export_timeout_ms: 10000       # Export timeout in milliseconds

  # GenAI semantic conventions (OpenTelemetry)
  genai_conventions:
    enabled: true                # Track LLM-specific attributes (gen.ai.*)
    track_token_usage: true      # Track token counts and costs
    track_prompts: false         # Include prompts in spans (may contain sensitive data)
    track_completions: false     # Include completions in spans (may contain sensitive data)

  # Cost tracking
  cost_tracking:
    enabled: true                # Calculate LLM API costs
    pricing_table:               # Model pricing (per 1M tokens)
      openai:
        gpt-4o-mini:
          input: 0.15            # $0.15 per 1M input tokens
          output: 0.60           # $0.60 per 1M output tokens
        gpt-4o:
          input: 2.50            # $2.50 per 1M input tokens
          output: 10.00          # $10.00 per 1M output tokens
        gpt-4-turbo:
          input: 10.00
          output: 30.00
      anthropic:
        claude-3-5-sonnet-20241022:
          input: 3.00            # $3.00 per 1M input tokens
          output: 15.00          # $15.00 per 1M output tokens
        claude-3-opus-20240229:
          input: 15.00
          output: 75.00

# Bulk Document Loading (User Story 5 - P2)
batch_operations:
  enabled: true                  # Enable batch operations (10x+ speedup)
  default_batch_size: 1000       # Default batch size for bulk operations
  max_batch_size: 10000          # Maximum allowed batch size
  show_progress: true            # Show progress bars for batch operations
  error_handling: "continue"     # Error handling strategy: continue, stop, rollback
  max_errors_in_response: 100    # Maximum number of errors to return in response

  # Performance targets
  performance:
    target_throughput_docs_per_sec: 1000  # Target: >=1000 docs/sec for 10K documents
    target_bulk_load_time_ms: 10000       # Target: <10s for 10K documents

# Collection Management (User Story 2 - P1)
collections:
  enabled: true                  # Enable collection lifecycle management
  default_collection_id: "default"
  max_collection_id_length: 128
  allowed_collection_id_pattern: "^[a-zA-Z0-9_-]+$"  # Alphanumeric + hyphens/underscores

  # Statistics and metadata
  track_statistics: true         # Track document counts, sizes, timestamps
  compute_size_in_bytes: true    # Calculate total size of documents in collections

# Metadata Schema Discovery (User Story 6 - P3)
schema_discovery:
  enabled: true                  # Enable metadata schema discovery
  default_sample_size: 100       # Default number of documents to sample
  max_sample_size: 200           # Maximum allowed sample size
  min_sample_size: 10            # Minimum allowed sample size

  # Type inference
  type_inference:
    enabled: true                # Enable automatic type inference
    detect_datetime: true        # Detect ISO 8601 datetime strings
    detect_numeric: true         # Detect integers and floats
    detect_boolean: true         # Detect boolean values
    detect_arrays: true          # Detect array types

  # Statistics
  compute_statistics:
    enabled: true                # Compute min/max/avg for numeric fields
    max_example_values: 5        # Maximum number of example values per field

  # Performance target
  performance:
    target_discovery_time_ms: 5000  # Target: <5s for sample_size=200