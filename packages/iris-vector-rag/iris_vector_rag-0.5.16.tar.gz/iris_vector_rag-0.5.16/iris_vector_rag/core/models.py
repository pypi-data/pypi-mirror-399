import uuid
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union
from datetime import datetime
from enum import Enum


def default_id_factory():
    """Generates a default UUID for document ID."""
    return str(uuid.uuid4())


@dataclass(
    frozen=True
)  # frozen=True makes instances hashable if all fields are hashable
class Document:
    """
    Represents a single document or a piece of text content.

    Attributes:
        page_content: The main textual content of the document.
        metadata: A dictionary of additional information about the document
                  (e.g., source, page number, author). Defaults to an empty dict.
        id: A unique identifier for the document. Defaults to a generated UUID string.
            Can be provided if a specific ID is required.
    """

    page_content: str
    # metadata will be stored as a tuple of sorted (key, value) tuples to ensure hashability
    # The __init__ generated by dataclass will still accept a dict for metadata.
    # We will handle the conversion in __post_init__, but this requires frozen=False temporarily,
    # or a custom __init__.
    # For frozen=True, we must ensure the input type is already hashable or convert it
    # before the object is frozen.
    # A common pattern is to use a property or a factory, or to accept an immutable type.

    # Let's adjust so metadata is stored as a tuple of (key, value) items, sorted for consistency.
    # We'll need a custom __init__ if we want to accept a dict and convert it while frozen=True.
    # Or, we can require the user to pass an immutable representation.

    # Simpler: For frozen dataclasses, if a field is mutable (like dict) and part of hashing/equality,
    # it causes issues. If metadata is truly part of the Document's identity for hashing,
    # it should be immutable.
    # Let's make it so that the Document is hashable by its ID primarily, and eq checks all.
    # This means we might need to implement __hash__ manually if metadata dict is kept.
    # Or, if frozen=True, all fields contributing to eq must be hashable.

    # The simplest fix for the test, given frozen=True, is to ensure metadata is stored as an immutable, hashable type.
    # We can convert it in __post_init__ by temporarily unfreezing, or by using a more complex setup.
    # OR, we can define __hash__ manually.
    # Let's try defining __hash__ manually based on all fields, converting metadata on the fly.
    metadata: Dict[str, Any] = field(default_factory=dict)
    id: str = field(default_factory=default_id_factory)

    # __eq__ is auto-generated and compares all fields.
    # For __hash__ to work with a dict field when frozen=True, it's tricky.
    # Python's default for frozen=True is: if __eq__ is True (default) and __hash__ is None (default),
    # it tries to generate __hash__. If it finds an unhashable field like dict, it fails.

    # Let's remove frozen=True for a moment to implement custom __hash__ and __eq__
    # to handle the dict correctly. Or, better, keep frozen=True and make metadata
    # a field that doesn't participate in hashing if it's a dict, or convert it.

    # The test expects doc1 == doc2, where metadata is part of comparison.
    # And it expects doc1 to be hashable.
    # The issue is that dict itself is unhashable.

    # If we keep frozen=True, we must ensure all fields are hashable.
    # We can change metadata to store an immutable representation.
    # For example, a frozenset of items.

    # Let's try this: metadata will be converted to a frozenset of its items.
    # This requires a custom __init__ or a factory, as __post_init__ can't modify fields in a frozen dataclass.

    # Alternative: Keep frozen=True, and for metadata, use a type that is hashable.
    # The test passes `metadata={"a": 1}`.
    # The simplest way to pass the test is to ensure that the `metadata` field,
    # as part of the dataclass, is treated as hashable.
    # This means the `dict` must be converted to a hashable form for the hash calculation.
    # Dataclasses with `frozen=True` will generate `__hash__` if all fields are hashable.
    # Since `dict` is not, it fails.
    # The fix is to ensure `metadata` is stored as a hashable type, e.g., `frozenset(self.metadata.items())`
    # or a tuple of sorted items.

    # Let's modify the field itself to be a tuple of tuples, and handle dict input in __init__
    # This is not possible with frozen=True and default __init__.

    # Easiest for now: make Document not frozen, and implement __eq__ and __hash__.
    # Or, for the test to pass with frozen=True, the test itself needs to ensure it passes
    # hashable metadata, or the Document needs to transform it.

    # The problem is `metadata: Dict[str, Any]`. If this is part of the hash, it fails.
    # If we make it `eq=False, hash=False` for this field, then it would work but not meet test criteria for equality.

    # Let's make metadata a field that is converted to a hashable type.
    # We can use a property, but that's for access.
    # The actual stored type needs to be hashable.

    # Simplest change to pass the test:
    # The test uses `metadata={"a": 1}`.
    # The dataclass tries to hash this dict.
    # If we change the Document to not be frozen, then __hash__ is None by default.
    # But the test `d = {doc1: "test"}` requires hashability.

    # The core issue: a frozen dataclass with a dict field used in eq comparison
    # will not auto-generate a hash function.
    # We need to provide one, or change the field type.
    # Let's provide a custom __hash__.

    def __hash__(self):
        # Convert dict to a hashable form (tuple of sorted items) for hashing
        meta_tuple = tuple(sorted(self.metadata.items()))
        return hash((self.page_content, meta_tuple, self.id))

    # Note: If __hash__ is defined, __eq__ should also be defined if not using dataclass's default.
    # Since frozen=True implies eq=True by default, the auto-generated __eq__ should be fine
    # as it compares dictionaries correctly. The issue was only hashing.

    def __post_init__(self):
        """Post-initialization checks."""
        if not isinstance(self.page_content, str):
            raise TypeError("page_content must be a string.")
        if not isinstance(self.metadata, dict):
            raise TypeError("metadata must be a dictionary.")
        if not isinstance(self.id, str):
            raise TypeError("id must be a string.")
        if not self.id:
            raise ValueError("id cannot be empty.")


@dataclass(frozen=True)
class Entity:
    """
    Represents an extracted entity from text processing.

    Attributes:
        text: The actual text span of the entity as it appears in the source
        entity_type: The type/category of the entity (e.g., PERSON, DISEASE, DRUG, etc.)
        confidence: Confidence score of the extraction (0.0 to 1.0)
        start_offset: Character offset where entity starts in source text
        end_offset: Character offset where entity ends in source text
        source_document_id: ID of the document this entity was extracted from
        metadata: Additional information about the entity (embeddings, context, etc.)
        id: Unique identifier for the entity
    """

    text: str
    entity_type: str
    confidence: float
    start_offset: int
    end_offset: int
    source_document_id: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    id: str = field(default_factory=default_id_factory)

    def __hash__(self):
        """Convert dict metadata to hashable form for hashing."""
        meta_tuple = tuple(sorted(self.metadata.items()))
        return hash(
            (
                self.text,
                self.entity_type,
                self.confidence,
                self.start_offset,
                self.end_offset,
                self.source_document_id,
                meta_tuple,
                self.id,
            )
        )

    def __post_init__(self):
        """Post-initialization validation."""
        if not isinstance(self.text, str) or not self.text.strip():
            raise ValueError("Entity text must be a non-empty string.")
        if not isinstance(self.entity_type, str) or not self.entity_type.strip():
            raise ValueError("Entity type must be a non-empty string.")
        if not (0.0 <= self.confidence <= 1.0):
            raise ValueError("Confidence must be between 0.0 and 1.0.")
        if not isinstance(self.start_offset, int) or self.start_offset < 0:
            raise ValueError("Start offset must be a non-negative integer.")
        if not isinstance(self.end_offset, int) or self.end_offset < self.start_offset:
            raise ValueError("End offset must be >= start offset.")
        if (
            not isinstance(self.source_document_id, str)
            or not self.source_document_id.strip()
        ):
            raise ValueError("Source document ID must be a non-empty string.")
        if not isinstance(self.metadata, dict):
            raise TypeError("Metadata must be a dictionary.")
        if not isinstance(self.id, str) or not self.id.strip():
            raise ValueError("Entity ID must be a non-empty string.")


@dataclass(frozen=True)
class Relationship:
    """
    Represents a relationship between two entities.

    Attributes:
        source_entity_id: ID of the source entity in the relationship
        target_entity_id: ID of the target entity in the relationship
        relationship_type: Type of relationship (e.g., "causes", "treats", "interacts_with")
        confidence: Confidence score of the relationship extraction (0.0 to 1.0)
        source_document_id: ID of the document where this relationship was found
        metadata: Additional information about the relationship (context, evidence, etc.)
        id: Unique identifier for the relationship
    """

    source_entity_id: str
    target_entity_id: str
    relationship_type: str
    confidence: float
    source_document_id: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    id: str = field(default_factory=default_id_factory)

    def __hash__(self):
        """Convert dict metadata to hashable form for hashing."""
        meta_tuple = tuple(sorted(self.metadata.items()))
        return hash(
            (
                self.source_entity_id,
                self.target_entity_id,
                self.relationship_type,
                self.confidence,
                self.source_document_id,
                meta_tuple,
                self.id,
            )
        )

    def __post_init__(self):
        """Post-initialization validation."""
        if (
            not isinstance(self.source_entity_id, str)
            or not self.source_entity_id.strip()
        ):
            raise ValueError("Source entity ID must be a non-empty string.")
        if (
            not isinstance(self.target_entity_id, str)
            or not self.target_entity_id.strip()
        ):
            raise ValueError("Target entity ID must be a non-empty string.")
        if self.source_entity_id == self.target_entity_id:
            raise ValueError("Source and target entity IDs cannot be the same.")
        if (
            not isinstance(self.relationship_type, str)
            or not self.relationship_type.strip()
        ):
            raise ValueError("Relationship type must be a non-empty string.")
        if not (0.0 <= self.confidence <= 1.0):
            raise ValueError("Confidence must be between 0.0 and 1.0.")
        if (
            not isinstance(self.source_document_id, str)
            or not self.source_document_id.strip()
        ):
            raise ValueError("Source document ID must be a non-empty string.")
        if not isinstance(self.metadata, dict):
            raise TypeError("Metadata must be a dictionary.")
        if not isinstance(self.id, str) or not self.id.strip():
            raise ValueError("Relationship ID must be a non-empty string.")


# Common entity types for biomedical domain (based on SOTA research)
class EntityTypes:
    """Standard entity types for biomedical and general domain extraction."""

    # Biomedical entities (from OpenMed and medical NER research)
    PERSON = "PERSON"
    DISEASE = "DISEASE"
    DRUG = "DRUG"
    TREATMENT = "TREATMENT"
    SYMPTOM = "SYMPTOM"
    GENE = "GENE"
    PROTEIN = "PROTEIN"
    ANATOMY = "ANATOMY"
    PROCEDURE = "PROCEDURE"
    DEVICE = "DEVICE"

    # Additional medical entity types for comprehensive coverage
    VIRUS = "VIRUS"
    BACTERIA = "BACTERIA"
    VACCINE = "VACCINE"
    BIOMARKER = "BIOMARKER"

    # General entities
    ORGANIZATION = "ORGANIZATION"
    LOCATION = "LOCATION"
    DATE = "DATE"
    MONEY = "MONEY"
    PERCENT = "PERCENT"
    PRODUCT = "PRODUCT"
    EVENT = "EVENT"

    @classmethod
    def all_types(cls) -> set:
        """Return all available entity types."""
        return {
            cls.PERSON,
            cls.DISEASE,
            cls.DRUG,
            cls.TREATMENT,
            cls.SYMPTOM,
            cls.GENE,
            cls.PROTEIN,
            cls.ANATOMY,
            cls.PROCEDURE,
            cls.DEVICE,
            cls.ORGANIZATION,
            cls.LOCATION,
            cls.DATE,
            cls.MONEY,
            cls.PERCENT,
            cls.PRODUCT,
            cls.EVENT,
        }

    @classmethod
    def biomedical_types(cls) -> set:
        """Return biomedical-specific entity types."""
        return {
            cls.PERSON,
            cls.DISEASE,
            cls.DRUG,
            cls.TREATMENT,
            cls.SYMPTOM,
            cls.GENE,
            cls.PROTEIN,
            cls.ANATOMY,
            cls.PROCEDURE,
            cls.DEVICE,
        }


# Common relationship types
class RelationshipTypes:
    """Standard relationship types for entity connections."""

    # Medical/biomedical relationships
    TREATS = "treats"
    CAUSES = "causes"
    PREVENTS = "prevents"
    INTERACTS_WITH = "interacts_with"
    LOCATED_IN = "located_in"
    PART_OF = "part_of"
    ASSOCIATED_WITH = "associated_with"

    # General relationships
    RELATED_TO = "related_to"
    MENTIONS = "mentions"
    REFERS_TO = "refers_to"
    WORKS_FOR = "works_for"
    BASED_IN = "based_in"
    HAPPENS_ON = "happens_on"

    @classmethod
    def all_types(cls) -> set:
        """Return all available relationship types."""
        return {
            cls.TREATS,
            cls.CAUSES,
            cls.PREVENTS,
            cls.INTERACTS_WITH,
            cls.LOCATED_IN,
            cls.PART_OF,
            cls.ASSOCIATED_WITH,
            cls.RELATED_TO,
            cls.MENTIONS,
            cls.REFERS_TO,
            cls.WORKS_FOR,
            cls.BASED_IN,
            cls.HAPPENS_ON,
        }


# Batch Processing Models (Feature 041: Batch LLM Entity Extraction)


class BatchStatus(str, Enum):
    """Status enumeration for batch processing lifecycle."""

    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    SPLIT = "SPLIT"


@dataclass
class DocumentBatch:
    """
    Represents a batch of documents grouped for batch entity extraction.

    This model supports FR-001 (batch 5-10 documents), FR-006 (token budget enforcement),
    and FR-005 (retry tracking).

    Attributes:
        batch_id: Unique identifier for the batch
        document_ids: List of document IDs in this batch
        batch_size: Number of documents in the batch
        total_token_count: Total token count across all documents
        creation_timestamp: When the batch was created
        processing_status: Current status (PENDING, PROCESSING, COMPLETED, FAILED, SPLIT)
        retry_count: Number of retry attempts (for FR-005 exponential backoff)
    """

    batch_id: str = field(default_factory=default_id_factory)
    document_ids: List[str] = field(default_factory=list)
    batch_size: int = 0
    total_token_count: int = 0
    creation_timestamp: datetime = field(default_factory=datetime.now)
    processing_status: BatchStatus = BatchStatus.PENDING
    retry_count: int = 0

    def add_document(self, document_id: str, token_count: int) -> None:
        """
        Add a document to the batch.

        Args:
            document_id: ID of the document to add
            token_count: Token count of the document

        Raises:
            ValueError: If document ID is invalid or token count is negative
        """
        if not document_id or not isinstance(document_id, str):
            raise ValueError("Document ID must be a non-empty string")
        if token_count < 0:
            raise ValueError("Token count must be non-negative")

        # Prevent duplicates
        if document_id in self.document_ids:
            raise ValueError(f"Document {document_id} already in batch")

        self.document_ids.append(document_id)
        self.batch_size += 1
        self.total_token_count += token_count

    def is_within_budget(self, token_budget: int = 8192) -> bool:
        """
        Check if batch is within token budget (FR-006).

        Args:
            token_budget: Maximum allowed tokens (default: 8192)

        Returns:
            True if batch is within budget, False otherwise
        """
        return self.total_token_count <= token_budget

    def __post_init__(self):
        """Post-initialization validation."""
        if self.batch_size < 0:
            raise ValueError("Batch size must be non-negative")
        if self.total_token_count < 0:
            raise ValueError("Token count must be non-negative")
        if self.retry_count < 0:
            raise ValueError("Retry count must be non-negative")
        if len(self.document_ids) != self.batch_size:
            raise ValueError("Document IDs length must match batch size")


@dataclass
class BatchExtractionResult:
    """
    Represents the result of batch entity extraction.

    This model supports FR-003 (entity traceability), FR-004 (document ID preservation),
    and FR-005 (retry tracking).

    Attributes:
        batch_id: ID of the batch that was processed
        per_document_entities: Mapping of document_id -> list of extracted entities
        per_document_relationships: Mapping of document_id -> list of extracted relationships
        processing_time: Time taken to process the batch (seconds)
        success_status: Whether extraction succeeded
        retry_count: Number of retry attempts (for FR-005)
        error_message: Error message if extraction failed
    """

    batch_id: str
    per_document_entities: Dict[str, List[Entity]] = field(default_factory=dict)
    per_document_relationships: Dict[str, List[Relationship]] = field(
        default_factory=dict
    )
    processing_time: float = 0.0
    success_status: bool = True
    retry_count: int = 0
    error_message: Optional[str] = None

    def get_all_entities(self) -> List[Entity]:
        """
        Get all entities across all documents in the batch.

        Returns:
            Flattened list of all entities
        """
        all_entities = []
        for entities in self.per_document_entities.values():
            all_entities.extend(entities)
        return all_entities

    def get_all_relationships(self) -> List[Relationship]:
        """
        Get all relationships across all documents in the batch.

        Returns:
            Flattened list of all relationships
        """
        all_relationships = []
        for relationships in self.per_document_relationships.values():
            all_relationships.extend(relationships)
        return all_relationships

    def get_entity_count_by_document(self) -> Dict[str, int]:
        """
        Get entity counts grouped by document (for FR-003 quality validation).

        Returns:
            Mapping of document_id -> entity count
        """
        return {
            doc_id: len(entities)
            for doc_id, entities in self.per_document_entities.items()
        }

    def __post_init__(self):
        """Post-initialization validation."""
        if not self.batch_id:
            raise ValueError("Batch ID cannot be empty")
        if self.processing_time < 0:
            raise ValueError("Processing time must be non-negative")
        if self.retry_count < 0:
            raise ValueError("Retry count must be non-negative")


@dataclass
class ProcessingMetrics:
    """
    Represents processing metrics for batch entity extraction (FR-007).

    This model tracks statistics required by FR-007: total batches, average processing time,
    entity extraction rate, zero-entity documents, and failure tracking.

    Attributes:
        total_batches_processed: Total number of batches processed
        total_documents_processed: Total number of documents processed
        average_batch_processing_time: Average time per batch (seconds)
        speedup_factor: Actual speedup achieved vs single-document baseline
        entity_extraction_rate_per_batch: Average entities extracted per batch
        zero_entity_documents_count: Count of documents with zero entities (quality signal)
        failed_batches_count: Number of batches that failed
        retry_attempts_total: Total retry attempts across all batches
    """

    total_batches_processed: int = 0
    total_documents_processed: int = 0
    average_batch_processing_time: float = 0.0
    speedup_factor: float = 1.0
    entity_extraction_rate_per_batch: float = 0.0
    zero_entity_documents_count: int = 0
    failed_batches_count: int = 0
    retry_attempts_total: int = 0

    def update_with_batch(
        self,
        batch_result: BatchExtractionResult,
        batch_size: int,
        single_doc_baseline_time: float = 7.2,
    ) -> None:
        """
        Update metrics with a new batch result.

        Args:
            batch_result: Result from batch processing
            batch_size: Number of documents in the batch
            single_doc_baseline_time: Baseline time per document (default: 7.2s from spec)
        """
        # Update counts
        self.total_batches_processed += 1
        self.total_documents_processed += batch_size
        self.retry_attempts_total += batch_result.retry_count

        if not batch_result.success_status:
            self.failed_batches_count += 1
            return  # Don't update other metrics for failed batches

        # Update average processing time (running average)
        total_time = (
            self.average_batch_processing_time * (self.total_batches_processed - 1)
        )
        total_time += batch_result.processing_time
        self.average_batch_processing_time = total_time / self.total_batches_processed

        # Update entity extraction rate (running average)
        entity_counts = batch_result.get_entity_count_by_document()
        total_entities = sum(entity_counts.values())

        total_entity_rate = (
            self.entity_extraction_rate_per_batch * (self.total_batches_processed - 1)
        )
        total_entity_rate += total_entities
        self.entity_extraction_rate_per_batch = (
            total_entity_rate / self.total_batches_processed
        )

        # Update zero-entity document count
        for count in entity_counts.values():
            if count == 0:
                self.zero_entity_documents_count += 1

        # Calculate speedup factor
        self.speedup_factor = self.calculate_speedup(single_doc_baseline_time)

    def calculate_speedup(self, single_doc_baseline_time: float = 7.2) -> float:
        """
        Calculate speedup factor vs single-document baseline (FR-002).

        Args:
            single_doc_baseline_time: Baseline time per document (default: 7.2s from spec)

        Returns:
            Speedup factor (e.g., 3.0 for 3x speedup)
        """
        if self.average_batch_processing_time <= 0:
            return 1.0

        # Calculate average documents per batch
        if self.total_batches_processed == 0:
            return 1.0

        avg_docs_per_batch = (
            self.total_documents_processed / self.total_batches_processed
        )

        # Speedup = (baseline_time * docs_per_batch) / actual_batch_time
        baseline_batch_time = single_doc_baseline_time * avg_docs_per_batch
        return baseline_batch_time / self.average_batch_processing_time

    def __post_init__(self):
        """Post-initialization validation."""
        if self.total_batches_processed < 0:
            raise ValueError("Total batches must be non-negative")
        if self.total_documents_processed < 0:
            raise ValueError("Total documents must be non-negative")
        if self.average_batch_processing_time < 0:
            raise ValueError("Average batch processing time must be non-negative")
        if self.speedup_factor < 0:
            raise ValueError("Speedup factor must be non-negative")
        if self.entity_extraction_rate_per_batch < 0:
            raise ValueError("Entity extraction rate must be non-negative")
        if self.zero_entity_documents_count < 0:
            raise ValueError("Zero entity documents count must be non-negative")
        if self.failed_batches_count < 0:
            raise ValueError("Failed batches count must be non-negative")
        if self.retry_attempts_total < 0:
            raise ValueError("Retry attempts total must be non-negative")


# Coverage Analysis Models

import re


# Critical modules requiring 80% coverage per constitutional requirements
CRITICAL_MODULES = {
    "iris_rag.config",
    "iris_rag.validation",
    "iris_rag.pipelines",
    "iris_rag.services",
    "iris_rag.storage"
}


@dataclass
class CoverageReport:
    """Data model for coverage analysis reports.

    Represents a complete coverage analysis with overall metrics,
    module-level breakdowns, and metadata for tracking and comparison.
    """

    # Required fields
    report_id: str
    timestamp: datetime
    overall_coverage_percentage: float
    total_lines: int
    covered_lines: int
    analysis_duration_seconds: float

    # Optional metadata fields
    git_commit_hash: Optional[str] = None
    ci_build_id: Optional[str] = None
    branch_coverage_percentage: Optional[float] = None

    # Module coverage breakdown
    module_coverage: List[Dict[str, Any]] = field(default_factory=list)

    def __post_init__(self):
        """Validate report data after initialization."""
        self._validate_percentages()
        self._validate_line_counts()
        self._validate_report_id()

    def _validate_percentages(self):
        """Validate coverage percentages are within valid range."""
        if not 0 <= self.overall_coverage_percentage <= 100:
            raise ValueError("Coverage percentage must be between 0 and 100")

        if self.branch_coverage_percentage is not None:
            if not 0 <= self.branch_coverage_percentage <= 100:
                raise ValueError("Branch coverage percentage must be between 0 and 100")

    def _validate_line_counts(self):
        """Validate line count consistency."""
        if self.covered_lines > self.total_lines:
            raise ValueError("Covered lines cannot exceed total lines")

        if self.total_lines < 0 or self.covered_lines < 0:
            raise ValueError("Line counts must be non-negative")

    def _validate_report_id(self):
        """Validate report ID format."""
        if not self.report_id or len(self.report_id.strip()) == 0:
            raise ValueError("Invalid report ID format")

        if len(self.report_id) > 100:
            raise ValueError("Invalid report ID format")

        # Check for invalid characters
        if not re.match(r'^[a-zA-Z0-9\-_]+$', self.report_id):
            raise ValueError("Invalid report ID format")

    @classmethod
    def from_line_counts(
        cls,
        report_id: str,
        timestamp: datetime,
        total_lines: int,
        covered_lines: int,
        analysis_duration_seconds: float,
        **kwargs
    ) -> 'CoverageReport':
        """Create report with automatic percentage calculation."""
        if total_lines == 0:
            overall_coverage_percentage = 0.0
        else:
            overall_coverage_percentage = (covered_lines / total_lines) * 100.0

        return cls(
            report_id=report_id,
            timestamp=timestamp,
            overall_coverage_percentage=overall_coverage_percentage,
            total_lines=total_lines,
            covered_lines=covered_lines,
            analysis_duration_seconds=analysis_duration_seconds,
            **kwargs
        )

    def to_dict(self) -> Dict[str, Any]:
        """Serialize report to dictionary for API responses."""
        data = {
            'report_id': self.report_id,
            'timestamp': self.timestamp.isoformat(),
            'overall_coverage_percentage': self.overall_coverage_percentage,
            'total_lines': self.total_lines,
            'covered_lines': self.covered_lines,
            'analysis_duration_seconds': self.analysis_duration_seconds,
            'module_coverage': self.module_coverage
        }

        # Add optional fields if present
        if self.git_commit_hash:
            data['git_commit_hash'] = self.git_commit_hash
        if self.ci_build_id:
            data['ci_build_id'] = self.ci_build_id
        if self.branch_coverage_percentage is not None:
            data['branch_coverage_percentage'] = self.branch_coverage_percentage

        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CoverageReport':
        """Deserialize report from dictionary data."""
        # Parse timestamp
        if isinstance(data['timestamp'], str):
            timestamp = datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00'))
        else:
            timestamp = data['timestamp']

        # Extract required fields
        required_fields = {
            'report_id': data['report_id'],
            'timestamp': timestamp,
            'overall_coverage_percentage': data['overall_coverage_percentage'],
            'total_lines': data['total_lines'],
            'covered_lines': data['covered_lines'],
            'analysis_duration_seconds': data['analysis_duration_seconds']
        }

        # Extract optional fields
        optional_fields = {}
        for field_name in ['git_commit_hash', 'ci_build_id', 'branch_coverage_percentage', 'module_coverage']:
            if field_name in data:
                optional_fields[field_name] = data[field_name]

        return cls(**required_fields, **optional_fields)

    def __lt__(self, other: 'CoverageReport') -> bool:
        """Compare reports by coverage percentage."""
        if not isinstance(other, CoverageReport):
            return NotImplemented
        return self.overall_coverage_percentage < other.overall_coverage_percentage

    def __gt__(self, other: 'CoverageReport') -> bool:
        """Compare reports by coverage percentage."""
        if not isinstance(other, CoverageReport):
            return NotImplemented
        return self.overall_coverage_percentage > other.overall_coverage_percentage

    def __eq__(self, other: object) -> bool:
        """Compare reports by ID and timestamp."""
        if not isinstance(other, CoverageReport):
            return NotImplemented
        return (self.report_id == other.report_id and
                self.timestamp == other.timestamp)

    def __ne__(self, other: object) -> bool:
        """Compare reports by ID and timestamp."""
        return not self.__eq__(other)

    def generate_summary(self) -> str:
        """Generate human-readable summary text."""
        # Format numbers with commas
        total_lines_formatted = f"{self.total_lines:,}"
        covered_lines_formatted = f"{self.covered_lines:,}"

        # Calculate analysis time in human-readable format
        duration_minutes = int(self.analysis_duration_seconds // 60)
        duration_seconds = int(self.analysis_duration_seconds % 60)

        if duration_minutes > 0:
            duration_str = f"{duration_minutes}m {duration_seconds}s"
        else:
            duration_str = f"{duration_seconds}s"

        # Generate summary
        summary = (
            f"Coverage Report {self.report_id}: "
            f"{self.overall_coverage_percentage:.1f}% coverage "
            f"({covered_lines_formatted} of {total_lines_formatted} lines covered). "
            f"Analysis completed in {duration_str} on {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}."
        )

        # Add module count if available
        if self.module_coverage:
            summary += f" Analyzed {len(self.module_coverage)} modules."

        # Add branch coverage if available
        if self.branch_coverage_percentage is not None:
            summary += f" Branch coverage: {self.branch_coverage_percentage:.1f}%."

        return summary


@dataclass
class ModuleCoverage:
    """Data model for module-level coverage analysis.

    Represents coverage metrics for a specific module including
    critical module status, target validation, and detailed breakdowns.
    """

    # Required fields
    module_name: str
    file_path: str
    coverage_percentage: float
    total_lines: int
    covered_lines: int
    is_critical_module: bool
    target_coverage_percentage: float

    # Optional detailed fields
    uncovered_lines: List[int] = field(default_factory=list)
    priority_level: Optional[str] = None
    is_legacy_module: bool = False
    exemption_justification: Optional[str] = None
    analysis_time_ms: Optional[float] = None
    vector_operation_coverage: Optional[float] = None

    def __post_init__(self):
        """Validate module data after initialization."""
        self._validate_percentages()
        self._validate_line_counts()
        self._validate_module_name()
        self._calculate_derived_fields()

    def _validate_percentages(self):
        """Validate coverage percentages are within valid range."""
        if not 0 <= self.coverage_percentage <= 100:
            raise ValueError("Coverage percentage must be between 0 and 100")

        if not 0 <= self.target_coverage_percentage <= 100:
            raise ValueError("Target coverage percentage must be between 0 and 100")

        if self.vector_operation_coverage is not None:
            if not 0 <= self.vector_operation_coverage <= 100:
                raise ValueError("Vector operation coverage must be between 0 and 100")

    def _validate_line_counts(self):
        """Validate line count consistency."""
        if self.covered_lines > self.total_lines:
            raise ValueError("Covered lines cannot exceed total lines")

        if self.total_lines < 0 or self.covered_lines < 0:
            raise ValueError("Line counts must be non-negative")

    def _validate_module_name(self):
        """Validate module name format."""
        if not self.module_name or len(self.module_name.strip()) == 0:
            raise ValueError("Invalid module name format")

        # Check for unreasonably deep nesting
        if self.module_name.count('.') > 10:
            raise ValueError("Invalid module name format")

        # Basic format validation for Python module names
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*(\.[a-zA-Z_][a-zA-Z0-9_]*)*$', self.module_name):
            # Allow some special characters for test modules
            if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_.-]*$', self.module_name):
                raise ValueError("Invalid module name format")

    def _calculate_derived_fields(self):
        """Calculate derived fields based on module data."""
        # Set priority level if not explicitly provided
        if self.priority_level is None:
            self.priority_level = self._calculate_priority_level()

    def _calculate_priority_level(self) -> str:
        """Calculate priority level based on module characteristics."""
        # High priority: config and validation (foundational)
        if self.module_name in ["iris_rag.config", "iris_rag.validation"]:
            return "HIGH"

        # Medium/High priority: other critical modules
        if self.is_critical_module:
            return "MEDIUM"

        # Lower priority for non-critical modules
        return "LOW"

    @property
    def target_met(self) -> bool:
        """Check if module meets its coverage target."""
        return self.coverage_percentage >= self.target_coverage_percentage

    @classmethod
    def create_for_module(
        cls,
        module_name: str,
        file_path: str,
        coverage_percentage: float,
        total_lines: int,
        covered_lines: int,
        **kwargs
    ) -> 'ModuleCoverage':
        """Create ModuleCoverage with automatic critical module detection."""
        # Determine if module is critical
        is_critical = module_name in CRITICAL_MODULES

        # Set target coverage based on critical status
        if is_critical:
            target_coverage = 80.0
        else:
            target_coverage = 60.0

        # Override target if explicitly provided
        if 'target_coverage_percentage' in kwargs:
            target_coverage = kwargs.pop('target_coverage_percentage')

        return cls(
            module_name=module_name,
            file_path=file_path,
            coverage_percentage=coverage_percentage,
            total_lines=total_lines,
            covered_lines=covered_lines,
            is_critical_module=is_critical,
            target_coverage_percentage=target_coverage,
            **kwargs
        )

    def to_dict(self) -> Dict[str, Any]:
        """Serialize module coverage to dictionary for API responses."""
        data = {
            'module_name': self.module_name,
            'file_path': self.file_path,
            'coverage_percentage': self.coverage_percentage,
            'total_lines': self.total_lines,
            'covered_lines': self.covered_lines,
            'is_critical_module': self.is_critical_module,
            'target_coverage_percentage': self.target_coverage_percentage,
            'target_met': self.target_met,
            'uncovered_lines': self.uncovered_lines,
            'is_legacy_module': self.is_legacy_module
        }

        # Add optional fields if present
        if self.priority_level:
            data['priority_level'] = self.priority_level
        if self.exemption_justification:
            data['exemption_justification'] = self.exemption_justification
        if self.analysis_time_ms is not None:
            data['analysis_time_ms'] = self.analysis_time_ms
        if self.vector_operation_coverage is not None:
            data['vector_operation_coverage'] = self.vector_operation_coverage

        return data


@dataclass
class BatchExtractionResult:
    """Represents the result of batch entity extraction."""
    batch_id: str
    per_document_entities: Dict[str, List[Entity]] = field(default_factory=dict)
    per_document_relationships: Dict[str, List[Relationship]] = field(default_factory=dict)
    processing_time: float = 0.0
    success_status: bool = True
    retry_count: int = 0
    error_message: Optional[str] = None

    def get_entity_count_by_document(self) -> Dict[str, int]:
        return {doc_id: len(entities) for doc_id, entities in self.per_document_entities.items()}

@dataclass
class ProcessingMetrics:
    """Represents processing metrics for batch entity extraction."""
    total_batches_processed: int = 0
    total_documents_processed: int = 0
    average_batch_processing_time: float = 0.0
    speedup_factor: float = 1.0
    entity_extraction_rate_per_batch: float = 0.0
    zero_entity_documents_count: int = 0
    failed_batches_count: int = 0
    retry_attempts_total: int = 0

    def update_with_batch(self, batch_result: BatchExtractionResult, batch_size: int) -> None:
        self.total_batches_processed += 1
        self.total_documents_processed += batch_size

@dataclass(frozen=True)
class CacheEntry:
    """Represents a cached LLM response."""
    prompt: str
    response: str
    model_name: str
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass(frozen=True)
class BenchmarkQuery:
    """
    Standardized query object for multi-hop RAG evaluation.

    Attributes:
        id: Unique query identifier
        question: The natural language question
        answer: Gold-standard answer (string or list)
        supporting_docs: List of ground-truth document IDs required for answering
        metadata: Dataset-specific attributes (e.g., question_decomposition)
    """

    id: str
    question: str
    answer: Union[str, List[str]]
    supporting_docs: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
