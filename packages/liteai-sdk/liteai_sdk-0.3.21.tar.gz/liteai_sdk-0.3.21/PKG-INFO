Metadata-Version: 2.4
Name: liteai_sdk
Version: 0.3.21
Summary: A wrapper of LiteLLM
Author-email: BHznJNs <bhznjns@outlook.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
License-File: LICENSE
Requires-Dist: litellm>=1.80.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.2.1 ; extra == "dev"
Requires-Dist: pytest-cov ; extra == "test"
Requires-Dist: pytest-mock ; extra == "test"
Requires-Dist: pytest-runner ; extra == "test"
Requires-Dist: pytest ; extra == "test"
Requires-Dist: pytest-github-actions-annotate-failures ; extra == "test"
Project-URL: Source, https://github.com/BHznJNs/liteai
Project-URL: Tracker, https://github.com/BHznJNs/liteai/issues
Provides-Extra: dev
Provides-Extra: test

# LiteAI-SDK

LiteAI-SDK is a wrapper of LiteLLM which provides a more intuitive API and [AI SDK](https://github.com/vercel/ai) like DX.

## Installation

```
pip install liteai-sdk
```

### Develop with coding agent

You can access the complete usage guidance with [llms.txt](https://raw.githubusercontent.com/BHznJNs/liteai/refs/heads/main/llms.txt), just give it to your coding agent to tell it how to use LiteAI-SDK.

## Examples

Below is a simple example of just a API call:

```python
import os
from dotenv import load_dotenv
from liteai_sdk import LLM, LlmProviders, LlmRequestParams, UserMessage

load_dotenv()

llm = LLM(provider=LlmProviders.OPENAI,
          api_key=os.getenv("API_KEY", ""),
          base_url=os.getenv("BASE_URL", ""))

response = llm.generate_text_sync( # sync API of generate_text
    LlmRequestParams(
        model="deepseek-v3.1",
        messages=[UserMessage(content="Hello.")]))
print(response)
```

Below is an example that shows the automatically tool call:

```python
import os
from dotenv import load_dotenv
from liteai_sdk import LLM, LlmProviders, LlmRequestParams, UserMessage

load_dotenv()

def example_tool():
    """
    This is a test tool that is used to test the tool calling functionality.
    """
    print("The example tool is called.")
    return "Hello World"

llm = LLM(provider=LlmProviders.OPENAI,
          api_key=os.getenv("API_KEY", ""),
          base_url=os.getenv("BASE_URL", ""))

params = LlmRequestParams(
        model="deepseek-v3.1",
        tools=[example_tool],
        execute_tools=True,
        messages=[UserMessage(content="Please call the tool example_tool.")])

print("User: ", "Please call the tool example_tool.")
messages = llm.generate_text_sync(params)
for message in messages:
    match message.role:
        case "assistant":
            print("Assistant: ", message.content)
        case "tool":
            print("Tool: ", message.result)
```

