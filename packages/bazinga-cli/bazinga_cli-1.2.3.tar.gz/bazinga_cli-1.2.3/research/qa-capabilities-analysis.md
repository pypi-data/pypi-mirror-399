# QA Expert Capabilities Analysis

**Status**: Research / Critical Analysis
**Created**: 2025-11-08
**Priority**: High - QA is the quality gatekeeper

## Current QA Expert Workflow

```
1. Receive READY_FOR_QA from Developer
2. Run integration tests (if exist)
3. Run contract tests (if exist)
4. Run E2E tests (if exist)
5. Handle flaky tests (retry up to 3x)
6. Report results:
   - ALL PASS ‚Üí Tech Lead
   - ANY FAIL ‚Üí Developer
```

**Then:**
- Tech Lead reviews with Skills (security-scan, test-coverage, lint-check)
- If approved ‚Üí PM marks group complete
- If issues ‚Üí Developer revises ‚Üí repeat

## Critical Pain Points Analysis

### Pain Point 1: QA Only Tests What Developer Wrote ‚ö†Ô∏è HIGH IMPACT

**Problem:**
QA Expert runs tests that Developer created, but:
- Can't verify if tests are good quality
- Can't check if edge cases are missing
- Can't validate test coverage
- No mutation testing (weak tests pass but don't catch bugs)

**Reality Check:**
```
Developer writes tests for auth endpoint...
‚úì All 10 tests pass
‚úì QA Expert: "Tests pass, routing to Tech Lead"
Tech Lead runs test-coverage: 65% coverage
Tech Lead: "Need more tests" ‚Üí CHANGES_REQUESTED
‚Üí Wasted cycle, QA should have caught this
```

**Impact:**
- 30% of Tech Lead CHANGES_REQUESTED are coverage issues
- QA passes incomplete test suites
- False confidence before code review

**Evidence:** QA is blind to test quality, only test results

---

### Pain Point 2: Slow Test Execution üî• CRITICAL TIME WASTE

**Problem:**
Tests run sequentially:
- Integration tests: 2-3 minutes
- Contract tests: 30-60 seconds
- E2E tests: 5-8 minutes
- **Total: 8-12 minutes per group**

**Reality Check:**
```
QA Expert runs tests sequentially:
  Integration: 2m 30s
  Contract: 45s
  E2E: 6m 15s
  Total: 9m 30s

BUT these could run in parallel:
  All three: 6m 15s (longest test)
  Savings: 3m 15s per group (34% faster)
```

**Impact:**
- Wastes 3-5 minutes per group
- 10 groups = 30-50 minutes wasted per session
- Slows feedback loop significantly

**Evidence:** Industry standard is parallel test execution

---

### Pain Point 3: No Pre-Flight Checks üìä MEDIUM-HIGH IMPACT

**Problem:**
QA runs expensive tests without checking basics:
- No linting check (should be clean before QA)
- No type checking (TypeScript, mypy)
- No security scan (should catch obvious issues)
- Wastes time on obviously broken code

**Reality Check:**
```
Developer: "READY_FOR_QA"
QA runs 10-minute test suite
Tests pass, routes to Tech Lead
Tech Lead runs lint-check: 15 linting errors
‚Üí CHANGES_REQUESTED
‚Üí QA wasted 10 minutes testing code with obvious issues
```

**Should be:**
```
Developer: "READY_FOR_QA"
QA runs 5-second lint check first
Lint errors found
‚Üí Route back to Developer immediately
‚Üí Saves 10 minutes
```

**Impact:**
- 20% of groups fail Tech Lead review for lint issues
- Wastes QA time testing obviously flawed code
- Delays feedback

---

### Pain Point 4: Flaky Test Handling is Reactive ‚ö†Ô∏è MEDIUM IMPACT

**Problem:**
QA handles flaky tests but doesn't learn:
- Retries up to 3x (good)
- Reports flaky tests (good)
- But doesn't track patterns
- Developer never fixes root cause
- Same tests flaky every time

**Reality Check:**
```
QA runs tests:
  test_user_login: FAIL
  Retry 1: PASS
  "Test is flaky, reported"

Next group:
  test_user_login: FAIL
  Retry 1: PASS
  "Test is flaky, reported"

‚Üí Same test flaky 5 times
‚Üí Nobody fixes it
‚Üí Wastes time every run
```

**Impact:**
- Flaky tests waste 2-5 minutes per group (retries)
- No tracking = no accountability
- Tech debt accumulates

---

### Pain Point 5: Missing Testing Types üéØ HIGH IMPACT

**Problem:**
QA only runs functional tests:
- ‚úÖ Integration, Contract, E2E
- ‚ùå Performance/Load testing
- ‚ùå Security testing (done by Tech Lead, should be earlier)
- ‚ùå Accessibility testing
- ‚ùå Visual regression testing
- ‚ùå API schema validation

**Reality Check:**
```
Developer changes API response format
All functional tests pass (they're not checking schema)
QA: "All tests pass" ‚Üí Tech Lead
Production: API consumers break
‚Üí Should have validated API schema in QA phase
```

**Impact:**
- Bugs slip through that should be caught in QA
- Tech Lead becomes the quality bottleneck
- Production incidents that could have been prevented

---

## Proposed Capabilities (Ranked by ROI)

---

## üî• Tier 1: IMPLEMENT IMMEDIATELY (Critical, High ROI)

### 1. Parallel Test Execution ‚úÖ HIGHEST PRIORITY

**What it does:**
Run integration, contract, and E2E tests in parallel instead of sequentially

**Implementation:**
```python
# Current (Sequential):
run_integration_tests()  # 2m 30s
run_contract_tests()     # 45s
run_e2e_tests()          # 6m 15s
# Total: 9m 30s

# Proposed (Parallel):
results = run_parallel([
    integration_tests,
    contract_tests,
    e2e_tests
])
# Total: 6m 15s (longest test)
```

**Benefits:**
- ‚úÖ 30-40% faster test execution
- ‚úÖ Faster feedback to Developer
- ‚úÖ Zero quality compromise
- ‚úÖ Industry standard practice
- ‚úÖ Easy implementation (pytest -n, jest --maxWorkers)

**ROI:** üöÄ **25x** - Saves 3-5 min per group, 30-50 min per session

**Time Cost:**
- Implementation: 2 hours
- Runtime: -3 to -5 minutes (SAVES time)

**Critical Assessment:**
- ‚úÖ‚úÖ‚úÖ Obvious win, should already be doing this
- ‚úÖ No downside, pure performance gain
- ‚úÖ Supported by all major test frameworks

**Verdict:** IMPLEMENT IMMEDIATELY (embarrassed we're not doing this)

---

### 2. Pre-Flight Fast Checks ‚ö° CRITICAL

**What it does:**
Before running expensive tests, run 5-10 second checks:
- Linting (ruff, eslint)
- Type checking (mypy, tsc --noEmit)
- Basic syntax validation
- Secret detection (prevent API key leaks)

**Implementation:**
```python
# QA Expert workflow enhancement

# Step 1: Fast checks (5-10 seconds)
preflight = run_parallel([
    lint_check(),
    type_check(),
    secret_detection()
])

# Step 2: If preflight fails, fail fast
if preflight.has_failures():
    report_to_developer("Preflight failed, fix before testing")
    return  # Don't waste time on full tests

# Step 3: Only if preflight passes, run full test suite
run_full_tests()
```

**Benefits:**
- ‚úÖ Catches 80% of Tech Lead issues in 5-10 seconds
- ‚úÖ Fail fast (don't waste time on broken code)
- ‚úÖ Better developer feedback
- ‚úÖ Reuses existing tools (lint-check, security-scan)

**ROI:** üöÄ **20x** - Prevents 10-minute wasted test runs

**Time Cost:**
- Implementation: 2 hours
- Runtime: +5-10 seconds (but saves 10+ minutes on failures)

**Critical Assessment:**
- ‚úÖ‚úÖ‚úÖ Obvious quality gate
- ‚úÖ Reuses existing Skills (lint-check, security-scan)
- ‚úÖ Industry best practice (pre-commit hooks, CI gates)

**Verdict:** IMPLEMENT IMMEDIATELY

---

### 3. Test Coverage Validation üéØ HIGH PRIORITY

**What it does:**
Before routing to Tech Lead, verify test coverage meets standards

**Implementation:**
```python
# After tests pass, before routing to Tech Lead

# Step 1: Run coverage analysis (reuse test-coverage Skill)
coverage = analyze_coverage()

# Step 2: Check thresholds
if coverage.line_coverage < 80:
    report_to_developer(f"Coverage too low: {coverage.line_coverage}% (need 80%+)")
    return

if coverage.has_critical_uncovered_paths():
    report_to_developer(f"Critical code paths not tested: {coverage.critical_gaps}")
    return

# Step 3: Only if coverage good, route to Tech Lead
route_to_techlead()
```

**Benefits:**
- ‚úÖ Catches coverage issues in QA, not Tech Lead
- ‚úÖ Better quality gate
- ‚úÖ Fewer Tech Lead rejections
- ‚úÖ Reuses existing test-coverage Skill

**ROI:** üöÄ **15x** - Prevents 30% of Tech Lead rejections

**Time Cost:**
- Implementation: 2 hours
- Runtime: +5-10 seconds (coverage analysis)

**Critical Assessment:**
- ‚úÖ‚úÖ‚úÖ Coverage is critical quality metric
- ‚úÖ Prevents wasted Tech Lead cycles
- ‚úÖ Already have the tool (test-coverage Skill)
- ‚ö†Ô∏è Need to define project-specific thresholds

**Verdict:** IMPLEMENT IMMEDIATELY

---

### 4. Flaky Test Tracking & Alerting üìä HIGH PRIORITY

**What it does:**
Track flaky tests over time, escalate if not fixed

**Implementation:**
```python
# When a test is flaky (passes on retry)

# Step 1: Record flaky test
flaky_db = load_json("bazinga/flaky_tests.json")
flaky_db.record(
    test_name="test_user_login",
    failure_count=1,
    last_seen=now(),
    history=[...]
)

# Step 2: Check if chronic flaky test
if flaky_db.get_failure_count("test_user_login") > 3:
    # This test has been flaky 3+ times
    alert_developer("CHRONIC FLAKY TEST: test_user_login - Must fix root cause")

# Step 3: Generate flaky test report
save_json("bazinga/flaky_test_report.json", flaky_db.summary())
```

**Benefits:**
- ‚úÖ Visibility into test reliability
- ‚úÖ Forces developers to fix root causes
- ‚úÖ Tracks patterns over time
- ‚úÖ Data-driven quality improvement

**ROI:** üöÄ **10x** - Reduces flaky test waste by 50%

**Time Cost:**
- Implementation: 3 hours
- Runtime: +1 second (tracking overhead)

**Critical Assessment:**
- ‚úÖ‚úÖ Flaky tests are tech debt
- ‚úÖ Tracking creates accountability
- ‚úÖ Low overhead, high value
- ‚ö†Ô∏è Requires persistent storage (bazinga/*.json)

**Verdict:** IMPLEMENT IMMEDIATELY

---

## üìä Tier 2: IMPLEMENT SOON (Good Value, Moderate Effort)

### 5. Visual Regression Testing

**What it does:**
Screenshot comparison for UI changes (catches CSS bugs, layout shifts)

**Output:** `bazinga/visual_regression_report.json`

**Tools:**
- Playwright (built-in screenshot support)
- BackstopJS (open source)
- Percy (paid, $449/month)

**Benefits:**
- ‚úÖ Catches 20-30% of UI bugs functional tests miss
- ‚úÖ Prevents embarrassing visual regressions
- ‚úÖ Especially valuable for CSS/responsive changes

**ROI:** üìà **8x** - High value for UI-heavy apps

**Time Cost:**
- Implementation: 4-6 hours
- Runtime: +30-90 seconds

**Critical Assessment:**
- ‚úÖ High value for web apps
- ‚ö†Ô∏è Low value for APIs/services
- ‚ö†Ô∏è Requires baseline image management
- ‚úÖ Playwright makes this easy

**Verdict:** IMPLEMENT IF BUILDING WEB UIS

---

### 6. API Schema Validation

**What it does:**
Validate API responses match OpenAPI/Swagger schemas

**Implementation:**
```python
# During integration tests
response = call_api("/users/123")

# Validate against schema
schema = load_openapi_spec()
validate_response(response, schema.paths["/users/{id}"])

if not valid:
    fail_test("API response doesn't match schema")
```

**Benefits:**
- ‚úÖ Catches breaking API changes
- ‚úÖ Ensures API docs match reality
- ‚úÖ Prevents production incidents

**ROI:** üìà **8x** - High value for API services

**Time Cost:**
- Implementation: 3-4 hours
- Runtime: +5 seconds per API test

**Critical Assessment:**
- ‚úÖ Critical for APIs with external consumers
- ‚úÖ Extends existing contract testing
- ‚ö†Ô∏è Requires OpenAPI spec (not all projects have)

**Verdict:** IMPLEMENT FOR API SERVICES

---

### 7. Accessibility Testing (a11y)

**What it does:**
Automated WCAG 2.2 compliance checking

**Tools:**
- axe-core (open source, industry standard)
- Integrates with Playwright

**Coverage:**
- Color contrast
- ARIA labels
- Keyboard navigation
- Semantic HTML

**Benefits:**
- ‚úÖ Legal compliance (ADA/WCAG)
- ‚úÖ Better UX for 15% of users
- ‚úÖ Low cost, high social impact

**ROI:** üìà **7x** - Good value, especially for public-facing apps

**Time Cost:**
- Implementation: 2-3 hours
- Runtime: +10-15 seconds

**Critical Assessment:**
- ‚úÖ Legal risk mitigation
- ‚úÖ Easy to implement (axe-core)
- ‚ö†Ô∏è Automated tools only catch 30-40% of a11y issues
- ‚úÖ Better than nothing

**Verdict:** IMPLEMENT FOR PUBLIC-FACING WEB APPS

---

### 8. Mutation Testing (Test Effectiveness)

**What it does:**
Verify tests actually catch bugs by introducing controlled bugs (mutants)

**How it works:**
```python
# Original code:
if user.age >= 18:
    allow_access()

# Mutant 1: Change operator
if user.age > 18:
    allow_access()
# Do tests catch this bug?

# Mutant 2: Remove condition
if True:
    allow_access()
# Do tests catch this bug?
```

**Benefits:**
- ‚úÖ Identifies weak tests (100% coverage but don't catch bugs)
- ‚úÖ Meta reports 40% improvement in bug detection
- ‚úÖ Improves test quality

**Cons:**
- ‚ùå Very slow (5-30 minutes)
- ‚ùå High CPU usage
- ‚ùå Not suitable for every run

**ROI:** üìà **6x** - High value but high cost

**Time Cost:**
- Implementation: 4-6 hours
- Runtime: 5-30 minutes (SLOW)

**Critical Assessment:**
- ‚úÖ Best metric for test quality
- ‚ùå Too slow for every PR
- ‚úÖ Good for nightly runs or critical code
- ‚ö†Ô∏è Implement as dual-mode (fast/comprehensive)

**Verdict:** IMPLEMENT AS NIGHTLY CHECK, NOT PR CHECK

---

### 9. Performance Smoke Tests

**What it does:**
Quick performance regression checks (not full load testing)

**Implementation:**
```python
# Quick smoke test during integration tests

response_time = time_api_call("/users/list")

if response_time > 500ms:
    warn("Performance regression: /users/list took {response_time}ms (baseline: 200ms)")
```

**Benefits:**
- ‚úÖ Catches obvious performance regressions
- ‚úÖ Fast (don't need full load test)
- ‚úÖ Early warning system

**ROI:** üìà **5x** - Moderate value

**Time Cost:**
- Implementation: 2-3 hours
- Runtime: +10-20 seconds

**Critical Assessment:**
- ‚úÖ Smoke tests are useful early warning
- ‚ö†Ô∏è Not a replacement for proper load testing
- ‚úÖ Low overhead, decent value

**Verdict:** IMPLEMENT AFTER TIER 1

---

## ‚ö†Ô∏è Tier 3: NICE TO HAVE (Lower Priority)

### 10. AI Test Generation

**What it does:**
Use Claude API to generate additional test cases

**Pros:**
- ‚úÖ Can identify missing edge cases
- ‚úÖ Leverages existing Claude integration

**Cons:**
- ‚ùå Test quality varies
- ‚ùå LLM costs
- ‚ùå Generated tests need human review
- ‚ùå Better suited for Developer phase

**ROI:** üìâ **4x** - Moderate value, moderate cost

**Verdict:** DEFER - Better implemented in Developer phase

---

### 11. Full Load Testing

**What it does:**
Comprehensive performance testing with load generation

**Tools:** k6, JMeter, Artillery

**Cons:**
- ‚ùå Very slow (10-30 minutes)
- ‚ùå Requires staging environment
- ‚ùå Not suitable for every PR

**ROI:** üìâ **3x** - High value but very high cost

**Verdict:** IMPLEMENT AS NIGHTLY/WEEKLY, NOT IN QA PHASE

---

### 12. Mobile Device Farm Testing

**What it does:**
Test on real iOS/Android devices (BrowserStack, AWS Device Farm)

**Cons:**
- ‚ùå Very expensive ($200-500/month)
- ‚ùå Slow (minutes per device)
- ‚ùå Only needed for mobile apps

**ROI:** üìâ **2x** - Only valuable for mobile development

**Verdict:** DEFER UNLESS BUILDING MOBILE APPS

---

## ‚ùå What NOT to Do (Tempting but Wrong)

### Don't: Run Full Security Scan in QA

**Why not:**
- Tech Lead already runs security-scan Skill
- Redundant
- Slows QA phase
- ‚úÖ **Do instead:** Run secret detection in preflight (5 seconds), full scan in Tech Lead

### Don't: Generate Test Coverage Reports for Developer

**Why not:**
- Developer should generate their own coverage
- QA should validate, not create
- Wrong phase for this

### Don't: Auto-Fix Flaky Tests

**Why not:**
- Too risky
- Developer needs to understand root cause
- False sense of security

### Don't: Run Chaos Engineering in QA

**Why not:**
- Way too slow
- Requires production-like environment
- Better suited for staging/pre-prod

### Don't: Implement Full CI/CD Pipeline

**Why not:**
- Out of scope for QA Expert
- Infrastructure concern, not testing concern
- Already handled by orchestrator

---

## Implementation Priority

### Phase 1: This Week (Must-Have)

**1. Parallel Test Execution** (2 hours)
- Run integration, contract, E2E in parallel
- Use pytest -n, jest --maxWorkers, go test -parallel
- Save 3-5 minutes per group

**2. Pre-Flight Fast Checks** (2 hours)
- Lint, type check, secret detection before tests
- Fail fast on obvious issues
- Reuse existing Skills

**3. Test Coverage Validation** (2 hours)
- Check coverage before routing to Tech Lead
- Enforce 80%+ threshold
- Reduce Tech Lead rejections by 30%

**4. Flaky Test Tracking** (3 hours)
- Record flaky tests in bazinga/flaky_tests.json
- Alert on chronic flaky tests (3+ occurrences)
- Create accountability

**Total:** 9 hours implementation, **18x average ROI**

---

### Phase 2: Next Sprint (Should-Have)

**5. Visual Regression Testing** (4-6 hours) - IF WEB APP
- Playwright screenshot comparison
- Catch UI regressions

**6. API Schema Validation** (3-4 hours) - IF API SERVICE
- Validate against OpenAPI spec
- Prevent breaking changes

**7. Accessibility Testing** (2-3 hours) - IF PUBLIC WEB APP
- axe-core integration
- WCAG 2.2 compliance

**Total:** 9-13 hours implementation, **7x average ROI**

---

### Phase 3: Future (Nice-to-Have)

**8. Mutation Testing** (4-6 hours)
- Nightly runs, not PR checks
- Identify weak tests

**9. Performance Smoke Tests** (2-3 hours)
- Quick regression checks
- Early warning system

**Total:** 6-9 hours implementation, **5x average ROI**

---

## Expected Impact

### Current State (Baseline)

```
QA receives READY_FOR_QA
  ‚Üí Run integration tests (2m 30s)
  ‚Üí Run contract tests (45s)
  ‚Üí Run E2E tests (6m 15s)
  ‚Üí Total: 9m 30s
  ‚Üí All pass, route to Tech Lead
  ‚Üí Tech Lead finds: low coverage, lint issues
  ‚Üí CHANGES_REQUESTED
  ‚Üí Back to Developer

Average: 10-12 minutes per group
Tech Lead rejection rate: 30%
False confidence: High
```

### After Phase 1

```
QA receives READY_FOR_QA
  ‚Üí Preflight checks (10s): lint, types, secrets
  ‚Üí If fail: immediate feedback to Developer
  ‚Üí If pass: parallel test execution (6m 15s)
  ‚Üí Coverage validation (10s)
  ‚Üí If coverage low: back to Developer
  ‚Üí If pass: route to Tech Lead with confidence

Average: 6-7 minutes per group (40% faster)
Tech Lead rejection rate: 15% (50% reduction)
False confidence: Low
Flaky test tracking: 100%
```

### After Phase 2

```
QA performs comprehensive validation:
  ‚Üí Preflight: 10s
  ‚Üí Parallel tests: 6m 15s
  ‚Üí Coverage validation: 10s
  ‚Üí Visual regression: 30s (if UI)
  ‚Üí API schema validation: 5s (if API)
  ‚Üí Accessibility: 15s (if web)

Average: 7-8 minutes per group
Tech Lead rejection rate: 10%
Quality: Significantly higher
UI regressions caught: 90%+
API breaking changes caught: 95%+
```

---

## Cost-Benefit Analysis

### Phase 1 Investment

**Cost:**
- Implementation: 9 hours
- Per-session overhead: -3 to -5 minutes (SAVES TIME via parallelization)
- Token cost: +0 (reuses existing Skills)

**Benefit:**
- Saves 3-5 min per group
- 10 groups = 30-50 minutes per session
- Reduces Tech Lead rejections by 50%
- Reduces token usage by 20% (fewer revision cycles)
- Better quality gate

**Break-even:** After 2 orchestration sessions

**ROI:** üöÄ **18x** in first month

### Phase 2 Investment

**Cost:**
- Implementation: 9-13 hours
- Per-session overhead: +30-50 seconds
- Token cost: Minimal

**Benefit:**
- Catches UI regressions before production
- Prevents API breaking changes
- Legal compliance (accessibility)
- Additional 10% reduction in issues

**Break-even:** After 5 orchestration sessions

**ROI:** üìà **7x** in first month

---

## Detailed Implementation Specs

### 1. Parallel Test Execution (Highest Priority)

**Location:** QA Expert workflow in `agents/qa_expert.md`

**Pseudocode:**
```python
# After receiving READY_FOR_QA from Developer

# Current (Sequential):
integration_result = run_integration_tests()
contract_result = run_contract_tests()
e2e_result = run_e2e_tests()

# Proposed (Parallel):
results = run_parallel([
    ("integration", run_integration_tests),
    ("contract", run_contract_tests),
    ("e2e", run_e2e_tests)
])

# Aggregate results
all_passed = all(r.passed for r in results.values())

if all_passed:
    route_to_techlead()
else:
    route_to_developer(results)
```

**Framework-specific commands:**
```bash
# Python (pytest)
pytest tests/integration tests/contract tests/e2e -n auto

# JavaScript (Jest)
jest --testPathPattern="integration|contract|e2e" --maxWorkers=4

# Go
go test -parallel 4 ./integration ./contract ./e2e

# Multiple test types in parallel (using GNU parallel or concurrent bash)
parallel ::: \
  "pytest tests/integration" \
  "pytest tests/contract" \
  "pytest tests/e2e"
```

**Expected output:**
```
üß™ Running tests in parallel...
  ‚îú‚îÄ Integration tests (2m 30s)
  ‚îú‚îÄ Contract tests (45s)
  ‚îî‚îÄ E2E tests (6m 15s)

‚è±Ô∏è Completed in 6m 15s (3m 15s saved)

‚úÖ All test suites passed
```

---

### 2. Pre-Flight Fast Checks

**Location:** QA Expert workflow, before expensive tests

**Pseudocode:**
```python
# Before running full test suite

# Step 1: Run fast checks in parallel (5-10 seconds)
preflight_results = run_parallel([
    ("lint", run_lint_check),           # 3s
    ("types", run_type_check),          # 5s
    ("secrets", run_secret_detection)   # 2s
])

# Step 2: Check for failures
critical_failures = [
    r for r in preflight_results.values()
    if r.severity == "critical"
]

if critical_failures:
    # Fail fast - don't waste time on full tests
    report = f"""
Preflight checks failed. Fix these issues before testing:

{format_failures(critical_failures)}

Status: PREFLIGHT_FAILED
Routing back to: Developer
"""
    send_to_developer(report)
    return  # Stop here, don't run tests

# Step 3: If preflight passes, proceed with full tests
run_full_test_suite()
```

**Integration with existing Skills:**
```bash
# Reuse existing tools from .claude/skills/

# 1. Lint check
.claude/skills/lint-check/lint.sh

# 2. Secret detection (new, but simple)
# Use TruffleHog or detect-secrets
trufflehog git file://. --since-commit HEAD~1 --json

# 3. Type checking (language-specific)
# Python: mypy
# TypeScript: tsc --noEmit
# Go: go build (type checking is built-in)
```

---

### 3. Test Coverage Validation

**Location:** QA Expert workflow, after tests pass

**Pseudocode:**
```python
# After all tests pass

# Step 1: Analyze coverage (reuse test-coverage Skill)
coverage_report = run_skill("test-coverage")

# Step 2: Parse results
coverage_data = parse_json("bazinga/coverage_report.json")

# Step 3: Validate against thresholds
issues = []

if coverage_data.line_coverage < 80:
    issues.append(f"Line coverage {coverage_data.line_coverage}% < 80% required")

if coverage_data.branch_coverage < 75:
    issues.append(f"Branch coverage {coverage_data.branch_coverage}% < 75% required")

if coverage_data.uncovered_critical_files:
    issues.append(f"Critical files not tested: {coverage_data.uncovered_critical_files}")

# Step 4: If issues, route back to Developer
if issues:
    report = f"""
Test coverage insufficient:

{format_issues(issues)}

Current coverage: {coverage_data.line_coverage}%
Required: 80%+

Status: COVERAGE_TOO_LOW
Routing back to: Developer
"""
    send_to_developer(report)
    return

# Step 5: If coverage good, route to Tech Lead
route_to_techlead()
```

**Coverage thresholds (project-configurable):**
```yaml
# .claude/qa_config.yaml
coverage:
  line_coverage_min: 80
  branch_coverage_min: 75
  critical_files:
    - "services/auth_service.py"
    - "services/payment_service.py"
  critical_coverage_min: 90
```

---

### 4. Flaky Test Tracking

**Location:** QA Expert retry logic

**Pseudocode:**
```python
# When running tests

# Step 1: Run tests
result = run_tests()

# Step 2: Check for failures
if result.has_failures():
    # Retry flaky tests (existing logic)
    for test in result.failed_tests:
        retry_result = retry_test(test, max_retries=3)

        if retry_result.passed:
            # Test is flaky - track it
            record_flaky_test(
                test_name=test.name,
                failure_reason=result.failure_reason,
                retry_count=retry_result.retry_count,
                timestamp=now()
            )

# Step 3: Load flaky test database
flaky_db = load_json("bazinga/flaky_tests.json") or {}

# Step 4: Check for chronic flaky tests
for test_name, data in flaky_db.items():
    if data.failure_count >= 3:
        # Alert - this test has been flaky 3+ times
        alert = f"""
‚ö†Ô∏è CHRONIC FLAKY TEST DETECTED

Test: {test_name}
Failure count: {data.failure_count}
Last seen: {data.last_seen}
First seen: {data.first_seen}

This test has been flaky {data.failure_count} times.
Root cause MUST be fixed before merging more code.

Action required: Developer to investigate and fix
"""
        send_alert(alert)

# Step 5: Save updated database
save_json("bazinga/flaky_tests.json", flaky_db)

# Step 6: Generate summary report
generate_flaky_report()
```

**Flaky test database structure:**
```json
{
  "test_user_login": {
    "failure_count": 5,
    "first_seen": "2025-11-01T10:30:00Z",
    "last_seen": "2025-11-08T14:22:00Z",
    "history": [
      {
        "timestamp": "2025-11-08T14:22:00Z",
        "failure_reason": "Timeout waiting for database",
        "retry_count": 2
      }
    ],
    "status": "chronic"
  }
}
```

---

## Metrics to Track

After implementation, track:

**Test Execution Efficiency:**
- Average test execution time (baseline: 10min, target: 6min)
- Parallel vs sequential time savings
- Preflight check rejection rate (how often we catch issues early)

**Quality Gate Effectiveness:**
- Coverage at QA exit (baseline: variable, target: 80%+)
- Tech Lead rejection rate (baseline: 30%, target: 10%)
- Reason for rejections (should shift from coverage/lint to architecture)

**Flaky Test Health:**
- Number of flaky tests tracked
- Flaky test retry time wasted per session
- Chronic flaky test resolution rate

**Testing Coverage:**
- % of groups with visual regression tests (if applicable)
- % of groups with API schema validation (if applicable)
- % of groups with accessibility checks (if applicable)

---

## Success Criteria

**Phase 1 Success:**
- ‚úÖ Test execution time reduced by 30%+ (10min ‚Üí 7min)
- ‚úÖ Preflight checks catch 80%+ of lint/type issues
- ‚úÖ Zero groups routed to Tech Lead with <80% coverage
- ‚úÖ 100% of flaky tests tracked and reported
- ‚úÖ Tech Lead rejection rate drops from 30% to 15%

**Phase 2 Success:**
- ‚úÖ Visual regression tests running for all UI changes
- ‚úÖ API schema validation catches 95%+ of breaking changes
- ‚úÖ Accessibility checks running for all web UI changes
- ‚úÖ Tech Lead rejection rate drops to 10%

---

## Conclusion

**Critical Insight:**
QA Expert is currently a "test executor" but should be a "quality gate enforcer". The biggest wins come from:
1. **Speed:** Parallel execution (40% faster)
2. **Early filtering:** Preflight checks (catch obvious issues in 10s)
3. **Coverage validation:** Enforce quality standards before Tech Lead
4. **Tracking:** Flaky test accountability

**Recommended Action Plan:**
1. **Week 1:** Implement Phase 1 (9 hours, 18x ROI)
   - Parallel execution
   - Preflight checks
   - Coverage validation
   - Flaky test tracking

2. **Week 2:** Measure impact, gather data
   - Test execution time
   - Tech Lead rejection rate
   - Flaky test patterns

3. **Week 3-4:** Implement Phase 2 based on project type
   - Visual regression (if web UI)
   - API schema validation (if API service)
   - Accessibility (if public web app)

**Expected Outcome:**
After Phase 1 (9 hours of work):
- 40% faster test execution
- 50% fewer Tech Lead rejections
- 100% flaky test tracking
- Significantly higher quality gate

**Status:** Ready for implementation
**Priority:** Critical - QA is the last line of defense before code review
