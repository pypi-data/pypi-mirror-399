name: Performance Benchmarks

on:
  pull_request:
    paths:
      - 'common/**'
      - 'lib/**'
      - 'tests/performance/**'
      - '.github/workflows/benchmarks.yml'
  schedule:
    # Run weekly on Sunday at 3am UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    # Allow manual trigger

permissions:
  contents: read
  pull-requests: write  # For PR comments

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    services:
      nats:
        image: nats:2.10-alpine
        ports:
          - 4222:4222
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:8222/healthz || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run benchmarks
        run: |
          pytest tests/performance/test_nats_overhead.py \
            --json-report \
            --json-report-file=benchmark_results.json \
            --json-report-indent=2 \
            -v
        env:
          NATS_URL: nats://localhost:4222
        continue-on-error: true  # Don't fail workflow if benchmarks fail
      
      - name: Generate report
        if: always()
        run: |
          python tests/performance/generate_report.py benchmark_results.json
      
      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark_results.json
            tests/performance/BENCHMARK_RESULTS.md
          retention-days: 90
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = 'tests/performance/BENCHMARK_RESULTS.md';
            
            if (!fs.existsSync(reportPath)) {
              console.log('No benchmark report found');
              return;
            }
            
            const report = fs.readFileSync(reportPath, 'utf8');
            
            // Truncate if too long (GitHub comment limit)
            const maxLength = 65000;
            const summary = report.length > maxLength 
              ? report.substring(0, maxLength) + '\n\n... (truncated)'
              : report;
            
            // Find existing comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.data.find(c => 
              c.user.login === 'github-actions[bot]' && 
              c.body.includes('## Performance Benchmark Results')
            );
            
            const commentBody = `## Performance Benchmark Results\n\n${summary}\n\n---\n*Benchmark run: ${context.sha}*`;
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody,
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody,
              });
            }
