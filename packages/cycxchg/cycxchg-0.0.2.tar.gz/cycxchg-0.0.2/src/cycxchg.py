############################################
# Cycle Exchange                           #
############################################
# Author: Perrin Ruth                      #
# Current version: Dec. 4, 2025            #
############################################
# To-Dos
#  - make into a package
#  - Use sparse formatting for MCB merge matrices.

# necessary packages
import numpy as np 
import networkx as nx
# custom codes for sparse matrices on the GF(2) field
import sparseb as spb     
# base packages useful for optimization
from time import time
from collections import defaultdict




#################################################
# 1. Fundamental Cycle Basis
#      Preliminary basis representation before 
#      computing an MCB. 
#################################################
class Fundamental_Cycle_Basis:
    """
    Generate a fundamental (Kirchoff) cycle basis, a cycle basis
    generated by a spanning tree T with a single cycle for each
    edge in E(G)\\E(T). Treated as a class with two items
      .edges:    list of edges in E(G)\\E(T).
      .edge2idx: dictionary with keys as edges (u,v) in .edges and
                 values their index in .edges
    The input is assumed to be an unweighted, undirected simple graph.
    """
    def __init__(self,G):
        # perform BFS to obtain a spanning tree
        # Node status: 0 unobserved, 1 in queue, 2 left queue
        nx.set_node_attributes(G, 0, 'BFS_color')
        # datasets we want to compute
        edge_list = []            # edges outside BFS tree
        edge_idx  = 0             # corresponding idx for edge
        edge_to_idx_dict = dict() # corresponding indices keys are tuples
        
        for u0 in G.nodes():
            # already observed component
            if G.nodes[u0]['BFS_color'] != 0: continue
            # initialize root node for given component
            queue = [(u0,None)]                        # queue stores tuples (node,parent)
            G.nodes[u0]['BFS_color'] = 1
            # search queue
            while len(queue)>0:
                u,p = queue.pop(0) # current node, parent
                G.nodes[u]['BFS_color'] = 2
                for v in G[u]:
                    if v == p: continue # skip parent
                    if G.nodes[v]['BFS_color'] == 0:          # unobserved, add to queue
                        queue.append([v,u])
                        G.nodes[v]['BFS_color'] = 1
                    elif G.nodes[v]['BFS_color'] == 1:        # edge (u,v) new and not in tree, add to edge_list
                        edge_list.append((u,v))
                        edge_to_idx_dict[(u,v)] = edge_idx
                        edge_idx+=1
                    else:                                     # edge (u,v) not in tree and already added as (v,u), include (u,v) symmetrically
                        edge_to_idx_dict[(u,v)] = edge_to_idx_dict[(v,u)]

        for node in G.nodes():
            c = G.nodes[node].pop('BFS_color') # remove auxilliary coloring from G
        self.edges = edge_list
        self.edge2idx = edge_to_idx_dict




#################################################
# 2. Cycle Decomposition
#      Main class: decomposition of a graphs cycles
#################################################
class cycle_decomposition:
    """
    Decompose the relevant cycles of a graph into pi and sli
    classes. The input is assumed to be a simple unweighted,
    undirected networkx graph. The result is a cycle_decomposition 
    object whose main components are
      .pi_classes
      .sli_classes
      .families
    These are the polyhedron-interchangeability classes, short
    loop-interchangeability classes and modified Vismara cycle
    families, respectively. They are implemented as nested
    classes. See the readme file in the main Github 
      https://github.com/perrineruth/Sampling_Minimum_Cycle_Bases
    as well as the description of the related methods and 
    objects for more information.
    """
    def __init__(self, G, verbose=None):
        ######################################## Preliminary items ########################################
        # Set node labels to integers; allows lists which are faster than dictionaries indexing.
        node_labels = list(G)
        try: 
            labels2node = {u:idx for idx,u in enumerate(node_labels)}   # to go the other direction
        except TypeError:
            raise TypeError('Nodes in G must be hashable')              # required to make node sets/dicts
        G = nx.convert_node_labels_to_integers(G)
        
        # Number of independent cycles
        nu = G.number_of_edges()-G.number_of_nodes()+nx.number_connected_components(G) 
        
        # Fundamental cycle basis (FCB) representation.
        FCB = Fundamental_Cycle_Basis(G)
        ###################################################################################################
        
        if verbose: start = time()
        ### 1. Construct Relevant Cycle Families
        # Give control of construction of the matrix R to an outside function smaller class, 
        # to keep code in the cycle_decomposition initializer high level.
        # Node labels are given to the cycle families which are the lowest level objects used to construct cycles.
        kwargs = {'node_labels':node_labels,'labels2node':labels2node} 
        RelCycMat = Relevant_Cycle_Matrix_Constructor(G, FCB, nu, **kwargs)
        

        ### 2. compute the minimum cycle basis (MCB) using witness vectors, 
        # see Kavitha et al. (2009)
        Wit_Mat = spb.speye(nu)                # initial set of witness vectors Si, standard unit vectors grouped into identity
        MCB = [[] for i in range(nu)]
        MCB_idxs = np.zeros(nu, dtype=int)     # row index of MCB cycles in RelCycMat
        for i in range(nu):
            # get first row with product R0 @ S_i = 1
            Cyc,idx = RelCycMat.get_dot(Wit_Mat[:,i])
            MCB[i] = sorted(Cyc) # as an FCB vector
            MCB_idxs[i] = idx
            # update future witnesses to satisfy orthogonality condition (<Si,Bj>=0 for i>j)
            dot_vec = MCB[i] @ Wit_Mat[:,(i+1):]
            for j,val in enumerate(dot_vec):
                if val: 
                    # since witness i already dots 1, this will make the new witness have dot 1+1=0
                    Wit_Mat[:,(i+1)+j] += Wit_Mat[:,i] 
        
        # Now we can retrieve the matrix and relevant cycle families, since no new families (with larger cycles) need to be generated
        # For large bicomponents this can be roughly 2x faster than constructing the full set of candidate Vismara families
        R_Mat, Rel_Cyc_Fams = RelCycMat.yield_matrix()
        
        if verbose: 
            # This first computation is the biggest (mostly computing the Vismara families)
            stop  = time()
            print(f"Initial Vismara Families & MCB:   {stop-start:5.2f}s")
            start = time()
        

        ### 3. Backtrack to invert witness matrix used to convert cycles representation from the FCB to the MCB
        for k in range(nu-1):
            # if cycle has last MCB basis cycle in its representation then its dot with last witness=1.
            # use this to get the following inversion. Section 4.3
            # update earlier witness if their dot product is affected by adding back the later witness.
            dot_vec = MCB[nu-k-1] @ Wit_Mat[:,:(nu-k-1)] 
            for j,val in enumerate(dot_vec):
                if val:
                    Wit_Mat[:,j] += Wit_Mat[:,nu-k-1]
        # sort MCB and Witnesses to be in increasing length
        MCB_order = sorted(range(nu), key=lambda i: Rel_Cyc_Fams[MCB_idxs[i]].length)
        MCB_idxs  = [MCB_idxs[idx] for idx in MCB_order]
        MCB       = [MCB[idx] for idx in MCB_order]
        Wit_Mat   = Wit_Mat[:,MCB_order]                  # Converts FCB -> MCB vectors
        # Convert cycles to MCB vectors
        R_Mat = R_Mat @ Wit_Mat 

        if verbose: 
            stop = time()
            print(f"Change of basis:                  {stop-start:5.2f}s")
            start= time()
        

        ### 4. Trim cycle families to only contain relevant cycles
        # cycle families are relevant if they are the sum of shorter cycles
        Fam_lens = np.array([Fam.length for Fam in Rel_Cyc_Fams])
        max_MCB_lens = np.zeros_like(Fam_lens)
        # MCB sorted by length, longest cycle in the decomposition is simply the one with largest index
        for j in range(nu): 
            max_MCB_lens[R_Mat[j]] = Rel_Cyc_Fams[MCB_idxs[j]].length
        isRelevant = (Fam_lens==max_MCB_lens) 
        # easier to slice the relevant cycles as columns in the transpose
        RT = R_Mat.T[:,isRelevant] 
        R_Mat = RT.T
        Rel_Cyc_Fams = [Rel_Cyc_Fams[i] for i,val in enumerate(isRelevant) if val]
        # update indices for MCB cycles
        Old2New_idx = np.cumsum(isRelevant)-1
        for i,Old_idx in enumerate(MCB_idxs): MCB_idxs[i] = Old2New_idx[Old_idx]
        
        if verbose: 
            stop = time()
            print(f"Remove irrelevant cycles:         {stop-start:5.2f}s")
            start= time()
        
        ### 5. Find pi classes via breadth-first search
        # consist of connected components in the bipartite graph with R_Mat as a connectivity matrix.
        pi_classes = []            
        # spanning tree over a bipartite graph split by nodes in and outside MCB
        observed = np.zeros(R_Mat.nrows,dtype=bool)
        fam2pi   = np.zeros(R_Mat.nrows,dtype=int)
        for j in range(nu):
            # construct pi class containing jth MCB cycle
            # skip if already observed
            if observed[MCB_idxs[j]]: continue 
            # two items for each pi class, [MCB cycles, and # Families in pi class]
            pi_classes.append([[j],1])
            queue = [j]
            observed[MCB_idxs[j]] = True
            fam2pi[MCB_idxs[j]]=j
            while len(queue)>0:
                queue2 = []
                # search a column (in MCB to outside MCB, i.e., cycles C with given B in their decomposition)
                for col in queue:
                    for row in R_Mat[col].nonzero[1:]: # skip first entry, corresponding to the MCB cycle, use nonzero indexing which works better with slicing
                        if not observed[row]:
                            # cycles in column are sorted by length, stop searching column when cycles are too large
                            if Rel_Cyc_Fams[row].length > Rel_Cyc_Fams[MCB_idxs[col]].length: break 
                            observed[row]=True
                            fam2pi[row]=j
                            queue2.append(row)
                pi_classes[-1][1]+=len(queue2)
                # search a row (outside MCB to MCB, i.e., cycles B in the decomposition of given cycle C)
                queue=[]
                for row in queue2:
                    for col in RT[row]:
                        if not observed[MCB_idxs[col]]:
                            # not large enough yet
                            if Rel_Cyc_Fams[row].length != Rel_Cyc_Fams[MCB_idxs[col]].length: continue
                            observed[MCB_idxs[col]]=True
                            fam2pi[MCB_idxs[col]]=j
                            queue.append(col)
                pi_classes[-1][0].extend(queue)
                pi_classes[-1][1]+=len(queue)
        # pointer for the start of the block in R_Mat for each pi class
        # as pairs for the row and column in R_Mat
        pi_ptrs = np.vstack(( np.cumsum([0]+[pc[1] for pc in pi_classes]),
                              np.cumsum([0]+[len(pc[0]) for pc in pi_classes]) ), dtype=int).T
        # sort R_Mat so that pi classes form consecutive columns
        colOrder = sum([pc[0] for pc in pi_classes],start=[])
        R_Mat = R_Mat[:,colOrder]
        
        ### 6. Sort rows and construct sli classes
        # sort rows in order by 
        #   a. pi class,
        #   b. number of non-zeros in block (so MCB cycles with one item appear first),
        #   c. lexicographically (read right-to-left of binary rows) so that cycles in pi and sli classes are consecutive rows.
        RT = R_Mat.T # transpose for row operations
        def aux_f(row):
            # concatenate a,b,c and use lexicographic sort of lists to do the above sort
            right2left = [col for col in RT[row].nonzero[::-1] \
                          if Rel_Cyc_Fams[row].length == Rel_Cyc_Fams[MCB_idxs[col]].length]
            return [fam2pi[row], len(right2left)] + right2left
        rowOrder = sorted(range(R_Mat.nrows), key = aux_f)
        RT = RT[:,rowOrder] # sort rows
        for i in range(nu): MCB_idxs[i] = rowOrder[MCB_idxs[i]]                # not really needed since they are just the first non-zero index in a column.
        Rel_Cyc_Fams = [Rel_Cyc_Fams[rowOrder[i]] for i in range(R_Mat.nrows)] # sort relevant cycle families
        
        # Collapse sli classes and break into block structure
        keep_row = np.ones(R_Mat.nrows,dtype=bool)
        aux = pi_ptrs.copy() # buffer for pi_ptrs which will replace it when the sli classes collapse to a single row each
        for pc_idx in range(len(pi_ptrs)-1): 
            row0,col0   = pi_ptrs[pc_idx]                   # first row/col of pi class
            nrows,ncols = pi_ptrs[pc_idx+1]-pi_ptrs[pc_idx] # number of rows/cols in block
            # expansion of row in matrix restricted to block of matrix corresponding to pi class
            prev_row = tuple([0]) # just the first MCB item
            for row in range(row0+1,row0+nrows):
                curr_row = tuple([col-col0 for col in RT[row] if col>=col0])
                # if two consecutive rows share the same cycles then they're in the same sli class
                if curr_row == prev_row: 
                    keep_row[row]=False
                    aux[(pc_idx+1):,0] -= 1
                prev_row=curr_row
        # update pi_ptrs
        pi_ptrs = aux
        # sli_ptrs, one per updated row in R_Mat, point to first index in list of cycle Families
        sli_ptrs = np.zeros(sum(keep_row)+1,dtype=int)
        sli_ptrs[:-1] = np.arange(R_Mat.nrows)[keep_row]
        sli_ptrs[-1]  = R_Mat.nrows
        # extract valid rows
        RT = RT[:,keep_row]
                    
        R_Mat=RT.T

        # Steps 5. and 6. are fairly quick.
        if verbose: 
            stop = time()
            print(f"Sort and build pi+sli classes:    {stop-start:5.2f}s")
            start= time()
        
        # entries to save after computation
        # underlying graph and fundamental cycle basis
        self.G   = G
        self.node_labels = node_labels
        self.labels2node = labels2node
        self.nu  = nu
        self.FCB = FCB
        # Vismara Families
        self.families = Rel_Cyc_Fams      # modified Vismara relevant cycle families
        # Associated matrix and decomposition into pi/sli classes
        self.R_Mat = R_Mat
        # short loop-interchangeability, as an object needs a set of families 
        self.sli_classes = [self.short_loop_class(self.families[sli_ptrs[i]:sli_ptrs[i+1]]) for i in range(R_Mat.nrows)]
        # polyhedron-interchangeability
        self.pi_ptrs = pi_ptrs            # pointers for each pi class (as row,col pairs) in the R matrix
        self.pi_classes = []
        for idx in range(len(pi_ptrs)-1):
            # indexes bounding sub matrix
            i0,j0 = self.pi_ptrs[idx]
            i1,j1 = self.pi_ptrs[idx+1]
            # polyhedron-interchangeable class is a group of short loop-interchangeability classes related by a matrix
            # we take transpose because row operations are more important for the pi class
            block_mat = self.R_Mat[i0:i1,j0:j1].T
            # define pi class by a collection of sli classes and a matrix
            self.pi_classes.append(self.polyhedron_class(self.sli_classes[i0:i1],block_mat))

        # make sure these last bits are still fast enough
        if verbose: 
            stop = time()
            print(f"Save cycle class:                 {stop-start:5.2f}s")
            start= time()
        
    
    ######### inner classes #########
    class short_loop_class:
        """
        A collection of cycles that differ by shorter cycles. Composed of
        a set of cycle families.
        """
        def __init__(self,families):
            self.families = families
            
        def random_cycle(self,rep='nodes'):
            """Sample cycle from sli class uniformly at random."""
            weights = np.array([Fam.num_cycles for Fam in self.families])
            idx = np.random.choice(np.arange(len(self.families)), p=weights/sum(weights))
            return self.families[idx].random_cycle(rep=rep)
            
        def arbitrary_cycle(self,rep='nodes'):
            """Return an arbitrary cycle from the sli class."""
            return self.families[0].arbitrary_cycle(rep=rep)

        def nodes(self):
            """Union of nodes in sli class"""
            return set().union(*[Fam.nodes() for Fam in self.families])
        
        def edges(self):
            """Union of edges in sli class"""
            return set().union(*[Fam.edges() for Fam in self.families])

        def contains(self,C,rep='nodes'):
            """Returns True if cycle C belongs to the sli class."""
            return bool(sum([Fam.contains(C,rep=rep) for Fam in self.families]))

        def __contains__(self,item):
            # Wrapper so that the line "C in <sli_class>" uses the contains function.
            # Assumes cycle uses list of nodes representation.
            return self.contains(item)

        @property
        def num_cycles(self):
            """Number of cycles in sli class."""
            return sum([Fam.num_cycles for Fam in self.families])

        @property
        def length(self):
            """Length of cycles in sli class."""
            return self.families[0].length
        
        def __repr__(self):
            return f'short_loop_class(length={self.length},num_cycles={self.num_cycles})'

    @property
    def num_sli_classes(self):
        """Number of sli classes"""
        return len(self.sli_classes)


    class polyhedron_class:
        """
        A maximal set of relevant cycles that all can be exchanged for each other
        in some minimum cycle basis. Stored as a set of short loop-interchangeability
        classes related by a matrix.

        The matrix is the _transpose_ of the corresponding diagonal block in the
        main R matrix.
        """
        def __init__(self,sli_classes,matrix,**kwargs):
            # union of sli classes related by a matrix.
            self.sli_classes = sli_classes  
            # transposed block of the R matrix. Easier to work with rows to construct polyhedra.
            self.matrix = matrix 
        
        def random_sample(self,N=None,rep='nodes'):
            """
            Randomly sample a maximal set of mutually relevant cycles as a set of sli classes. N polyhedron shuffling steps are taken 
            to randomly select sli classes. The default number of steps grows quadratically with the dimension of the polyhedron
            space and linearly with respect to the ratio of the size of the largest and smallest sli classes.
                Output: a list of cycles with format given by rep.
            """
            # get number of shuffle steps
            npoly = self.dim_polyhedra
            match npoly:
                # cannot shuffle without polyhedra
                case 0: N = 0
                # single polyhedron, perfect shuffle in a single step
                case 1: N = 1
                case _:
                    # use default if not given
                    if N is None: 
                        sli_lens = [sc.num_cycles for sc in self.sli_classes]
                        N = int( np.ceil(2*npoly**2 * max(sli_lens)/min(sli_lens)) )
            # use polyhedron MCMC sampling to randomize
            for _ in range(N):
                # matrix form: [I:D], D some matrix to add degeneracy to block
                col_idx1 = np.random.randint(self.dim_polyhedra)+self.rank                          # index of cycle C1 to introduce into pi class
                polyhedron = self.matrix[:,col_idx1].nonzero + [col_idx1]                     # abstract polyhedron (up to shorter cycles)
                Weights = np.array( [1/self.sli_classes[i].num_cycles for i in polyhedron] )  # prob to remove sli class proportional to its inverse size
                col_idx2 = np.random.choice(polyhedron,p=Weights/sum(Weights))                # index of cycle C2 to be replaced by C1
                # if no change then skip
                if col_idx1 == col_idx2: continue
                # otherwise, C2 is in the working MCB (in I matrix), replace with C1
                # update representation of cycles, only needed for cycles other than C2
                update_vec = self.matrix[:,col_idx1] + [col_idx2] # the cycles scriptX used to exchange C1 for C2
                for aux in range(self.dim_polyhedra):
                    aux2 = aux+self.rank
                    if aux2!=col_idx1 and self.matrix[col_idx2,aux2]: 
                        # C2 is in the decomposition of C (which is not C1) 
                        # then we have to add script X to this cycles decomposition
                        self.matrix[:,aux2] += update_vec
                # finally swap the two cycles in our ordering
                aux = self.sli_classes[col_idx1]
                self.sli_classes[col_idx1] = self.sli_classes[col_idx2]
                self.sli_classes[col_idx2] = aux
            # now randomly sample from selected sli classes
            return [sc.random_cycle(rep=rep) for sc in self.sli_classes[:self.rank]]

        def arbitrary_sample(self,rep='nodes'):
            """
            Sample a full rank set of cycles from polyhedron-interchangeability class. The first set of
            sli classes are always ordered to be mutually relevant.
            """
            return [sc.arbitrary_cycle(rep=rep) for sc in self.sli_classes[:self.rank]]

        def nodes(self):
            """Union of nodes in polyhedron-interchangeability class"""
            return set().union(*[sc.nodes() for sc in self.sli_classes])
        
        def edges(self):
            """Union of edges in polyhedron-interchangeability class"""
            return set().union(*[sc.edges() for sc in self.sli_classes])

        def contains(self,C,rep='nodes'):
            """Returns True if cycle C belongs to the polyhedron-interchangeability class."""
            return bool(sum([sc.contains(C,rep=rep) for sc in self.sli_classes]))

        def __contains__(self,item):
            # Wrapper so that the line "C in <pi_Class>" uses the contains function.
            # Assumes cycle uses list of nodes representation.
            return self.contains(item)

        @property    
        def rank(self):
            """Number of cycles in pi class in a given MCB, equal to the rank of the block matrix representing the pi class"""
            return self.matrix.nrows

        @property   
        def dim_polyhedra(self):
            """dimension of the polyhedron space in the pi class, given by the nullity of the block matrix representing the pi class."""
            return self.matrix.ncols - self.matrix.nrows 

        @property
        def num_cycles(self):
            """Number of relevant cycles in polyhedron-interchangeability class (as the union of short loop-interchangeability classes)."""
            return sum([sc.num_cycles for sc in self.sli_classes])

        def num_samples(self):
            """
            Number of ways a full rank set of cycles can be sampled from the polyhedron-interchangeability class. Used 
            to compute the number of minimum cycle bases.

            WARNING: very slow, time-complexity exponential with respect to number of polyhedra
            """
            # matrix of polyhedra
            PMat0 = self.matrix.copy()
            # Initially [I_nu; X] rows in X are expansion cycles in polyhedra 
            # Columns in [X'; I_nullity] also include the cycle being expanded...
            # (X already transposed)
            PMat0 = spb.vstack2(PMat0[self.rank:], 
                                spb.speye(self.dim_polyhedra))
            # recursively trim sli classes until they are all mutually relevant.
            # This is significantly faster than joining sli classes despite being exponential time.
            # uses floats to reduce the risk of overflow errors.
            def num_samples_pi_class(r0=0,cur_prod=1.0,PMat=PMat0):
                num_samples = 0.0
                # stopping condition
                if PMat.ncols==0:
                    cur_prod *= np.prod([sc.num_cycles for sc in self.sli_classes[r0:]])
                    num_samples+=cur_prod
                    return num_samples
                # remove another cycle
                for r_idx in range(PMat.nrows):
                    for c_idx,col in enumerate(PMat):
                        if len(col.nonzero)>0 and col.nonzero[0] == r_idx:
                            # update matrix
                            for c_idx2,col2 in enumerate(PMat[c_idx+1:]):
                                if len(col2.nonzero)>0 and col2.nonzero[0] == r_idx:
                                    # no future cols have 1 at r_idx
                                    PMat[c_idx+c_idx2+1] += PMat[c_idx]
                            # move this to first column
                            PMat[c_idx] = PMat[0] 
                            PMat[0]=col
                            # set row to 0 to not interfere with future rows
                            PMat[r_idx,0]=0
                            # step inwards
                            num_samples+=num_samples_pi_class(r0+r_idx+1,cur_prod,PMat[r_idx+1:,1:])
                    # increase product including row r in MCB
                    cur_prod *= self.sli_classes[r0+r_idx].num_cycles
                return num_samples
            # start recursion
            return num_samples_pi_class()
            
        @property
        def length(self):
            """Length of cycles in polyhedron-interchangeability class."""
            return self.sli_classes[0].length

    @property
    def num_pi_classes(self):
        """Number of polyhedron-interchangeability classes"""
        return len(self.pi_classes)


    @property
    def num_relevant_cycles(self):
        """Number of relevant cycles."""
        return sum([pc.num_cycles for pc in self.pi_classes])

    ######### methods: mostly for sampling minimium cycle bases #########
    def get_MCB(self, rep="nodes"):
        """
        Return the initial minimum cycle basis used to sort the cycle space. These are designed to be the arbitrary cycle
        for the first nu sli classes. Output as a list of cycles with the given format.
        """
        # each MCB cycle corresponds to the arbitrary cycle in the first sli class in a column
        return [self.sli_classes[self.R_Mat[i].nonzero[0]].arbitrary_cycle(rep=rep) for i in range(self.nu)]

    # good tests for random MCB:
    # bicyclo[2,2,2]octane with extra paths, like spokes on a bike
    # two large cycles with a hexagon joining them
    # a large cycle with a nested bicyclo[2,2,2]octane
    # degenerate case a (large cycle with a nested bicyclo[2,2,2]octane) where one of the cycles is not relevant
    # this degenerate case causes 99% of the bugs... grr
    def random_MCB(self,Nrand=None,rep="nodes",merge_MCB=False,Nmerge=100,verbose=False):
        """
        Sample minimum cycle basis uniformly at random.
        Inputs:
         - Nrand:     number of randomization steps for polyhedron-interchangeability classes, either 
                      an integer or a function on polyhedron-interchangeability classes to integers.
         - rep:       representation of MCB cycles.
         - merge_MCB: boolean if pairs of MCB cycles should be modified to intersect on paths.
         - Nmerge:    maximum number of steps to merge the MCB as a safety mechanism. 

        Outputs:
         - rMCB (merge_MCB=False): random MCB as a list of cycles
         - Dual (merge_MCB=True):  dual graph representation of the random minimum cycle basis
        """
        # make Nrand into a function regardless
        if not callable(Nrand): Nrandf = lambda _: Nrand
        else:                   Nrandf = Nrand
        ##############################################
        # 1. build random MCB
        # Recursively call on polyhedron-interchangeability classes for cycles, change representation last
        if not merge_MCB:
            # rely on cycle families for rep if no merging is required
            return sum([pc.random_sample(N=Nrandf(pc),rep=rep) for pc in self.pi_classes],start=[])
        # use node representation if we need to merge cycles
        rMCB = sum([pc.random_sample(N=Nrandf(pc),rep='nodes') for pc in self.pi_classes],start=[])

        ##############################################
        # 2. compute and merge intersections 
        DualGraph = nx.Graph()
        DualGraph.add_nodes_from([(i,{'nodes':C,'length':len(C)}) for i,C in enumerate(rMCB)])
        # build initial dual graph from appropriate edges
        badPairs = [] # pairs of cycles (as indices) that need to be merged
        numBad   = 0
        for i,C1 in enumerate(rMCB):
            for aux,C2 in enumerate(rMCB[i+1:]):
                j = aux+i+1 # index of the second cycle
                paths = pair_intersect(C1,C2)
                if len(paths)==1:
                    P = paths[0]
                    # index of first node in each cycle
                    idx_1,idx_2 = C1.index(P[0]),C2.index(P[0])
                    # If orientation of P in C2 is in the negative direction, then swap index
                    if len(P)>1 and ((C2.index(P[1])-C2.index(P[0])) % len(C2)) != 1:   idx_2 = C2.index(P[-1])
                    # Add path as edges in dual graph. Edge i->j is the perspective of the path from cycle i.
                    # assumes i<j, which is true by the structure of the for loops
                    DualGraph.add_edge(i,j, length=len(P)-1, indices=(idx_1,idx_2))
                    # for debug purposes:
                    assert (len(P)-1)<=(min(len(C1),len(C2))//2), 'invalid path length'
                elif len(paths)>1:
                    numBad+=1
                    badPairs.append([i,j])
                    # add edge but skip attributes since they will be modified
                    DualGraph.add_edge(i,j)


        # merge the cycle pairs joined by multiple paths
        merge_step = 0
        if verbose: print(numBad,'initial merge steps')
        while len(badPairs)>0:
            # timeout check
            if merge_step>=Nmerge:
                print(f'Warning: minimum cycle basis failed to merge paths in {Nmerge} steps')
                break
            merge_step+=1
            # pop a random pair with random ordering
            pair = badPairs.pop(np.random.randint(len(badPairs)))
            np.random.shuffle(pair)
            # get cycles
            i,j = pair
            C1,C2 = rMCB[i],rMCB[j]
            if verbose: print(f'''C1,C2 init \nlength {len(C1)}: {C1}, \nlength {len(C2)}: {C2}''')
            flag,Cycs = merge_pair(C1,C2)
            # merge the cycles
            if flag==1: # main case, single replacement cycle for C1
                rMCB[i] = Cycs
                C1 = Cycs
            else:       # degenerate case, two candidates to swap for C1
                # locate which pi class holds C1
                idx = np.where(self.pi_ptrs[:,1]<=i)[0][-1] # index for pi class
                pc  = self.pi_classes[idx]
                row = i-self.pi_ptrs[idx,1]                 # row in dice class matrix corresponding to C1
                assert C1 in pc.sli_classes[row]              # sanity check -> correct sli class
                # determine which cycle C = C1',C1'' replaces C1
                for C in Cycs:
                    for col in range(pc.matrix.ncols):
                        # use if statement to (1) find which column sli class (column because block matrices are transposed) contains
                        #                     (2) and verify C1 is in the expansion of C using the block matrix
                        if C in pc.sli_classes[col] and row in pc.matrix[:,col]:
                            pc.sli_classes[row],pc.sli_classes[col] = pc.sli_classes[col],pc.sli_classes[row]
                            # update MCB cycle
                            rMCB[i] = C
                            C1 = C
                            # update matrix
                            update_vec = pc.matrix[:,col] + [row] # the cycles used to exchange C1 for C2
                            for col2 in range(pc.dim_polyhedra):
                                col2 += pc.rank
                                if col2!=col and pc.matrix[row,col2]: 
                                    pc.matrix[:,col2] += update_vec
                            break # already done, don't check second cycle
            if verbose: print(f'''Adjusted C1: \nlength {len(C1)}: {C1}''')
            # update cycle's nodes in dual graph
            DualGraph.nodes[i]['nodes'] = C1
            
            # reset pairs and matrices for C1 to recompute
            badPairs = [aux for aux in badPairs if i not in aux] # remove bad pairs with C1 and recompute
            # new C1 is in the union of the old C1 and C2. Check the neighbors of the previous cycles to compute the new neighbors
            for k in set(DualGraph[i]).union(DualGraph[j]):
                if i==k: continue
                C2 = rMCB[k]
                paths = pair_intersect(C1,C2)
                # clear edge to reset, avoids errors when trying to reintroduce the edge
                if k in DualGraph[i]: DualGraph.remove_edge(i,k)
                if len(paths) == 1: # valid intersection
                    P = paths[0]
                    # add connection like above
                    idx_1,idx_2 = C1.index(P[0]),C2.index(P[0])
                    if len(P)>1 and ((C2.index(P[1])-C2.index(P[0])) % len(C2)) != 1:   idx_2 = C2.index(P[-1])
                    # indices and edge has an ordering corresponding to i<k
                    if i<k:     DualGraph.add_edge(i,k, length=len(P)-1, indices=(idx_1,idx_2))
                    else:       DualGraph.add_edge(k,i, length=len(P)-1, indices=(idx_2,idx_1))
                    # check valid path length for debugging
                    assert (len(P)-1)<=(min(len(C1),len(C2))//2), f'{C1} {C2} invalid path length'
                elif len(paths)>1: # invalid intersection
                    badPairs.append([i,k])
                    if k not in DualGraph[i]: DualGraph.add_edge(i,k) # cycles will change, skip edge attributes
        if verbose: print(merge_step,'actual merge steps')
            
        if rep!='nodes': 
            print("Warning: only 'nodes' representation is valid when mergeMCB is set True. The nodes representation of cycle i" \
            "can be accessed from the output dual graph using DualGraph[i]['nodes']")
        return DualGraph

    def num_MCB(self):
        """
        Number of distinct minimum cycle bases. 
        
        WARNING: This algorithm can take exponential time in the presence of
        large polyhedron-interchangeability classes.
        WARNING: The number of MCBs grows exponentially with the number of 
        polyhedra. Hence, the number of MCBs is computed using integers, which 
        may lead to rounding errors.
        """
        # solution is given by the product of the number of ways cycles can be sampled from each pi class
        return np.prod([pc.num_samples() for pc in self.pi_classes],dtype=float)

    def essential_cycles(self):
        # list essential cycles
        raise NotImplementedError("")

    def draw_Mat(self,scale=None):
        """Plot the underlying R matrix relating the pi and sli classes"""
        if scale is None:   self.R_Mat.plot()
        else:               self.R_Mat.plot(scale=scale)




#################################################
# 2'. Relevant Cycle Matrix
#      Auxilliary function for matrix 
#      representation of sli/pi classes
#################################################
class Relevant_Cycle_Matrix_Constructor:
    def __init__(self, G, FCB, nu, **kwargs):
        # graph and number of independent cycles
        NeighborLists = [list(G[u]) for u in range(len(G))] # list of lists representation, faster iteration
        self.nu  = nu 
        self.FCB = FCB # fundamental cycle basis
        # initialize iterators for computing Vismara families
        self._edge_iterators = [_Edge_Families(NeighborLists, idx, FCB, **kwargs) for idx in range(self.nu)]
        # initialize everything else to be empty, generate more as needed
        self.Rel_Cyc_Fams = []
        self.R_Mat = spb.zeros(0,self.nu)     # 0 rows without a family
        self.Lmax = 2                                # no cycles length 2, next round is length 3 and 4 by counting Lmax+2
        self.buffer = [None for i in range(self.nu)] # buffer for later when generating cycles
    
    def get_dot(self, S):
        # Get the index of the first Vismara family to have dot product 1 with the input witness
        Cur_Mat = self.R_Mat
        offset = 0
        while (Cyc_idx:=next(Cur_Mat.right_vec_dot_iter(S), False)) is False: # the second argument in next is a default if the iterator empties itself
            # if fails then we need to compute more Vismara families
            # The following extends the R_Mat and outputs the block of larger cycles added so we don't need to search smaller ones again
            offset+=Cur_Mat.nrows
            Cur_Mat = self.extend_Mat()
        # The ':=' operator assigns the index of the first family with dot product 1 to Cyc_idx, return this
        Cyc_idx+=offset
        return self.Rel_Cyc_Fams[Cyc_idx].arbitrary_cycle(), Cyc_idx
        
    def extend_Mat(self):
        # add a buffer
        self.Lmax += 2  # introduce a new round of odd and even cycles
        newOddCycs  = [] # length Lmax-1 
        newEvenCycs = [] # length Lmax
        
        # pass through Vismara families at each edge
        for idx,Fam in enumerate(self.buffer):
            # start with the cycle in the buffer
            if Fam is not None:
                if Fam.length == self.Lmax-1:
                    newOddCycs.append(Fam)
                elif Fam.length == self.Lmax:
                    newEvenCycs.append(Fam)
                else: 
                    # cycle is already larger than Lmax, we can look at the next starting edge, keep the same cycle in the buffer
                    continue
                    
            # iterate through new Vismara families until we reach a cycle larger than Lmax or run out of families
            while (Fam:=next(self._edge_iterators[idx],None)) is not None and Fam.length<=self.Lmax:
                if Fam.length == self.Lmax-1:
                    newOddCycs.append(Fam)
                elif Fam.length == self.Lmax:
                    newEvenCycs.append(Fam)
                else: raise ValueError('Invalid cycle length') # it should not be possible to get here
            
            # update buffer
            self.buffer[idx] = Fam # either a larger cycle or None if out of families
            
        # update Relevant Cycle Families, sorted by length
        newFams = newOddCycs+newEvenCycs # odd cycles shorter than even cycles
        self.Rel_Cyc_Fams += newFams     # previous cycles smaller than current cycles
        # create new matrix for cycles of current length
        NewCols = [[] for i in range(self.nu)]
        for row,Fam in enumerate(newFams):
            for edge in Fam.arbitrary_cycle(rep='FCB'): # edge from FCB representation
                NewCols[edge].append(row)
        New_Mat = spb.sparse_GF2_mat(NewCols,nrows=len(newFams),ncols=self.nu)
        # append to R matrix
        self.R_Mat = spb.vstack2(self.R_Mat,New_Mat)
        # output new matrix
        return New_Mat
        
    def yield_matrix(self):
        return self.R_Mat, self.Rel_Cyc_Fams

# Helper function for constructing relevant cycle families
# - Optimizing is important here: this is currently the highest cost
def _Edge_Families(G, e_idx, FCB, **kwargs):
    N = len(G) # number of nodes, works for list of lists and networkx
    u,v = FCB.edges[e_idx]
    # list of lists representation of directed-acyclic graph
    # with shortest paths to (u,v)
    parents = [[] for _ in range(N)] 
    # node ancestor / closest node between u and v: 
    #    1 = u,  2 = v,  3 = both
    #    0 default for unobserved,
    ancestor = [0]*N # appears to run faster with lists than arrays
    distance = [0]*N # minimum distance to u or v
    numPaths = [0]*N # number of (valid) shortest paths from to u or v
    # good to wrap these into a dictionary - Directed acyclic graph with node properties
    edge_DAG = {'parents': parents, 'ancestor': ancestor, 'distance': distance, 'numPaths': numPaths} 
    # initialize root nodes
    ancestor[u],ancestor[v] = 1,2
    distance[u],distance[v] = 0,0
    numPaths[u],numPaths[v] = 1,1

    isValid = [True]*N             # invalid if following a node with both ancestors...
    queue = [u,v]                  
    for p in queue: 
        # if p is at the tip of an odd relevant cycle family
        if ancestor[p] == 3 and isValid[p]:
            # look at parents for num. path pairs, use DAG to backtrack
            num_cycles = sum([numPaths[x] for x in parents[p] if ancestor[x]==1]) * \
                            sum([numPaths[x] for x in parents[p] if ancestor[x]==2]) # product of path counts to u and v
            # possible that there are none, this is a degenerate case and should be skipped
            if num_cycles > 0: 
                # cycles are 2 shortest paths to u and v and (u,v)
                length = 2*distance[p] + 1
                yield RelCyc_Family((u,v), length, num_cycles, x=p, edge_DAG=edge_DAG, FCB=FCB, **kwargs)
                                                        
        # look at neighbors
        for q in G[p]:
            ###### case 1: v undiscovered #######
            if ancestor[q]==0:
                # inherit distance and ancestor properties directly
                distance[q] = distance[p]+1
                ancestor[q] = ancestor[p]
                queue.append(q)
                # degenerate - if p has ancestor u & v then q is invalid
                if ancestor[p]==3:
                    isValid[q] = False # numpaths already 0,
                # "there is a valid path to p and (p,q) is valid"
                elif numPaths[p]>0 and not ((p,q) in FCB.edge2idx and e_idx<FCB.edge2idx[(p,q)]): 
                    parents[q].append(p)
                    numPaths[q] = numPaths[p]
            ###### case 2: v after u ############
            elif distance[q] > distance[p]:
                if not isValid[q]: continue
                # degenerate - if p has ancestor u & v then q is invalid
                if ancestor[p]==3:
                    isValid[q] = False
                    numPaths[q],parents[q] = 0,[]
                else:
                    # update ancestor if u adds a new one (or if v is already 3 this is no change)
                    if ancestor[p]!=ancestor[q]: ancestor[q]=3
                    # add paths and parent if valid
                    if numPaths[p]>0 and not ((p,q) in FCB.edge2idx and e_idx<FCB.edge2idx[(p,q)]): 
                        parents[q].append(p)
                        numPaths[q]+=numPaths[p]
            ###### case 3: v same distance ###### 
            #  add even family if p has ancestor u, and q ancestor v to avoid double counting
            #  need to remove corner case p=u, q=v, it may make this more efficient to parse this before the loop.
            elif ancestor[p]==1 and ancestor[q]==2 and p!=u:
                num_cycles = numPaths[p] * numPaths[q]
                if num_cycles>0 and ((p,q) not in FCB.edge2idx or e_idx>=FCB.edge2idx[(p,q)]):
                    # cycle is path p->u & q->v and edges (u,v) & (p,q)
                    length = 2*distance[p]+2     
                    yield RelCyc_Family((u,v), length, num_cycles, e1=(p,q), edge_DAG=edge_DAG, FCB=FCB, **kwargs)

# class for representing Vismara's cycle families.
class RelCyc_Family:
    """Description"""
    def __init__(self, e0, length, num_cycles, x=None, e1=None, **kwargs):# edge_DAG=None, FCB=None, node_labels=None):
        self.e0 = e0                    # base edge
        self.length = length            # length of cycles
        self.parity = self.length % 2   # even or odd cycle
        self.num_cycles = num_cycles    # number of cycles in the family
        if self.parity == 0: # even cyc., e1 is opposite edge
            assert (x is None and e1 is not None)
            self.e1 = e1
        else:
            assert (x is not None and e1 is None)
            self.x = x
        # pass additional arguments into class
        #  edge_DAG -> edges from child to parent
        #  FCB -> fundamental cycle basis
        #  node_labels / labels2idx -> convert from node index to label and vice versa
        for key, value in kwargs.items():
            setattr(self, key, value)

    def __repr__(self):
        if self.parity == 0:
            return f"RelCyc_Family(e0={(self.node_labels[self.e0[0]],self.node_labels[self.e0[1]])}, e1={(self.node_labels[self.e1[0]],self.node_labels[self.e1[1]])}, length={self.length}, num_cycles={self.num_cycles})"
        else:
            return f"RelCyc_Family(e0={(self.node_labels[self.e0[0]],self.node_labels[self.e0[1]])}, x={self.node_labels[self.x]}, length={self.length}, num_cycles={self.num_cycles})"

    def arbitrary_cycle(self, rep='FCB'):
        """Extract an arbitrary cycle from a cycle family."""
        # initialize away from the root edge
        if self.parity == 1:
            x = self.x
            u = next(u for u in self.edge_DAG['parents'][x] if self.edge_DAG['ancestor'][u]==1) # arbitrary parent w/ u0 as ancestor
            v = next(v for v in self.edge_DAG['parents'][x] if self.edge_DAG['ancestor'][v]==2)
            left_nodes = [x,u]
            right_nodes = [v]
        else:
            u,v = self.e1
            left_nodes,right_nodes = [u],[v]
        # backtrack cycles to first parent
        while u not in self.e0:
            u = self.edge_DAG['parents'][u][0]
            left_nodes.append(u)
        while v not in self.e0:
            v = self.edge_DAG['parents'][v][0]
            right_nodes.append(v)
        # merge both paths, move upwards through the right path
        left_nodes.extend(right_nodes[::-1])
        nodes = left_nodes
        # convert to desired format
        if rep == 'nodes': 
            return [self.node_labels[u] for u in nodes]
        edges = {tuple(sorted([nodes[i], nodes[(i+1)%len(nodes)]])) for i in range(len(nodes))}
        if rep == 'edges':
            return {(self.node_labels[u],self.node_labels[v]) for u,v in edges}
        elif rep == 'FCB':
            return [self.FCB.edge2idx[e] for e in edges if e in self.FCB.edge2idx]

    def random_cycle(self, rep='FCB'):
        """random_cycle
        choose a random cycle from Vismara's cycle family uniformly at random.
        """
        def weighted_random_node(nodes):
            # draw random node from selection with probability proportional to number of cycles using node
            weights = np.array([self.edge_DAG['numPaths'][u] for u in nodes])
            return np.random.choice(nodes, p=weights/sum(weights))
        # opposite end of cycle
        if self.parity == 1:
            x = self.x
            u = weighted_random_node([u for u in self.edge_DAG['parents'][x] if self.edge_DAG['ancestor'][u]==1])
            v = weighted_random_node([v for v in self.edge_DAG['parents'][x] if self.edge_DAG['ancestor'][v]==2]) 
            left_nodes = [x,u]
            right_nodes = [v]
        else:
            u,v = self.e1
            left_nodes,right_nodes = [u],[v]
        # backtrack cycles at random
        while u not in self.e0:
            u = weighted_random_node(self.edge_DAG['parents'][u])
            left_nodes.append(u)
        while v not in self.e0:
            v = weighted_random_node(self.edge_DAG['parents'][v])
            right_nodes.append(v)
        # merge both paths, move upwards through the right path
        left_nodes.extend(right_nodes[::-1])
        nodes = left_nodes
        if rep == 'nodes': 
            return [self.node_labels[u] for u in nodes]
        edges = {tuple(sorted([nodes[i], nodes[(i+1)%len(nodes)]])) for i in range(len(nodes))}
        if rep == 'edges':
            return {(self.node_labels[u],self.node_labels[v]) for u,v in edges}
        elif rep == 'FCB':
            return [self.FCB.edge2idx[e] for e in edges if e in self.FCB.edge2idx]
                
    def nodes(self):
        """
        Union of nodes belonging to cycles in the cycle family. Output format is a set 
        where the order of nodes is not important.
        """
        # tree-traversal over the DAG backwards to obtain all nodes in relevant family
        # initial queue -> nodes opposite of e0
        if self.parity == 1:    queue = [self.x]
        else:                   queue = list(self.e1) # e1 is a tuple
        searched = set(queue)
        # loop through queue nodes once
        for u in queue:
            for v in self.edge_DAG['parents'][u]:
                if v not in searched:
                    searched.add(v)
                    queue.append(v)
        return {self.node_labels[u] for u in searched}
    
    def edges(self):
        """Union of edges belonging to the cycle family"""
        # tree-traversal over the DAG backwards to obtain all edges in family
        # edge indexes are sorted to make edges unique as tuples
        e_sort = lambda pair: (min(pair),max(pair))
        if self.parity == 1:
            queue = [self.x]
            edge_set = set([e_sort(self.e0)])
        else:
            queue = list(self.e1) # deque from tuple
            edge_set = set([e_sort(self.e0),e_sort(self.e1)])
        searched = set(queue)
        # loop while more nodes
        for u in queue:
            for v in self.edge_DAG['parents'][u]:
                edge_set.add(e_sort((u,v)))
                if v not in searched:
                    searched.add(v)
                    queue.append(v)
        return {(self.node_labels[u],self.node_labels[v]) for u,v in edge_set}

    # test cases (1) cube and (2) bracelet w/ 4 diamonds - try reversing and shifting lists too
    def contains(self,C,rep='nodes'):
        """Returns True if cycle C belongs to the cycle family."""
        if rep == 'nodes':
            # validate length
            L = self.length
            if len(C)!=L: return False
            # convert nodes to integer labels
            C = [self.labels2node[u] for u in C]
            u0,v0 = self.e0
            # locate u0 in C
            try:                idx = C.index(u0)
            except ValueError:  return False    # not found
            C = C[idx:]+C[:idx]             # move u0 to start of list
            # locate and move v0 to end of list
            if C[1]==v0:    C[1:] = C[1:][::-1] # reverse orientation
            elif C[-1]!=v0: return False        # not found in valid location
            # validate descriptors
            if self.parity == 0:
                if self.e1 != tuple(C[L//2-1 : L//2+1]):    return False
            elif self.x != C[L//2]:                         return False
            # validate paths
            for i in range((L-1)//2):
                # left path
                if C[i] not in self.edge_DAG['parents'][C[i+1]]:
                    return False
                # right path
                if C[-i-1] not in self.edge_DAG['parents'][C[-i-2]]:
                    return False
            # passed test
            return True
        else:
            raise NotImplementedError('Only nodes representation implemented for contains method.')

    def __contains__(self,item):
        # Wrapper so that the line "C in <Family>" uses the contains function.
        # Assumes cycle uses list of nodes representation.
        return self.contains(item)
    



#################################################
# 3. MCB Pair Intersections
#      Process cycle pairs to obtain intersection
#      paths and candidate cycles to merge them.
#################################################
# good test case
# C1=[0,1,2,3,4,5],C2=[0,1,6,7,8,5] -> starting point has to be moved to work here
def pair_intersect(C1,C2):
    """
    pair_intersect
    Finds the intersections of cycles C1 and C2 which are in list of node representation. All outputs use C1 for
    finding the length of separation between paths.
    Output:
     - paths = intersection paths as a list of lists of nodes with the orientation they appear in C1
    """
    # C1, C2 simple cycles as lists of nodes - format not validated
    k1,k2  = len(C1),len(C2)
    
    # shared nodes, check if there is any intersection
    # this runs very fast and filters most cycle pairs do not intersect - keep this first! (1s vs 10s)
    shared_nodes = set(C1).intersection(C2)
    if len(shared_nodes)==0: 
        return []            # empty intersection
    # Edges belonging to C1 and C2 (note edges are ordered, but that is unimportant)
    shared_edges = set((C1[i-1],C1[i]) for i in range(k1) if (C1[i-1] in shared_nodes) and (C1[i] in shared_nodes))
    if len(shared_edges) == k1 == k2: 
        return [C1.copy()]     # cycles are equal
    
    # degree of nodes with respect to intersection edges
    degrees = defaultdict(int) # 0 if unspecified
    for u,v in shared_edges:
        degrees[u]+=1
        degrees[v]+=1
    # loop through C1 to interpret as a list of paths
    C1_idx,u = 0,C1[0]
    # if in the middle of path (deg. 2) or end of path (deg. 1, following edge not present)
    # then move to start of path (path of length 0 is deg. 0)
    if degrees[u]==2 or (degrees[u]==1 and (C1[0],C1[1]) not in shared_edges):
        # backtrack until next 1
        for C1_idx in range(k1-1,0,-1):
            u = C1[C1_idx]
            if degrees[u]<2:
                break # found start of path
                
    # start constructing paths
    state = 0       # not in path yet
    paths,dists = [],[]
    for _ in range(k1):
        if state == 0: # path not started
            if degrees[u]>0: # start of path
                Path = [u]
                state = 1
            elif u in shared_nodes: # path with a single node, no edges
                paths.append([u]) 
                dists.append(1) # initialize distance as 1 and increment as needed
            else:
                if len(dists)>0:
                    dists[-1]+=1
        elif state == 1: # in a path already
            Path.append(u)
            if degrees[u] == 1: # end of path
                paths.append(Path)
                state = 0
                dists.append(1) # initialize distance as 1 and increment as needed
        C1_idx = (C1_idx+1) % k1
        u = C1[C1_idx]
        
    # sort paths so first and last paths are furthest apart
    dists[-1] = k1 - sum(len(P)-1 for P in paths) - sum(dists[:-1])
    idx = np.argmax(dists)
    paths = paths[idx+1:]+paths[:idx+1]
    return paths
 
# Test cases
# C1 = [0,1,2,3,4,5,6,7,8,9]; C2 = ['a',1,'c',3,4,'f','g','h']
# vary with C1,C2  C2,C1  C1[::-1],C2  C2,C1[::-1] etc.
# C1 = [0,1,2,3,4,5,6,7]; C2 = [0,'a','b',3,'c','d']
# C1,C2 -> random swap of 1,2
# C2,C1 -> pair of cycles
# C1 = [0,1,2,3,4,5]; C2 = [0,'a','b',3,'c','d']
# random pair of cycles
def merge_pair(C1,C2):
    """
    Modify the cycle C1 such that C1 and C2 intersect over a single path.
    Outputs:
     - flag:   (0) degenerate case (1) main case
     - Cycles: Cycles C1' to exchange for C1 in main case and cycles [C1',C1'']
               as candidates to exchange for C1 in degenerate case
    """
    # modifies C2 so that its intersection with C1 is a path
    k1,k2 = len(C1),len(C2)
    paths = pair_intersect(C1,C2)
    if len(paths) <= 1:
        # this should only be called to merge cycles that intersect over >1 paths
        print(C1,C2)
        return None # already a single path or no intersection paths
    else:
        # primary case
        i1,j1 = C1.index(paths[0][0]),C1.index(paths[-1][-1]) # start,end of intersection region
        # length of intersection area, including gaps:
        PLen = (j1-i1) % k1
        # get orientation of second cycle
        i2,j2 = C2.index(paths[0][0]),C2.index((paths[0]+paths[1])[1])
        if (j2-i2)%k2 < k2/2: orientation = 1  # forward index through C2
        else:                 orientation = -1 # reverse index through C2
        # special case a cycle has two paths half the length of the cycle
        if len(paths)==2 and len(paths[0])==len(paths[1])==1:
            if PLen==k2/2 and np.random.randint(2):
                # two valid orientations in C2, pick one at random
                orientation *= -1 
            # degenerate case for C1. Two candidate cycles for the exchange
            if PLen==k1/2:
                C1p,C1pp = C1.copy(),C1.copy() # C1' and C1''
                for t in range(1,PLen):
                    # one for each orientation in C1, return to function caller to determine which is valid
                    C1p[(i1+t)%k1] = C2[(i2+t*orientation)%k2]
                    C1pp[(i1-t)%k1] = C2[(i2+t*orientation)%k2]
                return 0,[C1p,C1pp]
        # main case, merge C1 over shared path
        C1p = C1.copy() # C1'
        for t in range(1,PLen): # skip endpoints by doing 1,PLen-1
            C1p[(i1+t)%k1] = C2[(i2+t*orientation)%k2]
        return 1, C1p
