{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e6a56c",
   "metadata": {},
   "source": [
    "# 06 Auto Selection\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/08_workflow_system/06_auto_selection.ipynb)\n",
    "\n",
    "Converted from 06_auto_selection.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\n",
    "Features demonstrated:\n",
    "- WorkflowSelector decision algorithm\n",
    "- auto_select_workflow() convenience function\n",
    "- DatasetSizeTier and MemoryTier classification\n",
    "- Memory detection with get_total_available_memory_gb()\n",
    "- Adaptive tolerance calculation\n",
    "\n",
    "Run this example:\n",
    "    python examples/scripts/08_workflow_system/06_auto_selection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffa588",
   "metadata": {
    "id": "colab-install"
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"âœ… NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38654154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nlsq.streaming.large_dataset import GPUMemoryEstimator, MemoryEstimator, get_memory_tier\n",
    "from nlsq.core.workflow import (\n",
    "    DatasetSizeTier,\n",
    "    MemoryTier,\n",
    "    OptimizationGoal,\n",
    "    WorkflowSelector,\n",
    "    WorkflowTier,\n",
    "    auto_select_workflow,\n",
    "    calculate_adaptive_tolerances,\n",
    ")\n",
    "\n",
    "FIG_DIR = Path.cwd() / \"figures\"  # Modified for notebook compatibility\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90002a1",
   "metadata": {},
   "outputs": [],
   "source": "def main():\n    print(\"=\" * 70)\n    print(\"Automatic Workflow Selection Deep Dive\")\n    print(\"=\" * 70)\n    print()\n\n    # Set random seed for reproducibility\n    np.random.seed(42)\n\n    # =========================================================================\n    # 1. Display the selection matrix\n    # =========================================================================\n    print(\"1. WorkflowSelector Decision Matrix\")\n    print(\"-\" * 80)\n    print()\n    print(\n        \"Dataset Size    | Low (<16GB) | Medium (16-64GB) | High (64-128GB) | Very High (>128GB)\"\n    )\n    print(\"-\" * 80)\n    print(\n        \"Small (<10K)    | standard    | standard         | standard        | standard+quality\"\n    )\n    print(\n        \"Medium (10K-1M) | chunked     | standard         | standard+ms     | standard+ms\"\n    )\n    print(\n        \"Large (1M-10M)  | streaming   | chunked          | chunked+ms      | chunked+ms\"\n    )\n    print(\n        \"Huge (10M-100M) | stream+ckpt | streaming        | chunked         | chunked+ms\"\n    )\n    print(\n        \"Massive (>100M) | stream+ckpt | streaming+ckpt   | streaming       | streaming+ms\"\n    )\n    print()\n    print(\"Legend:\")\n    print(\"  ms = multi-start enabled\")\n    print(\"  ckpt = checkpointing enabled\")\n    print(\"  streaming/stream+ckpt = 4-layer defense enabled (v0.3.6+)\")\n\n    # =========================================================================\n    # 2. Dataset Size Tiers\n    # =========================================================================\n    print()\n    print()\n    print(\"2. DatasetSizeTier Classification\")\n    print(\"-\" * 60)\n    print(f\"{'Tier':<12s} | {'Max Points':<15s} | {'Tolerance':<12s}\")\n    print(\"-\" * 60)\n\n    for tier in DatasetSizeTier:\n        max_pts = tier.max_points\n        tol = tier.tolerance\n\n        if max_pts == float(\"inf\"):\n            max_str = \"unlimited\"\n        elif max_pts >= 1_000_000:\n            max_str = f\"{max_pts / 1_000_000:.0f}M\"\n        elif max_pts >= 1_000:\n            max_str = f\"{max_pts / 1_000:.0f}K\"\n        else:\n            max_str = str(max_pts)\n\n        print(f\"{tier.name:<12s} | {max_str:<15s} | {tol:.0e}\")\n\n    # Demonstrate tier classification\n    test_sizes = [500, 5_000, 50_000, 500_000, 5_000_000, 50_000_000, 500_000_000]\n\n    print()\n    print(\"Automatic Size Classification:\")\n    for n_points in test_sizes:\n        tier = DatasetSizeTier.from_n_points(n_points)\n\n        if n_points >= 1_000_000:\n            size_str = f\"{n_points / 1_000_000:.0f}M\"\n        elif n_points >= 1_000:\n            size_str = f\"{n_points / 1_000:.0f}K\"\n        else:\n            size_str = str(n_points)\n\n        print(f\"  {size_str:>8s} points -> {tier.name}\")\n\n    # =========================================================================\n    # 3. Memory Tiers\n    # =========================================================================\n    print()\n    print()\n    print(\"3. MemoryTier Classification\")\n    print(\"-\" * 60)\n    print(f\"{'Tier':<12s} | {'Max Memory':<12s} | {'Description'}\")\n    print(\"-\" * 60)\n\n    for tier in MemoryTier:\n        max_mem = tier.max_memory_gb\n        desc = tier.description\n\n        if max_mem == float(\"inf\"):\n            max_str = \"unlimited\"\n        else:\n            max_str = f\"{max_mem:.0f} GB\"\n\n        print(f\"{tier.name:<12s} | {max_str:<12s} | {desc}\")\n\n    # Check current system memory\n    cpu_memory = MemoryEstimator.get_available_memory_gb()\n    total_memory = MemoryEstimator.get_total_available_memory_gb()\n    current_tier = get_memory_tier(total_memory)\n\n    print()\n    print(\"Current System Memory:\")\n    print(f\"  CPU available:   {cpu_memory:.1f} GB\")\n\n    gpu_estimator = GPUMemoryEstimator()\n    if gpu_estimator.has_gpu():\n        gpu_memory = gpu_estimator.get_available_gpu_memory_gb()\n        print(f\"  GPU available:   {gpu_memory:.1f} GB\")\n    else:\n        print(\"  GPU available:   N/A (CPU only)\")\n\n    print(f\"  Total available: {total_memory:.1f} GB\")\n    print(f\"  Memory tier:     {current_tier.name}\")\n\n    # =========================================================================\n    # 4. WorkflowSelector in Action\n    # =========================================================================\n    print()\n    print()\n    print(\"4. WorkflowSelector Decisions\")\n    print(\"-\" * 80)\n\n    selector = WorkflowSelector()\n    print(f\"Using system memory: {total_memory:.1f} GB\")\n    print()\n\n    test_scenarios = [\n        (1_000, 3, None),\n        (100_000, 5, None),\n        (1_000_000, 5, OptimizationGoal.FAST),\n        (1_000_000, 5, OptimizationGoal.QUALITY),\n        (10_000_000, 5, OptimizationGoal.ROBUST),\n        (100_000_000, 5, OptimizationGoal.MEMORY_EFFICIENT),\n    ]\n\n    print(f\"{'n_points':<12s} | {'n_params':<8s} | {'Goal':<18s} | {'Config Type'}\")\n    print(\"-\" * 80)\n\n    for n_points, n_params, goal in test_scenarios:\n        config = selector.select(n_points, n_params, goal)\n        config_type = type(config).__name__\n\n        if n_points >= 1_000_000:\n            size_str = f\"{n_points / 1_000_000:.0f}M\"\n        elif n_points >= 1_000:\n            size_str = f\"{n_points / 1_000:.0f}K\"\n        else:\n            size_str = str(n_points)\n\n        goal_str = goal.name if goal else \"None (ROBUST)\"\n\n        print(f\"{size_str:<12s} | {n_params:<8d} | {goal_str:<18s} | {config_type}\")\n\n    # =========================================================================\n    # 5. Selection with Different Memory Limits\n    # =========================================================================\n    print()\n    print()\n    print(\"5. Selection with Different Memory Limits\")\n    print(\"-\" * 70)\n\n    memory_limits = [8.0, 32.0, 96.0, 256.0]  # GB\n    n_points = 5_000_000  # 5M points\n    n_params = 5\n\n    for mem_limit in memory_limits:\n        selector_fixed = WorkflowSelector(memory_limit_gb=mem_limit)\n        config = selector_fixed.select(n_points, n_params)\n        config_type = type(config).__name__\n        mem_tier = MemoryTier.from_available_memory_gb(mem_limit)\n\n        print(f\"  Memory: {mem_limit:>6.0f} GB ({mem_tier.name:<10s}) -> {config_type}\")\n\n    # =========================================================================\n    # 6. auto_select_workflow() Convenience Function\n    # =========================================================================\n    print()\n    print()\n    print(\"6. auto_select_workflow() Examples\")\n    print(\"-\" * 50)\n\n    config1 = auto_select_workflow(n_points=5_000, n_params=5)\n    print(f\"  5K points: {type(config1).__name__}\")\n\n    config2 = auto_select_workflow(\n        n_points=5_000_000,\n        n_params=5,\n        goal=OptimizationGoal.QUALITY,\n    )\n    print(f\"  5M points + QUALITY: {type(config2).__name__}\")\n\n    config3 = auto_select_workflow(\n        n_points=5_000_000,\n        n_params=5,\n        memory_limit_gb=8.0,\n    )\n    print(f\"  5M points + 8GB limit: {type(config3).__name__}\")\n\n    # =========================================================================\n    # 7. Adaptive Tolerances\n    # =========================================================================\n    print()\n    print()\n    print(\"7. Adaptive Tolerance Calculation\")\n    print(\"-\" * 60)\n\n    test_configs = [\n        (1_000, None),\n        (1_000_000, None),\n        (1_000_000, OptimizationGoal.FAST),\n        (1_000_000, OptimizationGoal.QUALITY),\n        (100_000_000, OptimizationGoal.ROBUST),\n    ]\n\n    print(f\"{'n_points':<12s} | {'Goal':<12s} | {'gtol':<12s} | {'ftol':<12s}\")\n    print(\"-\" * 60)\n\n    for n_points, goal in test_configs:\n        tolerances = calculate_adaptive_tolerances(n_points, goal)\n\n        if n_points >= 1_000_000:\n            size_str = f\"{n_points / 1_000_000:.0f}M\"\n        else:\n            size_str = f\"{n_points / 1_000:.0f}K\"\n\n        goal_str = goal.name if goal else \"None\"\n\n        print(\n            f\"{size_str:<12s} | {goal_str:<12s} | {tolerances['gtol']:.0e} | {tolerances['ftol']:.0e}\"\n        )\n\n    # =========================================================================\n    # 8. Defense Layer Awareness (v0.3.6+)\n    # =========================================================================\n    print()\n    print()\n    print(\"8. Defense Layer Awareness (v0.3.6+)\")\n    print(\"-\" * 70)\n    print()\n    print(\"When WorkflowSelector chooses STREAMING or STREAMING_CHECKPOINT tiers,\")\n    print(\"the returned HybridStreamingConfig automatically includes 4-layer defense.\")\n    print()\n    print(\"Defense presets for streaming workflows:\")\n    print(\"  defense_strict()     - Warm-start refinement (previous fit as p0)\")\n    print(\"  defense_relaxed()    - Exploration (rough initial guesses)\")\n    print(\"  scientific_default() - Production scientific computing (default)\")\n    print(\"  defense_disabled()   - Pre-0.3.6 behavior (no protection)\")\n    print()\n    print(\"The 4-layer defense strategy protects against Adam warmup divergence:\")\n    print(\"  Layer 1: Warm Start Detection - Skip warmup if near optimal\")\n    print(\"  Layer 2: Adaptive Learning Rate - Scale LR based on fit quality\")\n    print(\"  Layer 3: Cost-Increase Guard - Abort if loss increases > 5%\")\n    print(\"  Layer 4: Step Clipping - Limit parameter update magnitude\")\n    print()\n    print(\"To use defense presets with auto_select_workflow:\")\n    print()\n    print(\"  from nlsq import HybridStreamingConfig\")\n    print()\n    print(\"  # Get the auto-selected config\")\n    print(\"  config = auto_select_workflow(n_points=50_000_000, n_params=5)\")\n    print()\n    print(\"  # If it's a streaming config, apply defense preset\")\n    print(\"  if isinstance(config, HybridStreamingConfig):\")\n    print(\"      # For warm-start refinement scenarios\")\n    print(\"      config = HybridStreamingConfig.defense_strict()\")\n\n    # =========================================================================\n    # 9. Visualization\n    # =========================================================================\n    print()\n    print()\n    print(\"9. Saving selection algorithm visualization...\")\n\n    fig, ax = plt.subplots(figsize=(16, 12))\n    ax.set_xlim(0, 16)\n    ax.set_ylim(0, 14)\n    ax.axis(\"off\")\n\n    # Title\n    ax.text(\n        8,\n        13.5,\n        \"WorkflowSelector Decision Algorithm\",\n        ha=\"center\",\n        fontsize=18,\n        fontweight=\"bold\",\n    )\n\n    # Step 1: Input\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 11.5),\n            3,\n            1.2,\n            fill=True,\n            facecolor=\"lightblue\",\n            edgecolor=\"black\",\n            linewidth=2,\n        )\n    )\n    ax.text(2, 12.1, \"INPUT\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n    ax.text(2, 11.7, \"n_points, n_params, goal\", ha=\"center\", va=\"center\", fontsize=9)\n\n    # Arrow down\n    ax.annotate(\n        \"\",\n        xy=(2, 10.3),\n        xytext=(2, 11.5),\n        arrowprops={\"arrowstyle\": \"->\", \"color\": \"black\", \"lw\": 2},\n    )\n\n    # Step 2: Get Memory\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 9.3), 3, 1, fill=True, facecolor=\"lightyellow\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        2,\n        9.8,\n        \"1. Get Memory\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=10,\n        fontweight=\"bold\",\n    )\n    ax.text(2, 9.5, \"get_available_memory_gb()\", ha=\"center\", va=\"center\", fontsize=8)\n\n    # Arrow down\n    ax.annotate(\n        \"\",\n        xy=(2, 8.1),\n        xytext=(2, 9.3),\n        arrowprops={\"arrowstyle\": \"->\", \"color\": \"black\", \"lw\": 2},\n    )\n\n    # Step 3: Classify Memory\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 7.1), 3, 1, fill=True, facecolor=\"lightyellow\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        2,\n        7.6,\n        \"2. Classify Memory\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=10,\n        fontweight=\"bold\",\n    )\n    ax.text(2, 7.3, \"MemoryTier.from_...()\", ha=\"center\", va=\"center\", fontsize=8)\n\n    # Arrow down\n    ax.annotate(\n        \"\",\n        xy=(2, 5.9),\n        xytext=(2, 7.1),\n        arrowprops={\"arrowstyle\": \"->\", \"color\": \"black\", \"lw\": 2},\n    )\n\n    # Step 4: Classify Dataset\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 4.9), 3, 1, fill=True, facecolor=\"lightyellow\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        2,\n        5.4,\n        \"3. Classify Dataset\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=10,\n        fontweight=\"bold\",\n    )\n    ax.text(2, 5.1, \"DatasetSizeTier.from_...()\", ha=\"center\", va=\"center\", fontsize=8)\n\n    # Arrow down\n    ax.annotate(\n        \"\",\n        xy=(2, 3.7),\n        xytext=(2, 4.9),\n        arrowprops={\"arrowstyle\": \"->\", \"color\": \"black\", \"lw\": 2},\n    )\n\n    # Step 5: Apply Matrix\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 2.7),\n            3,\n            1,\n            fill=True,\n            facecolor=\"lightgreen\",\n            edgecolor=\"black\",\n            linewidth=2,\n        )\n    )\n    ax.text(\n        2,\n        3.2,\n        \"4. Decision Matrix\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=10,\n        fontweight=\"bold\",\n    )\n    ax.text(2, 2.9, \"tier, multistart, ckpt\", ha=\"center\", va=\"center\", fontsize=8)\n\n    # Arrow down\n    ax.annotate(\n        \"\",\n        xy=(2, 1.5),\n        xytext=(2, 2.7),\n        arrowprops={\"arrowstyle\": \"->\", \"color\": \"black\", \"lw\": 2},\n    )\n\n    # Step 6: Output\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 0.5),\n            3,\n            1,\n            fill=True,\n            facecolor=\"lightsalmon\",\n            edgecolor=\"black\",\n            linewidth=2,\n        )\n    )\n    ax.text(2, 1, \"OUTPUT\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n    ax.text(2, 0.7, \"ConfigType\", ha=\"center\", va=\"center\", fontsize=9)\n\n    # Memory Tier Reference (right side)\n    ax.add_patch(\n        plt.Rectangle((5, 8), 4.5, 4.5, fill=True, facecolor=\"white\", edgecolor=\"black\")\n    )\n    ax.text(7.25, 12, \"MemoryTier\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n    ax.text(5.2, 11.3, \"LOW: < 16 GB\", fontsize=9)\n    ax.text(5.2, 10.6, \"MEDIUM: 16-64 GB\", fontsize=9)\n    ax.text(5.2, 9.9, \"HIGH: 64-128 GB\", fontsize=9)\n    ax.text(5.2, 9.2, \"VERY_HIGH: > 128 GB\", fontsize=9)\n    ax.text(\n        5.2,\n        8.4,\n        f\"Current: {current_tier.name}\",\n        fontsize=9,\n        fontweight=\"bold\",\n        color=\"blue\",\n    )\n\n    # Dataset Size Reference\n    ax.add_patch(\n        plt.Rectangle(\n            (10, 8), 5.5, 4.5, fill=True, facecolor=\"white\", edgecolor=\"black\"\n        )\n    )\n    ax.text(12.75, 12, \"DatasetSizeTier\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n    ax.text(10.2, 11.3, \"TINY: < 1K (tol=1e-12)\", fontsize=9)\n    ax.text(10.2, 10.7, \"SMALL: 1K-10K (tol=1e-10)\", fontsize=9)\n    ax.text(10.2, 10.1, \"MEDIUM: 10K-100K (tol=1e-9)\", fontsize=9)\n    ax.text(10.2, 9.5, \"LARGE: 100K-1M (tol=1e-8)\", fontsize=9)\n    ax.text(10.2, 8.9, \"VERY_LARGE: 1M-10M (tol=1e-7)\", fontsize=9)\n    ax.text(10.2, 8.3, \"HUGE/MASSIVE: >10M\", fontsize=9)\n\n    # Config Types Reference\n    ax.add_patch(\n        plt.Rectangle(\n            (5, 0.5), 10.5, 3, fill=True, facecolor=\"white\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        10.25, 3, \"Output Config Types\", ha=\"center\", fontsize=11, fontweight=\"bold\"\n    )\n    ax.text(\n        5.2,\n        2.3,\n        \"GlobalOptimizationConfig: STANDARD + multistart\",\n        fontsize=9,\n        color=\"green\",\n    )\n    ax.text(5.2, 1.7, \"LDMemoryConfig: STANDARD or CHUNKED\", fontsize=9, color=\"orange\")\n    ax.text(\n        5.2,\n        1.1,\n        \"HybridStreamingConfig: STREAMING or STREAMING_CHECKPOINT\",\n        fontsize=9,\n        color=\"red\",\n    )\n\n    # Goal modifiers\n    ax.add_patch(\n        plt.Rectangle(\n            (5, 4.5), 10.5, 3, fill=True, facecolor=\"lavender\", edgecolor=\"black\"\n        )\n    )\n    ax.text(10.25, 7, \"Goal Modifiers\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n    ax.text(5.2, 6.3, \"FAST: Disable multistart, looser tolerances\", fontsize=9)\n    ax.text(5.2, 5.7, \"ROBUST/GLOBAL: Enable multistart (if memory allows)\", fontsize=9)\n    ax.text(5.2, 5.1, \"QUALITY: Enable multistart + tighter tolerances\", fontsize=9)\n    ax.text(5.2, 4.7, \"MEMORY_EFFICIENT: Force streaming/chunking\", fontsize=9)\n\n    plt.tight_layout()\n    plt.savefig(FIG_DIR / \"06_selection_algorithm.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  Saved: {FIG_DIR / '06_selection_algorithm.png'}\")\n\n    # =========================================================================\n    # Summary\n    # =========================================================================\n    print()\n    print()\n    print(\"=\" * 70)\n    print(\"Summary\")\n    print(\"=\" * 70)\n    print()\n    print(\"Key Functions:\")\n    print(\"  auto_select_workflow(n_points, n_params, goal)\")\n    print(\"  WorkflowSelector().select(n_points, n_params, goal)\")\n    print(\"  DatasetSizeTier.from_n_points(n_points)\")\n    print(\"  MemoryTier.from_available_memory_gb(memory_gb)\")\n    print(\"  MemoryEstimator.get_total_available_memory_gb()\")\n    print(\"  get_memory_tier(memory_gb)\")\n    print()\n    print(\"Current System:\")\n    print(f\"  Total memory: {total_memory:.1f} GB\")\n    print(f\"  Memory tier: {current_tier.name}\")\n    print()\n    print(\"Key Takeaways:\")\n    print(\"  - WorkflowSelector uses a decision matrix based on size + memory\")\n    print(\"  - Memory is re-evaluated on each call (no caching)\")\n    print(\"  - auto_select_workflow() provides a simple convenience API\")\n    print(\"  - Override with memory_limit_gb for reproducible behavior\")\n    print(\"  - Streaming configs include 4-layer defense by default (v0.3.6+)\")\n    print()\n    print(\"Defense presets for streaming (v0.3.6+):\")\n    print(\"  HybridStreamingConfig.defense_strict()     # Warm-start refinement\")\n    print(\"  HybridStreamingConfig.defense_relaxed()    # Exploration\")\n    print(\"  HybridStreamingConfig.scientific_default() # Production scientific\")\n    print(\"  HybridStreamingConfig.defense_disabled()   # Pre-0.3.6 behavior\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76141d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
