{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566d391e",
   "metadata": {},
   "source": [
    "# 07 Hpc And Checkpointing\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/08_workflow_system/07_hpc_and_checkpointing.ipynb)\n",
    "\n",
    "Converted from 07_hpc_and_checkpointing.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "\n",
    "Features demonstrated:\n",
    "- ClusterDetector and ClusterInfo for PBS Pro detection\n",
    "- WorkflowTier.STREAMING_CHECKPOINT for fault tolerance\n",
    "- Checkpointing with enable_checkpoints=True and checkpoint_dir\n",
    "- create_checkpoint_directory() for timestamp-based directories\n",
    "- Checkpoint resume workflow\n",
    "- PBS Pro job script generation\n",
    "\n",
    "Run this example:\n",
    "    python examples/scripts/08_workflow_system/07_hpc_and_checkpointing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fd545",
   "metadata": {
    "id": "colab-install"
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"âœ… NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nlsq.core.workflow import (\n",
    "    WORKFLOW_PRESETS,\n",
    "    ClusterDetector,\n",
    "    ClusterInfo,\n",
    "    OptimizationGoal,\n",
    "    WorkflowConfig,\n",
    "    WorkflowTier,\n",
    "    create_checkpoint_directory,\n",
    "    create_distributed_config,\n",
    "    get_multi_gpu_config,\n",
    ")\n",
    "\n",
    "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\n",
    "if QUICK:\n",
    "    print(\"Quick mode: reduced iterations for HPC and checkpointing demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_dir, iteration, params, loss, metadata=None):\n",
    "    \"\"\"Save optimization checkpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_dir : str\n",
    "        Directory to save checkpoint\n",
    "    iteration : int\n",
    "        Current iteration number\n",
    "    params : np.ndarray\n",
    "        Current parameter values\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    metadata : dict, optional\n",
    "        Additional metadata to save\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_dir) / f\"checkpoint_{iteration:06d}.pkl\"\n",
    "\n",
    "    checkpoint_data = {\n",
    "        \"iteration\": iteration,\n",
    "        \"params\": np.array(params),\n",
    "        \"loss\": float(loss),\n",
    "        \"metadata\": metadata or {},\n",
    "    }\n",
    "\n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "\n",
    "    print(f\"  Saved checkpoint: {checkpoint_path.name}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Load the most recent checkpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_dir : str\n",
    "        Directory containing checkpoints\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        Checkpoint data if found, None otherwise\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    if not checkpoint_dir.exists():\n",
    "        return None\n",
    "\n",
    "    # Find all checkpoint files\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    # Sort by name (which includes iteration number)\n",
    "    latest = sorted(checkpoints)[-1]\n",
    "\n",
    "    with open(latest, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "\n",
    "    print(f\"  Loaded checkpoint: {latest.name}\")\n",
    "    return checkpoint_data\n",
    "\n",
    "\n",
    "def optimization_with_checkpoints(\n",
    "    checkpoint_dir, max_iterations=100, checkpoint_interval=10\n",
    "):\n",
    "    \"\"\"Example optimization loop with checkpoint support.\"\"\"\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    checkpoint = load_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "    if checkpoint:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        params = checkpoint[\"params\"]\n",
    "        print(f\"  Resuming from iteration {start_iteration}\")\n",
    "    else:\n",
    "        start_iteration = 0\n",
    "        params = np.array([1.0, 1.0, 0.0])  # Initial guess\n",
    "        print(\"  Starting fresh optimization\")\n",
    "\n",
    "    # Optimization loop\n",
    "    for iteration in range(start_iteration, max_iterations):\n",
    "        # Simulate optimization step\n",
    "        params = params + 0.001 * np.random.randn(3)\n",
    "        loss = np.sum(params**2)  # Dummy loss\n",
    "\n",
    "        # Checkpoint at intervals\n",
    "        if iteration > 0 and iteration % checkpoint_interval == 0:\n",
    "            save_checkpoint(checkpoint_dir, iteration, params, loss)\n",
    "\n",
    "    # Final checkpoint\n",
    "    save_checkpoint(checkpoint_dir, max_iterations - 1, params, loss)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf549085",
   "metadata": {},
   "outputs": [],
   "source": "def main():\n    print(\"=\" * 70)\n    print(\"HPC Integration and Checkpointing\")\n    print(\"=\" * 70)\n    print()\n\n    np.random.seed(42)\n\n    # =========================================================================\n    # 1. ClusterDetector and ClusterInfo\n    # =========================================================================\n    print(\"1. ClusterDetector and ClusterInfo:\")\n    print(\"-\" * 50)\n\n    detector = ClusterDetector()\n\n    print(f\"  PBS environment detected: {detector.is_pbs_environment()}\")\n\n    cluster_info = detector.detect()\n\n    if cluster_info:\n        print(\"\\n  Cluster detected:\")\n        print(f\"    Scheduler: {cluster_info.scheduler}\")\n        print(f\"    Node count: {cluster_info.node_count}\")\n        print(f\"    GPUs per node: {cluster_info.gpus_per_node}\")\n        print(f\"    Total GPUs: {cluster_info.total_gpus}\")\n    else:\n        print(\"\\n  No cluster environment detected (running locally)\")\n\n    # Simulated PBS cluster for demonstration\n    simulated_cluster = ClusterInfo(\n        node_count=4,\n        gpus_per_node=8,\n        total_gpus=32,\n        node_list=[\"node01\", \"node02\", \"node03\", \"node04\"],\n        scheduler=\"pbs\",\n        job_id=\"12345.pbs_server\",\n        interconnect=\"infiniband\",\n    )\n\n    print(\"\\n  Simulated PBS cluster:\")\n    print(f\"    Nodes: {simulated_cluster.node_count}\")\n    print(f\"    GPUs: {simulated_cluster.total_gpus}\")\n    print(f\"    Job ID: {simulated_cluster.job_id}\")\n    print(f\"    Interconnect: {simulated_cluster.interconnect}\")\n\n    # =========================================================================\n    # 2. WorkflowTier.STREAMING_CHECKPOINT\n    # =========================================================================\n    print()\n    print(\"2. WorkflowTier.STREAMING_CHECKPOINT:\")\n    print(\"-\" * 50)\n\n    print(\"  Available WorkflowTiers:\")\n    for tier in WorkflowTier:\n        print(f\"    - {tier.name}\")\n\n    hpc_config = WorkflowConfig(\n        tier=WorkflowTier.STREAMING_CHECKPOINT,\n        goal=OptimizationGoal.ROBUST,\n        gtol=1e-7,\n        ftol=1e-7,\n        xtol=1e-7,\n        enable_checkpoints=True,\n        checkpoint_dir=\"./nlsq_checkpoints\",\n        enable_multistart=True,\n        n_starts=10,\n    )\n\n    print(\"\\n  HPC Configuration:\")\n    print(f\"    tier: {hpc_config.tier}\")\n    print(f\"    goal: {hpc_config.goal}\")\n    print(f\"    enable_checkpoints: {hpc_config.enable_checkpoints}\")\n    print(f\"    checkpoint_dir: {hpc_config.checkpoint_dir}\")\n    print(f\"    enable_multistart: {hpc_config.enable_multistart}\")\n\n    if QUICK:\n        print()\n        print(\"=\" * 70)\n        print(\"Summary (Quick Mode)\")\n        print(\"=\" * 70)\n        print()\n        print(\"HPC Integration:\")\n        print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n        print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n        print()\n        print(\"Checkpointing:\")\n        print(\"  - WorkflowTier.STREAMING_CHECKPOINT for fault tolerance\")\n        print(\"  - enable_checkpoints=True, checkpoint_dir='./checkpoints'\")\n        print()\n        print(\"Defense Layers for Checkpoint Resume (v0.3.6+):\")\n        print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n        print(\"  - 4-layer defense prevents Adam warmup from diverging\")\n        return\n\n    # =========================================================================\n    # 3. Checkpointing Configuration\n    # =========================================================================\n    print()\n    print(\"3. Checkpointing Configuration:\")\n    print(\"-\" * 50)\n\n    checkpoint_dir = create_checkpoint_directory()\n    print(f\"  Created checkpoint directory: {checkpoint_dir}\")\n\n    custom_checkpoint_dir = create_checkpoint_directory(\n        base_dir=\"./my_project_checkpoints\"\n    )\n    print(f\"  Custom checkpoint directory: {custom_checkpoint_dir}\")\n\n    # =========================================================================\n    # 4. Checkpoint Resume Workflow\n    # =========================================================================\n    print()\n    print(\"4. Checkpoint Resume Workflow:\")\n    print(\"-\" * 50)\n\n    demo_dir = create_checkpoint_directory(base_dir=\"./demo_checkpoints\")\n\n    # Simulate saving checkpoints\n    print(\"\\n  Simulating optimization with checkpoints...\")\n    for i in range(0, 30, 10):\n        params = np.array([2.0 + 0.01 * i, 1.0 - 0.005 * i, 0.5])\n        loss = 0.1 / (1 + i * 0.1)\n        save_checkpoint(demo_dir, i, params, loss, metadata={\"epoch\": i // 10})\n\n    # Load latest checkpoint\n    print(\"\\n  Loading latest checkpoint for resume...\")\n    latest = load_latest_checkpoint(demo_dir)\n\n    if latest:\n        print(f\"    Iteration: {latest['iteration']}\")\n        print(f\"    Parameters: {latest['params']}\")\n        print(f\"    Loss: {latest['loss']:.6f}\")\n\n    # Run optimization with checkpoints\n    print(\"\\n  Running optimization loop with checkpoints...\")\n    final_params = optimization_with_checkpoints(demo_dir, max_iterations=50)\n    print(f\"  Final parameters: {final_params}\")\n\n    # =========================================================================\n    # 5. HPC Distributed Configuration\n    # =========================================================================\n    print()\n    print(\"5. HPC Distributed Configuration:\")\n    print(\"-\" * 50)\n\n    dist_config = create_distributed_config(simulated_cluster)\n\n    print(\"  Distributed config from cluster:\")\n    for key, value in list(dist_config.items())[:8]:\n        print(f\"    {key}: {value}\")\n\n    gpu_config = get_multi_gpu_config(simulated_cluster)\n\n    if gpu_config:\n        print(\"\\n  Multi-GPU configuration:\")\n        print(f\"    n_devices: {gpu_config.n_devices}\")\n        print(f\"    per_device_batch_size: {gpu_config.per_device_batch_size}\")\n        print(f\"    total_batch_size: {gpu_config.total_batch_size}\")\n\n    # =========================================================================\n    # 6. PBS Pro Job Script\n    # =========================================================================\n    print()\n    print(\"6. PBS Pro Job Script:\")\n    print(\"-\" * 50)\n\n    pbs_script = \"\"\"#!/bin/bash\n#PBS -N nlsq_fit\n#PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\n#PBS -l walltime=24:00:00\n#PBS -q gpu\n#PBS -j oe\n#PBS -o nlsq_fit.log\n\n# NLSQ Curve Fitting Job Script for PBS Pro\n# ===========================================\n\ncd $PBS_O_WORKDIR\n\n# Load required modules\nmodule load python/3.12\nmodule load cuda/12.0\nmodule load cudnn/8.9\n\n# Activate virtual environment\nsource ./venv/bin/activate\n\n# Set NLSQ environment variables\nexport NLSQ_WORKFLOW_GOAL=robust\nexport NLSQ_MEMORY_LIMIT_GB=200\nexport NLSQ_CHECKPOINT_DIR=$PBS_O_WORKDIR/checkpoints\n\n# Create checkpoint directory\nmkdir -p $NLSQ_CHECKPOINT_DIR\n\n# Display job information\necho \"========================================\"\necho \"NLSQ Fitting Job Started\"\necho \"========================================\"\necho \"Job ID: $PBS_JOBID\"\necho \"Node list:\"\ncat $PBS_NODEFILE\necho \"========================================\"\n\n# Run NLSQ fitting script\npython fit_large_dataset.py \\\\\n    --data-file ./data/large_dataset.h5 \\\\\n    --output-dir ./results \\\\\n    --checkpoint-dir $NLSQ_CHECKPOINT_DIR \\\\\n    --enable-checkpoints \\\\\n    --checkpoint-interval 50\n\necho \"========================================\"\necho \"Job Completed: $(date)\"\necho \"========================================\"\n\"\"\"\n\n    pbs_script_path = Path(\"nlsq_fit.pbs\")\n    pbs_script_path.write_text(pbs_script)\n\n    print(\"  Created PBS job script: nlsq_fit.pbs\")\n    print(\"  Key directives:\")\n    print(\"    #PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\")\n    print(\"    #PBS -l walltime=24:00:00\")\n    print(\"    #PBS -q gpu\")\n\n    # =========================================================================\n    # 7. HPC Distributed Preset\n    # =========================================================================\n    print()\n    print(\"7. HPC Distributed Preset:\")\n    print(\"-\" * 50)\n\n    hpc_preset = WORKFLOW_PRESETS[\"hpc_distributed\"]\n\n    for key, value in list(hpc_preset.items())[:8]:\n        print(f\"  {key}: {value}\")\n\n    # =========================================================================\n    # 8. Defense Layers for Checkpoint Resume (v0.3.6+)\n    # =========================================================================\n    print()\n    print(\"8. Defense Layers for Checkpoint Resume (v0.3.6+):\")\n    print(\"-\" * 70)\n    print()\n    print(\"When resuming from checkpoints, your initial parameters are near-optimal.\")\n    print(\"This is a classic warm-start scenario where defense layers are critical.\")\n    print()\n    print(\"Without defense layers, Adam warmup can DIVERGE from your checkpoint:\")\n    print(\"  - Momentum builds up from large initial gradients\")\n    print(\"  - Parameters overshoot and loss increases\")\n    print(\"  - All progress from previous run is lost\")\n    print()\n    print(\"With 4-layer defense, checkpoint resume is protected:\")\n    print(\"  Layer 1: Detects you're starting near-optimal -> may skip warmup\")\n    print(\"  Layer 2: Scales learning rate based on initial fit quality\")\n    print(\"  Layer 3: Aborts warmup if loss increases > 5%\")\n    print(\"  Layer 4: Clips step magnitudes to prevent overshooting\")\n    print()\n    print(\"Recommended configuration for checkpoint resume:\")\n    print()\n    print(\"  from nlsq import HybridStreamingConfig\")\n    print()\n    print(\"  # Use defense_strict for checkpoint resume scenarios\")\n    print(\"  config = HybridStreamingConfig.defense_strict()\")\n    print(\"  config = config.with_overrides(\")\n    print(\"      enable_checkpoints=True,\")\n    print(\"      checkpoint_dir='./checkpoints',\")\n    print(\"  )\")\n    print()\n    print(\"Defense presets comparison:\")\n    print(\"  defense_strict()     - Best for checkpoint resume (LR: 1e-6 to 1e-4)\")\n    print(\"  defense_relaxed()    - For fresh starts (LR: 1e-4 to 0.01)\")\n    print(\"  scientific_default() - Balanced for production\")\n    print(\"  defense_disabled()   - Pre-0.3.6 behavior (no protection)\")\n\n    # =========================================================================\n    # Cleanup\n    # =========================================================================\n    print()\n    print(\"Cleaning up...\")\n\n    for path_str in [\n        \"nlsq_checkpoints\",\n        \"demo_checkpoints\",\n        \"my_project_checkpoints\",\n    ]:\n        path = Path(path_str)\n        if path.exists():\n            shutil.rmtree(path)\n            print(f\"  Removed: {path_str}\")\n\n    if pbs_script_path.exists():\n        pbs_script_path.unlink()\n        print(\"  Removed: nlsq_fit.pbs\")\n\n    # =========================================================================\n    # Summary\n    # =========================================================================\n    print()\n    print(\"=\" * 70)\n    print(\"Summary\")\n    print(\"=\" * 70)\n    print()\n    print(\"HPC Integration:\")\n    print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n    print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n    print(\"  - create_distributed_config() for HPC-optimized settings\")\n    print()\n    print(\"Checkpointing:\")\n    print(\"  - WorkflowTier.STREAMING_CHECKPOINT for fault tolerance\")\n    print(\"  - enable_checkpoints=True, checkpoint_dir='./checkpoints'\")\n    print(\"  - create_checkpoint_directory() for timestamped directories\")\n    print()\n    print(\"PBS Pro Job Scripts:\")\n    print(\"  - #PBS -l select=N:ncpus=C:ngpus=G:mem=Mgb\")\n    print(\"  - Environment variables: NLSQ_WORKFLOW_GOAL, NLSQ_MEMORY_LIMIT_GB\")\n    print(\"  - Checkpoint directory: NLSQ_CHECKPOINT_DIR\")\n    print()\n    print(\"Defense Layers for Checkpoint Resume (v0.3.6+):\")\n    print(\"  - Checkpoint resume = warm-start scenario (parameters near optimal)\")\n    print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n    print(\"  - 4-layer defense prevents Adam warmup from diverging\")\n    print(\"  - See docs/guides/defense_layers.rst for full configuration\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
