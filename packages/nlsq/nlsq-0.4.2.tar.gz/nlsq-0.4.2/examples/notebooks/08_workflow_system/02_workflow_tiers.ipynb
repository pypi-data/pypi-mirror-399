{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b453493",
   "metadata": {},
   "source": [
    "# 02 Workflow Tiers\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/08_workflow_system/02_workflow_tiers.ipynb)\n",
    "\n",
    "Converted from 02_workflow_tiers.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\n",
    "Features demonstrated:\n",
    "- Understanding the four workflow tiers\n",
    "- Automatic tier selection based on dataset size and memory\n",
    "- Manual tier override with WorkflowConfig\n",
    "- Memory usage comparison across tiers\n",
    "\n",
    "Run this example:\n",
    "    python examples/scripts/08_workflow_system/02_workflow_tiers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fca05",
   "metadata": {
    "id": "colab-install"
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"âœ… NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ec09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nlsq import OptimizationGoal, WorkflowConfig, WorkflowTier, fit\n",
    "from nlsq.streaming.large_dataset import MemoryEstimator, get_memory_tier\n",
    "from nlsq.core.workflow import (\n",
    "    DatasetSizeTier,\n",
    "    MemoryTier,\n",
    "    auto_select_workflow,\n",
    ")\n",
    "\n",
    "FIG_DIR = Path.cwd() / \"figures\"  # Modified for notebook compatibility\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57431eda",
   "metadata": {},
   "outputs": [],
   "source": "def exponential_decay(x, a, b, c):\n    \"\"\"Exponential decay: y = a * exp(-b * x) + c\"\"\"\n    return a * jnp.exp(-b * x) + c\n\n\ndef estimate_memory_usage(n_points, n_params, tier):\n    \"\"\"Estimate memory usage in GB for a given tier.\"\"\"\n    bytes_per_point = 8 * (3 + n_params)  # x, y, residual + jacobian\n\n    if tier == WorkflowTier.STANDARD:\n        # All data in memory\n        return n_points * bytes_per_point / 1e9\n    elif tier == WorkflowTier.CHUNKED:\n        # Chunk size typically 100K-1M\n        chunk_size = min(1_000_000, n_points)\n        return chunk_size * bytes_per_point / 1e9\n    elif tier in (WorkflowTier.STREAMING, WorkflowTier.STREAMING_CHECKPOINT):\n        # Batch size typically 50K\n        batch_size = 50_000\n        return batch_size * bytes_per_point / 1e9\n    else:\n        return 0\n\n\ndef main():\n    print(\"=\" * 70)\n    print(\"Workflow Tiers: STANDARD, CHUNKED, STREAMING\")\n    print(\"=\" * 70)\n    print()\n\n    # Set random seed for reproducibility\n    np.random.seed(42)\n\n    # =========================================================================\n    # 1. Overview of Workflow Tiers\n    # =========================================================================\n    print(\"1. Workflow Tiers Overview\")\n    print(\"-\" * 50)\n\n    tier_info = {\n        WorkflowTier.STANDARD: {\n            \"description\": \"Standard curve_fit() for small datasets\",\n            \"dataset_size\": \"< 10K points\",\n            \"memory\": \"O(N) - loads all data into memory\",\n            \"defense_layers\": \"N/A\",\n        },\n        WorkflowTier.CHUNKED: {\n            \"description\": \"LargeDatasetFitter with automatic chunking\",\n            \"dataset_size\": \"10K - 10M points\",\n            \"memory\": \"O(chunk_size) - processes data in chunks\",\n            \"defense_layers\": \"N/A\",\n        },\n        WorkflowTier.STREAMING: {\n            \"description\": \"AdaptiveHybridStreamingOptimizer for huge datasets\",\n            \"dataset_size\": \"10M - 100M points\",\n            \"memory\": \"O(batch_size) - mini-batch gradient descent\",\n            \"defense_layers\": \"4-layer defense enabled (v0.3.6+)\",\n        },\n        WorkflowTier.STREAMING_CHECKPOINT: {\n            \"description\": \"Streaming with automatic checkpointing\",\n            \"dataset_size\": \"> 100M points\",\n            \"memory\": \"O(batch_size) + checkpoint storage\",\n            \"defense_layers\": \"4-layer defense enabled (v0.3.6+)\",\n        },\n    }\n\n    for tier, info in tier_info.items():\n        print(f\"\\n{tier.name}:\")\n        print(f\"  Description: {info['description']}\")\n        print(f\"  Dataset Size: {info['dataset_size']}\")\n        print(f\"  Memory: {info['memory']}\")\n        print(f\"  Defense Layers: {info['defense_layers']}\")\n\n    # =========================================================================\n    # 2. Dataset Size Tiers\n    # =========================================================================\n    print()\n    print()\n    print(\"2. Dataset Size Tiers and Thresholds\")\n    print(\"-\" * 50)\n\n    for size_tier in DatasetSizeTier:\n        max_pts = size_tier.max_points\n        tol = size_tier.tolerance\n        if max_pts == float(\"inf\"):\n            print(f\"  {size_tier.name:12s}: > 100M points, tolerance = {tol:.0e}\")\n        else:\n            print(\n                f\"  {size_tier.name:12s}: < {max_pts / 1e6:.0f}M points, tolerance = {tol:.0e}\"\n            )\n\n    # =========================================================================\n    # 3. Memory Tiers\n    # =========================================================================\n    print()\n    print()\n    print(\"3. Memory Tiers\")\n    print(\"-\" * 50)\n\n    for mem_tier in MemoryTier:\n        print(f\"  {mem_tier.name:10s}: {mem_tier.description}\")\n\n    # Check current system memory\n    available_memory = MemoryEstimator.get_available_memory_gb()\n    current_tier = get_memory_tier(available_memory)\n    print(\n        f\"\\n  Current system: {available_memory:.1f} GB available -> {current_tier.name}\"\n    )\n\n    # =========================================================================\n    # 4. Automatic Tier Selection\n    # =========================================================================\n    print()\n    print()\n    print(\"4. Automatic Tier Selection\")\n    print(\"-\" * 50)\n    print(f\"  Available memory: {available_memory:.1f} GB\")\n    print()\n\n    test_sizes = [1_000, 50_000, 500_000, 5_000_000, 50_000_000, 500_000_000]\n    n_params = 5\n\n    for n_points in test_sizes:\n        config = auto_select_workflow(n_points, n_params)\n        config_type = type(config).__name__\n\n        # Determine tier from config type\n        if \"GlobalOptimization\" in config_type:\n            tier = \"STANDARD (with multi-start)\"\n        elif \"LDMemory\" in config_type:\n            tier = \"STANDARD or CHUNKED\"\n        elif \"HybridStreaming\" in config_type:\n            tier = \"STREAMING or STREAMING_CHECKPOINT\"\n        else:\n            tier = config_type\n\n        if n_points >= 1_000_000:\n            size_str = f\"{n_points / 1_000_000:.0f}M\"\n        elif n_points >= 1_000:\n            size_str = f\"{n_points / 1_000:.0f}K\"\n        else:\n            size_str = str(n_points)\n\n        print(f\"  {size_str:>8s} points -> {tier}\")\n\n    # =========================================================================\n    # 5. Tier Selection Decision Tree Visualization\n    # =========================================================================\n    print()\n    print()\n    print(\"5. Saving tier selection decision tree...\")\n\n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.axis(\"off\")\n\n    # Title\n    ax.text(\n        5,\n        9.5,\n        \"Workflow Tier Selection Decision Tree\",\n        ha=\"center\",\n        fontsize=16,\n        fontweight=\"bold\",\n    )\n\n    # Root node\n    ax.add_patch(\n        plt.Rectangle(\n            (3.5, 8.2), 3, 0.8, fill=True, facecolor=\"lightblue\", edgecolor=\"black\"\n        )\n    )\n    ax.text(5, 8.6, \"Dataset Size?\", ha=\"center\", va=\"center\", fontsize=11)\n\n    # Level 1 branches\n    # Small\n    ax.plot([4.2, 2, 2], [8.2, 7.5, 7.0], \"k-\", linewidth=1)\n    ax.text(2.5, 7.7, \"< 10K\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 6.2), 3, 0.8, fill=True, facecolor=\"lightgreen\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        2, 6.6, \"STANDARD\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\"\n    )\n\n    # Medium\n    ax.plot([5, 5], [8.2, 7.0], \"k-\", linewidth=1)\n    ax.text(5.3, 7.5, \"10K - 10M\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (3.5, 6.2), 3, 0.8, fill=True, facecolor=\"lightyellow\", edgecolor=\"black\"\n        )\n    )\n    ax.text(5, 6.6, \"Memory Check\", ha=\"center\", va=\"center\", fontsize=10)\n\n    # Large\n    ax.plot([5.8, 8, 8], [8.2, 7.5, 7.0], \"k-\", linewidth=1)\n    ax.text(7.2, 7.7, \"> 10M\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (6.5, 6.2), 3, 0.8, fill=True, facecolor=\"lightyellow\", edgecolor=\"black\"\n        )\n    )\n    ax.text(8, 6.6, \"Memory Check\", ha=\"center\", va=\"center\", fontsize=10)\n\n    # Level 2 - Medium dataset branches\n    ax.plot([4.2, 3, 3], [6.2, 5.5, 5.0], \"k-\", linewidth=1)\n    ax.text(3.3, 5.6, \"> 16GB\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (1.5, 4.2), 3, 0.8, fill=True, facecolor=\"lightgreen\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        3, 4.6, \"STANDARD\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\"\n    )\n\n    ax.plot([5.8, 7, 7], [6.2, 5.5, 5.0], \"k-\", linewidth=1)\n    ax.text(6.5, 5.6, \"< 16GB\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (5.5, 4.2), 3, 0.8, fill=True, facecolor=\"orange\", edgecolor=\"black\"\n        )\n    )\n    ax.text(7, 4.6, \"CHUNKED\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n\n    # Level 2 - Large dataset branches\n    ax.plot([7.2, 6, 6], [6.2, 5.5, 3.0], \"k-\", linewidth=1)\n    ax.text(6.3, 5.6, \"> 64GB\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (4.5, 2.2), 3, 0.8, fill=True, facecolor=\"orange\", edgecolor=\"black\"\n        )\n    )\n    ax.text(6, 2.6, \"CHUNKED\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n\n    ax.plot([8.8, 9.5, 9.5], [6.2, 5.5, 3.0], \"k-\", linewidth=1)\n    ax.text(9.2, 5.6, \"< 64GB\", fontsize=9)\n    ax.add_patch(\n        plt.Rectangle(\n            (8, 2.2), 1.8, 0.8, fill=True, facecolor=\"salmon\", edgecolor=\"black\"\n        )\n    )\n    ax.text(\n        8.9, 2.6, \"STREAMING\", ha=\"center\", va=\"center\", fontsize=9, fontweight=\"bold\"\n    )\n\n    # Additional note for massive datasets\n    ax.add_patch(\n        plt.Rectangle(\n            (0.5, 0.5),\n            9,\n            1.2,\n            fill=True,\n            facecolor=\"lightgray\",\n            edgecolor=\"black\",\n            alpha=0.3,\n        )\n    )\n    ax.text(\n        5,\n        1.1,\n        \"For > 100M points: STREAMING_CHECKPOINT (adds fault tolerance)\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=10,\n        style=\"italic\",\n    )\n\n    plt.tight_layout()\n    plt.savefig(FIG_DIR / \"02_tier_decision_tree.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  Saved: {FIG_DIR / '02_tier_decision_tree.png'}\")\n\n    # =========================================================================\n    # 6. Manual Tier Override\n    # =========================================================================\n    print()\n    print()\n    print(\"6. Manual Tier Override\")\n    print(\"-\" * 50)\n\n    config_standard = WorkflowConfig(tier=WorkflowTier.STANDARD)\n    config_chunked = WorkflowConfig(tier=WorkflowTier.CHUNKED)\n    config_streaming = WorkflowConfig(tier=WorkflowTier.STREAMING)\n    config_checkpoint = WorkflowConfig(tier=WorkflowTier.STREAMING_CHECKPOINT)\n\n    print(f\"  config_standard.tier = {config_standard.tier}\")\n    print(f\"  config_chunked.tier = {config_chunked.tier}\")\n    print(f\"  config_streaming.tier = {config_streaming.tier}\")\n    print(f\"  config_checkpoint.tier = {config_checkpoint.tier}\")\n\n    # =========================================================================\n    # 7. Test Fit with Default Tier\n    # =========================================================================\n    print()\n    print()\n    print(\"7. Test Fit\")\n    print(\"-\" * 50)\n\n    n_samples = 1000\n    x_data = np.linspace(0, 5, n_samples)\n    true_a, true_b, true_c = 3.0, 1.2, 0.5\n    y_true = true_a * np.exp(-true_b * x_data) + true_c\n    y_data = y_true + 0.1 * np.random.randn(n_samples)\n\n    print(f\"  Test dataset: {n_samples} points\")\n    print(f\"  True parameters: a={true_a}, b={true_b}, c={true_c}\")\n\n    popt, _ = fit(\n        exponential_decay,\n        x_data,\n        y_data,\n        p0=[1.0, 1.0, 0.0],\n    )\n    print(f\"  Fitted: a={popt[0]:.4f}, b={popt[1]:.4f}, c={popt[2]:.4f}\")\n\n    # =========================================================================\n    # 8. Memory Usage Comparison\n    # =========================================================================\n    print()\n    print()\n    print(\"8. Saving memory usage comparison...\")\n\n    dataset_sizes = np.logspace(3, 9, 50)  # 1K to 1B points\n    n_params = 5\n\n    memory_standard = [\n        estimate_memory_usage(int(n), n_params, WorkflowTier.STANDARD)\n        for n in dataset_sizes\n    ]\n    memory_chunked = [\n        estimate_memory_usage(int(n), n_params, WorkflowTier.CHUNKED)\n        for n in dataset_sizes\n    ]\n    memory_streaming = [\n        estimate_memory_usage(int(n), n_params, WorkflowTier.STREAMING)\n        for n in dataset_sizes\n    ]\n\n    # Plot memory comparison\n    fig, ax = plt.subplots(figsize=(12, 7))\n\n    ax.loglog(dataset_sizes, memory_standard, \"b-\", linewidth=2, label=\"STANDARD\")\n    ax.loglog(dataset_sizes, memory_chunked, \"orange\", linewidth=2, label=\"CHUNKED\")\n    ax.loglog(dataset_sizes, memory_streaming, \"r-\", linewidth=2, label=\"STREAMING\")\n\n    # Add memory threshold lines\n    ax.axhline(y=16, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"16 GB limit\")\n    ax.axhline(y=64, color=\"gray\", linestyle=\":\", alpha=0.5, label=\"64 GB limit\")\n\n    # Add tier transition zones\n    ax.axvline(x=10_000, color=\"green\", linestyle=\"--\", alpha=0.3)\n    ax.axvline(x=10_000_000, color=\"orange\", linestyle=\"--\", alpha=0.3)\n    ax.axvline(x=100_000_000, color=\"red\", linestyle=\"--\", alpha=0.3)\n\n    ax.text(3000, 100, \"STANDARD\\nzone\", fontsize=9, ha=\"center\")\n    ax.text(300_000, 100, \"CHUNKED\\nzone\", fontsize=9, ha=\"center\")\n    ax.text(30_000_000, 100, \"STREAMING\\nzone\", fontsize=9, ha=\"center\")\n\n    ax.set_xlabel(\"Dataset Size (points)\")\n    ax.set_ylabel(\"Peak Memory Usage (GB)\")\n    ax.set_title(\"Memory Usage by Workflow Tier\")\n    ax.legend(loc=\"upper left\")\n    ax.grid(True, alpha=0.3, which=\"both\")\n    ax.set_xlim(1e3, 1e9)\n    ax.set_ylim(1e-3, 1e3)\n\n    plt.tight_layout()\n    plt.savefig(FIG_DIR / \"02_memory_comparison.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  Saved: {FIG_DIR / '02_memory_comparison.png'}\")\n\n    # =========================================================================\n    # 9. Defense Layers for Streaming Tiers (v0.3.6+)\n    # =========================================================================\n    print()\n    print()\n    print(\"9. Defense Layers for Streaming Tiers (v0.3.6+)\")\n    print(\"-\" * 50)\n    print()\n    print(\"STREAMING and STREAMING_CHECKPOINT tiers use AdaptiveHybridStreamingOptimizer,\")\n    print(\"which includes a 4-layer defense strategy against Adam warmup divergence:\")\n    print()\n    print(\"  Layer 1 (Warm Start Detection):\")\n    print(\"    - Skips warmup if initial loss < 1% of data variance\")\n    print(\"    - Prevents overshooting when starting near the optimum\")\n    print()\n    print(\"  Layer 2 (Adaptive Learning Rate):\")\n    print(\"    - Scales LR based on fit quality (1e-6 to 0.001)\")\n    print(\"    - lr_refinement=1e-6, lr_careful=1e-5, lr_exploration=0.001\")\n    print()\n    print(\"  Layer 3 (Cost-Increase Guard):\")\n    print(\"    - Aborts warmup if loss increases > 5%\")\n    print(\"    - Triggers early switch to Gauss-Newton phase\")\n    print()\n    print(\"  Layer 4 (Step Clipping):\")\n    print(\"    - Limits parameter update magnitude (max norm 0.1)\")\n    print(\"    - Prevents catastrophic parameter jumps\")\n    print()\n    print(\"Defense Presets:\")\n    print(\"  - HybridStreamingConfig.defense_strict()     # Warm-start refinement\")\n    print(\"  - HybridStreamingConfig.defense_relaxed()    # Exploration\")\n    print(\"  - HybridStreamingConfig.scientific_default() # Production scientific\")\n    print(\"  - HybridStreamingConfig.defense_disabled()   # Pre-0.3.6 behavior\")\n\n    # =========================================================================\n    # Summary\n    # =========================================================================\n    print()\n    print()\n    print(\"=\" * 70)\n    print(\"Summary\")\n    print(\"=\" * 70)\n    print()\n    print(\"Workflow Tiers:\")\n    print(\"  STANDARD:             < 10K points, full precision\")\n    print(\"  CHUNKED:              10K - 10M points, memory-managed\")\n    print(\"  STREAMING:            10M - 100M points, mini-batch + defense layers\")\n    print(\"  STREAMING_CHECKPOINT: > 100M points, fault-tolerant + defense layers\")\n    print()\n    print(\"Override syntax:\")\n    print(\"  config = WorkflowConfig(tier=WorkflowTier.CHUNKED)\")\n    print()\n    print(f\"Current system memory: {available_memory:.1f} GB ({current_tier.name})\")\n    print()\n    print(\"Key takeaways:\")\n    print(\"  - Automatic tier selection based on dataset size and memory\")\n    print(\"  - Override for specific memory constraints\")\n    print(\"  - STREAMING provides O(batch_size) memory for unlimited data\")\n    print(\"  - STREAMING tiers include 4-layer defense against warmup divergence\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
