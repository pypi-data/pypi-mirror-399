dataset: "banking"
known_cls_ratio: 0.25

seed: 0
data_dir: "data"
save_results_path: "results/gcd/dpn"
pretrain_dir: "outputs/gcd/dpn/premodels"
bert_model: "./pretrained_models/bert-base-uncased"
labeled_ratio: 0.1
fold_num: 5
fold_idx: 0

pretrain_batch_size: 32
train_batch_size: 256
eval_batch_size: 64
num_pretrain_epochs: 100
num_train_epochs: 100
lr_pre: !!float 5e-5
lr: !!float 1e-5
warmup_proportion: 0.1
wait_patient: 20

method: "DPN"
cluster_num_factor: 1.0
feat_dim: 768
momentum_factor: 0.9
temperature: 0.07

dataset_specific_configs:
  clinc:
    max_seq_length: 30
    gamma: 10
    num_train_epochs: 80
  stackoverflow:
    max_seq_length: 45
    gamma: 90
    num_train_epochs: 10
  banking:
    max_seq_length: 55
    gamma: 10
    num_train_epochs: 60
  mcid:
    max_seq_length: 65
    gamma: 30
    num_train_epochs: 80
  ecdt:
    max_seq_length: 65
    gamma: 30
    num_train_epochs: 80
  hwu:
    max_seq_length: 20
    gamma: 30
    num_train_epochs: 80
  TREC:
    max_seq_length: 40
    gamma: 10
    num_train_epochs: 80
  AGNews:
    max_seq_length: 128
    gamma: 30
    num_train_epochs: 30
  Yahoo: 
    max_seq_length: 128
    gamma: 30
    num_train_epochs: 30
  DBPedia:
    max_seq_length: 256
    gamma: 30
    train_batch_size: 16
    eval_batch_size: 32
    pretrain_batch_size: 16
    num_train_epochs: 30
  medical:
    max_seq_length: 256
    gamma: 30
    num_train_epochs: 30
  ele:
    max_seq_length: 200
    gamma: 90
    train_batch_size: 8
    eval_batch_size: 32
    pretrain_batch_size: 16
  news:
    max_seq_length: 500
    gamma: 90
    num_train_epochs: 10
    train_batch_size: 8
    eval_batch_size: 32
    pretrain_batch_size: 16
  thucnews:
    max_seq_length: 500
    gamma: 90
    num_train_epochs: 10
    train_batch_size: 16
    eval_batch_size: 32
    pretrain_batch_size: 16
  20NG: 
    max_seq_length: 512
    gamma: 90
    num_train_epochs: 10
    train_batch_size: 16
    eval_batch_size: 32
    pretrain_batch_size: 16
  XTopic:
    max_seq_length: 512
    gamma: 90
    max_seq_length: 128
    train_batch_size: 16
    eval_batch_size: 32
    pretrain_batch_size: 16