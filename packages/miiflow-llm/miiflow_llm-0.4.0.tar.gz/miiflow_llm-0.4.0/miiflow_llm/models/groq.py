"""Groq model configurations."""

from typing import Dict

from .base import ModelConfig, ParameterConfig, ParameterType

# Groq uses OpenAI-compatible API with max_tokens parameter

GROQ_MODELS: Dict[str, ModelConfig] = {
    "llama3-70b-8192": ModelConfig(
        model_identifier="llama3-70b-8192",
        name="groq-llama-3-70b",
        description="Large 70B parameter model with strong performance across diverse tasks.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=8192,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.59,
        output_cost_hint=0.79,
    ),
    "llama-3.3-70b-versatile": ModelConfig(
        model_identifier="llama-3.3-70b-versatile",
        name="groq-llama-3.3-70b-versatile",
        description="Llama 3.3 model trained on 70B parameters, a general-purpose model that performs well across multiple domains and task types.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=8192,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.59,
        output_cost_hint=0.79,
    ),
    "llama-guard-3-8b": ModelConfig(
        model_identifier="llama-guard-3-8b",
        name="groq-llama-guard-3-8b",
        description="Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification.",
        support_images=False,
        support_files=False,
        support_streaming=False,
        supports_json_mode=False,
        supports_tool_call=False,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=8192,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.20,
        output_cost_hint=0.20,
    ),
    "llama-3.1-8b-instant": ModelConfig(
        model_identifier="llama-3.1-8b-instant",
        name="groq-llama-3-8b",
        description="Fast and efficient 8B parameter model ideal for quick responses and high throughput.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=8192,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.05,
        output_cost_hint=0.08,
    ),
    "llama-4-scout": ModelConfig(
        model_identifier="meta-llama/llama-4-scout-17b-16e-instruct",
        name="groq-llama-4-scout",
        description="Latest Llama 4 model with multimodal capabilities and extended context length.",
        support_images=True,
        support_files=True,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=10000000,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.11,
        output_cost_hint=0.34,
    ),
    "llama-4-maverick": ModelConfig(
        model_identifier="meta-llama/llama-4-maverick-17b-128e-instruct",
        name="groq-llama-4-maverick",
        description="Advanced Llama 4 model optimized for complex reasoning and multimodal tasks.",
        support_images=True,
        support_files=True,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=1000000,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.20,
        output_cost_hint=0.60,
    ),
    "mixtral-8x7b-32768": ModelConfig(
        model_identifier="mixtral-8x7b-32768",
        name="groq-mixtral-8x7b",
        description="Mixtral 8x7B is a 7B parameter model trained on 32768 context length.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=False,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=32768,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.24,
        output_cost_hint=0.24,
    ),
    "gemma2-9b-it": ModelConfig(
        model_identifier="gemma2-9b-it",
        name="groq-gemma2-9b-it",
        description="Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=8192,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.20,
        output_cost_hint=0.20,
    ),
}


GROQ_PARAMETERS: list[ParameterConfig] = [
    ParameterConfig(
        field_name="temperature",
        display_name="Temperature",
        description="What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
        parameter_type=ParameterType.NUMBER,
        default_value=0.5,
        min_value=0,
        max_value=2,
        step=0.1,
    ),
    ParameterConfig(
        field_name="frequency_penalty",
        display_name="Frequency Penalty",
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
        parameter_type=ParameterType.NUMBER,
        default_value=0,
        min_value=-2,
        max_value=2,
        step=0.1,
    ),
    ParameterConfig(
        field_name="presence_penalty",
        display_name="Presence Penalty",
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
        parameter_type=ParameterType.NUMBER,
        default_value=0,
        min_value=-2,
        max_value=2,
        step=0.1,
    ),
    ParameterConfig(
        field_name="max_tokens",
        display_name="Max Tokens",
        description="An upper bound for the number of tokens that can be generated for a completion.",
        parameter_type=ParameterType.NUMBER,
        default_value=5000,
        min_value=1,
        max_value={
            "llama3-70b-8192": 8192,
            "llama-3.3-70b-versatile": 32768,
            "llama-guard-3-8b": 8192,
            "llama-3.1-8b-instant": 8192,
            "llama-4-scout": 4096,
            "llama-4-maverick": 4096,
            "mixtral-8x7b-32768": 32768,
            "gemma2-9b-it": 8192,
            "default": 8192,
        },
        step=4,
    ),
]
