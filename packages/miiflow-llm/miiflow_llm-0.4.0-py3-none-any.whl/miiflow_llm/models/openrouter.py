"""OpenRouter model configurations.

OpenRouter is a gateway service that provides access to models from multiple providers.
All models use OpenAI-compatible API with max_tokens parameter.
"""

from typing import Dict

from .base import ModelConfig, ParameterConfig, ParameterType

# OpenRouter provides access to many models via a unified API
# Pricing varies by the underlying model provider

OPENROUTER_MODELS: Dict[str, ModelConfig] = {
    # OpenAI models via OpenRouter
    "openai/gpt-4o": ModelConfig(
        model_identifier="openai/gpt-4o",
        name="openai/gpt-4o",
        description="OpenAI GPT-4o via OpenRouter - flagship multimodal model.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=16384,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=2.50,
        output_cost_hint=10.0,
    ),
    "openai/gpt-4o-mini": ModelConfig(
        model_identifier="openai/gpt-4o-mini",
        name="openai/gpt-4o-mini",
        description="OpenAI GPT-4o Mini via OpenRouter - fast and efficient.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=16384,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.15,
        output_cost_hint=0.60,
    ),
    # Anthropic models via OpenRouter
    "anthropic/claude-3.5-sonnet": ModelConfig(
        model_identifier="anthropic/claude-3.5-sonnet",
        name="anthropic/claude-3.5-sonnet",
        description="Anthropic Claude 3.5 Sonnet via OpenRouter.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=200000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=3.0,
        output_cost_hint=15.0,
    ),
    "anthropic/claude-3-haiku": ModelConfig(
        model_identifier="anthropic/claude-3-haiku",
        name="anthropic/claude-3-haiku",
        description="Anthropic Claude 3 Haiku via OpenRouter - fast and affordable.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=200000,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.25,
        output_cost_hint=1.25,
    ),
    # Google models via OpenRouter
    "google/gemini-pro-1.5": ModelConfig(
        model_identifier="google/gemini-pro-1.5",
        name="google/gemini-pro-1.5",
        description="Google Gemini Pro 1.5 via OpenRouter.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=1000000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=1.25,
        output_cost_hint=5.0,
    ),
    "google/gemini-flash-1.5": ModelConfig(
        model_identifier="google/gemini-flash-1.5",
        name="google/gemini-flash-1.5",
        description="Google Gemini Flash 1.5 via OpenRouter - fast and efficient.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=1000000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.075,
        output_cost_hint=0.30,
    ),
    # Meta Llama models
    "meta-llama/llama-3.1-405b-instruct": ModelConfig(
        model_identifier="meta-llama/llama-3.1-405b-instruct",
        name="meta-llama/llama-3.1-405b-instruct",
        description="Meta Llama 3.1 405B via OpenRouter - largest open model.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=131072,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=2.70,
        output_cost_hint=2.70,
    ),
    "meta-llama/llama-3.1-70b-instruct": ModelConfig(
        model_identifier="meta-llama/llama-3.1-70b-instruct",
        name="meta-llama/llama-3.1-70b-instruct",
        description="Meta Llama 3.1 70B via OpenRouter.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=131072,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.52,
        output_cost_hint=0.75,
    ),
    "meta-llama/llama-3.1-8b-instruct": ModelConfig(
        model_identifier="meta-llama/llama-3.1-8b-instruct",
        name="meta-llama/llama-3.1-8b-instruct",
        description="Meta Llama 3.1 8B via OpenRouter - fast and efficient.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=131072,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.055,
        output_cost_hint=0.055,
    ),
    # Free models for testing
    "meta-llama/llama-3.2-3b-instruct:free": ModelConfig(
        model_identifier="meta-llama/llama-3.2-3b-instruct:free",
        name="meta-llama/llama-3.2-3b-instruct:free",
        description="Free Llama 3.2 3B model for testing.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=False,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=131072,
        maximum_output_tokens=4096,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.0,
        output_cost_hint=0.0,
    ),
}


OPENROUTER_PARAMETERS: list[ParameterConfig] = [
    ParameterConfig(
        field_name="temperature",
        display_name="Temperature",
        description="Controls randomness in responses.",
        parameter_type=ParameterType.NUMBER,
        default_value=0.7,
        min_value=0,
        max_value=2,
        step=0.1,
    ),
    ParameterConfig(
        field_name="top_p",
        display_name="Top P",
        description="Nucleus sampling parameter.",
        parameter_type=ParameterType.NUMBER,
        default_value=1.0,
        min_value=0,
        max_value=1,
        step=0.1,
    ),
    ParameterConfig(
        field_name="max_tokens",
        display_name="Max Tokens",
        description="Maximum number of tokens to generate.",
        parameter_type=ParameterType.NUMBER,
        default_value=4096,
        min_value=1,
        max_value=16384,
        step=4,
    ),
    ParameterConfig(
        field_name="frequency_penalty",
        display_name="Frequency Penalty",
        description="Penalizes tokens based on frequency.",
        parameter_type=ParameterType.NUMBER,
        default_value=0,
        min_value=-2,
        max_value=2,
        step=0.1,
    ),
    ParameterConfig(
        field_name="presence_penalty",
        display_name="Presence Penalty",
        description="Penalizes tokens based on presence.",
        parameter_type=ParameterType.NUMBER,
        default_value=0,
        min_value=-2,
        max_value=2,
        step=0.1,
    ),
]
