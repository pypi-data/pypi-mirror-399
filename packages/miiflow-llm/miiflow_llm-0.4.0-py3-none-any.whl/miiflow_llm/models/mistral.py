"""Mistral model configurations."""

from typing import Dict

from .base import ModelConfig, ParameterConfig, ParameterType

# Mistral uses max_tokens parameter

MISTRAL_MODELS: Dict[str, ModelConfig] = {
    "mistral-large-latest": ModelConfig(
        model_identifier="mistral-large-latest",
        name="mistral-large-latest",
        description="Mistral's most capable model for complex reasoning and generation tasks.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=2.0,
        output_cost_hint=6.0,
    ),
    "mistral-medium-latest": ModelConfig(
        model_identifier="mistral-medium-latest",
        name="mistral-medium-latest",
        description="Balanced performance model for a wide range of tasks.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=2.7,
        output_cost_hint=8.1,
    ),
    "mistral-small-latest": ModelConfig(
        model_identifier="mistral-small-latest",
        name="mistral-small-latest",
        description="Cost-effective model for simpler tasks with good performance.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.2,
        output_cost_hint=0.6,
    ),
    "codestral-latest": ModelConfig(
        model_identifier="codestral-latest",
        name="codestral-latest",
        description="Specialized coding model optimized for code generation and understanding.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=32000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.3,
        output_cost_hint=0.9,
    ),
    "pixtral-large-latest": ModelConfig(
        model_identifier="pixtral-large-latest",
        name="pixtral-large-latest",
        description="Mistral's multimodal model with vision capabilities.",
        support_images=True,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=2.0,
        output_cost_hint=6.0,
    ),
    "ministral-8b-latest": ModelConfig(
        model_identifier="ministral-8b-latest",
        name="ministral-8b-latest",
        description="Fast and efficient 8B parameter model for quick responses.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.1,
        output_cost_hint=0.1,
    ),
    "ministral-3b-latest": ModelConfig(
        model_identifier="ministral-3b-latest",
        name="ministral-3b-latest",
        description="Ultra-efficient 3B parameter model for cost-sensitive applications.",
        support_images=False,
        support_files=False,
        support_streaming=True,
        supports_json_mode=True,
        supports_tool_call=True,
        supports_structured_outputs=False,
        reasoning=False,
        maximum_context_tokens=128000,
        maximum_output_tokens=8192,
        token_param_name="max_tokens",
        supports_temperature=True,
        input_cost_hint=0.04,
        output_cost_hint=0.04,
    ),
}


MISTRAL_PARAMETERS: list[ParameterConfig] = [
    ParameterConfig(
        field_name="temperature",
        display_name="Temperature",
        description="Controls randomness in responses. Higher values make output more random.",
        parameter_type=ParameterType.NUMBER,
        default_value=0.7,
        min_value=0,
        max_value=1,
        step=0.1,
    ),
    ParameterConfig(
        field_name="top_p",
        display_name="Top P",
        description="Nucleus sampling parameter. Considers tokens with top_p probability mass.",
        parameter_type=ParameterType.NUMBER,
        default_value=1.0,
        min_value=0,
        max_value=1,
        step=0.1,
    ),
    ParameterConfig(
        field_name="max_tokens",
        display_name="Max Tokens",
        description="Maximum number of tokens to generate.",
        parameter_type=ParameterType.NUMBER,
        default_value=4096,
        min_value=1,
        max_value=8192,
        step=4,
    ),
]
