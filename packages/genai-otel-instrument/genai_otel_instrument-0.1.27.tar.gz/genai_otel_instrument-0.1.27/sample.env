# Configuration for GenAI OpenTelemetry instrumentation

# The name of the service. Defaults to "genai-app".
OTEL_SERVICE_NAME=genai-app

# The OTLP exporter endpoint. Defaults to "http://localhost:4318".
# If not set or empty, telemetry will be exported to console instead of OTLP.
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# OTLP exporter protocol. Defaults to "http/protobuf" (HTTP).
# Options:
#   - "http/protobuf" or unset: Use HTTP protocol (default)
#   - "grpc": Use gRPC protocol
# Note: This setting is checked at import time, so it must be set before importing genai_otel.
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

# Timeout for OTLP exporter in seconds. Defaults to 60 seconds.
# This is the maximum time the exporter will wait for a response from the OTLP endpoint.
# OTEL_EXPORTER_OTLP_TIMEOUT=60

# Optional headers for the OTLP exporter, in key=value format, separated by commas.
# Example: OTEL_EXPORTER_OTLP_HEADERS=my-header=my-value,another-header=another-value
# OTEL_EXPORTER_OTLP_HEADERS=

# Optional service instance identifier (e.g., container ID, pod name, hostname).
# Useful for distinguishing between multiple instances of the same service.
# OTEL_SERVICE_INSTANCE_ID=

# Deployment environment (e.g., dev, staging, production).
# OTEL_ENVIRONMENT=

# Comma-separated list of instrumentors to enable. If not set, all supported instrumentors are enabled by default.
# Example: GENAI_ENABLED_INSTRUMENTORS=openai,langchain,llama_index
# GENAI_ENABLED_INSTRUMENTORS=

# Enable GPU metrics collection. Defaults to "true".
# GENAI_ENABLE_GPU_METRICS=true

# GPU metrics collection interval in seconds. Defaults to 5 seconds.
# Controls how frequently GPU metrics (utilization, memory, temperature, power) are collected.
# GENAI_GPU_COLLECTION_INTERVAL=5

# Enable cost tracking. Defaults to "true".
# GENAI_ENABLE_COST_TRACKING=true

# Custom pricing for models not in llm_pricing.json
# Provide a JSON string with the same structure as llm_pricing.json
# Custom prices will be merged with default pricing, with custom taking precedence
# Format: {"chat": {"model-name": {"promptPrice": 0.001, "completionPrice": 0.002}}}
# Example for chat models:
# GENAI_CUSTOM_PRICING_JSON='{"chat":{"my-custom-model":{"promptPrice":0.001,"completionPrice":0.002}}}'
# Example for embeddings:
# GENAI_CUSTOM_PRICING_JSON='{"embeddings":{"my-custom-embeddings":0.00005}}'
# Example for multiple categories:
# GENAI_CUSTOM_PRICING_JSON='{"chat":{"custom-chat":{"promptPrice":0.001,"completionPrice":0.002}},"embeddings":{"custom-embed":0.00005}}'
# GENAI_CUSTOM_PRICING_JSON=

# Enable MCP (Model Context Protocol) instrumentation. Defaults to "true".
# This includes databases, vector DBs, Redis, Kafka, etc.
# GENAI_ENABLE_MCP_INSTRUMENTATION=true

# Enable HTTP/API instrumentation (requests/httpx). Defaults to "false".
# Disabled by default to avoid conflicts with OTLP HTTP exporters.
# GENAI_ENABLE_HTTP_INSTRUMENTATION=false

# Logging configuration
# Set the logging level for the genai-otel library. Defaults to INFO.
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
GENAI_OTEL_LOG_LEVEL=INFO

# If set to "true", the instrumentation will raise an error if it fails. Defaults to "false".
# GENAI_FAIL_ON_ERROR=false

# Enable CO2 tracking. Defaults to "false".
# When enabled, CO2 emissions are calculated based on GPU power consumption.
# If codecarbon is installed, it provides automatic region-based carbon intensity.
# Install codecarbon: pip install genai-otel-instrument[co2]
# GENAI_ENABLE_CO2_TRACKING=false

# Carbon intensity in gCO2e/kWh. Defaults to "475.0".
# Used as fallback when codecarbon is not installed or fails to initialize.
# This value represents the CO2 emissions per kilowatt-hour of electricity.
# Examples: US average ~420, UK ~233, Germany ~350, France ~56, China ~555
# GENAI_CARBON_INTENSITY=475.0

# Electricity cost per kilowatt-hour (kWh) in USD. Defaults to "0.12".
# Used to calculate power consumption cost for GPU usage tracking.
# This metric (gen_ai.power.cost) tracks cumulative electricity costs based on GPU power draw.
# Common rates: US average ~$0.12/kWh, Europe ~$0.20/kWh, Industrial ~$0.07/kWh
# GENAI_POWER_COST_PER_KWH=0.12

# --- Codecarbon Integration Settings (for CO2 tracking) ---
# These settings are used when GENAI_ENABLE_CO2_TRACKING=true and codecarbon is installed.
# Codecarbon provides more accurate CO2 calculations using region-specific carbon intensity data.

# Country ISO code (3-letter). Examples: "USA", "GBR", "DEU", "FRA", "CHN", "IND"
# IMPORTANT: If not set, defaults to "USA". Set this to match your actual location for accurate carbon intensity.
# Full list: https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/private_infra/2022/global_energy_mix.json
# GENAI_CO2_COUNTRY_ISO_CODE=USA
# GENAI_CO2_COUNTRY_ISO_CODE=IND  # For India

# Region/state within country (optional). Examples: "california", "texas", "new york"
# Provides more accurate carbon intensity for US states.
# GENAI_CO2_REGION=

# Cloud provider name (optional). Examples: "aws", "gcp", "azure"
# Used for cloud-specific carbon intensity data.
# GENAI_CO2_CLOUD_PROVIDER=

# Cloud region (optional). Examples: "us-east-1", "europe-west1", "eastus"
# Used together with cloud_provider for accurate cloud carbon intensity.
# GENAI_CO2_CLOUD_REGION=

# Run codecarbon in offline mode (no external API calls). Defaults to "true".
# When true, uses local carbon intensity data based on country_iso_code.
# When false, may fetch real-time carbon intensity from APIs (requires internet).
# GENAI_CO2_OFFLINE_MODE=true

# Tracking mode. Options: "machine" (all processes) or "process" (current only). Defaults to "machine".
# "machine": Tracks total machine power consumption (more accurate for dedicated GPU servers)
# "process": Tracks only the current process (useful for shared environments)
# GENAI_CO2_TRACKING_MODE=machine

# Force manual CO2 calculation. Defaults to "false".
# When true, uses GENAI_CARBON_INTENSITY for CO2 calculation even if codecarbon is installed.
# Useful when you want to use your own carbon intensity value instead of codecarbon's region-based data.
# GENAI_CO2_USE_MANUAL=false

# Codecarbon logging level. Options: "debug", "info", "warning", "error", "critical". Defaults to "error".
# Controls verbosity of codecarbon's internal logging (warnings about CPU tracking mode, multiple instances, etc.)
# Set to "error" (default) to suppress informational warnings
# Set to "warning" or "info" to see codecarbon's diagnostic messages
# GENAI_CODECARBON_LOG_LEVEL=error

# --- OpenTelemetry Semantic Conventions ---

# OpenTelemetry semantic convention stability opt-in. Defaults to "gen_ai".
# Options:
#   - "gen_ai": Use new semantic conventions only (default)
#   - "gen_ai/dup": Emit both old and new token attributes for migration compatibility
# When set to "gen_ai/dup", both attribute sets are emitted:
#   - New: gen_ai.usage.prompt_tokens, gen_ai.usage.completion_tokens
#   - Old: gen_ai.usage.input_tokens, gen_ai.usage.output_tokens
# OTEL_SEMCONV_STABILITY_OPT_IN=gen_ai

# Enable content capture as span events. Defaults to "false".
# WARNING: When enabled, this captures full prompt and completion content as span events.
#          This may expose sensitive data. Use with caution and ensure proper data handling.
# GENAI_ENABLE_CONTENT_CAPTURE=false

# --- Ollama Server Metrics Configuration ---

# Enable automatic Ollama server metrics polling. Defaults to "true".
# When enabled, polls Ollama's /api/ps endpoint to get VRAM usage and running models.
# This populates the gen_ai.server.kv_cache.usage metric automatically.
# GENAI_ENABLE_OLLAMA_SERVER_METRICS=true

# Ollama server base URL. Defaults to "http://localhost:11434".
# Change this if Ollama is running on a different host or port.
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama server metrics polling interval in seconds. Defaults to "5.0".
# How frequently to query /api/ps for running model metrics.
# GENAI_OLLAMA_METRICS_INTERVAL=5.0

# Maximum VRAM in GB for your GPU. OPTIONAL - Auto-detected if not set.
# The system will auto-detect GPU VRAM using nvidia-ml-py or nvidia-smi.
# Only set this if auto-detection fails or you want to override the detected value.
# Example: GENAI_OLLAMA_MAX_VRAM_GB=24 for a 24GB GPU (e.g., RTX 3090, 4090, A5000)
# Auto-detection requires: pip install genai-otel-instrument[gpu] (for nvidia-ml-py)
# GENAI_OLLAMA_MAX_VRAM_GB=

# --- API Keys and Connection Strings ---

# LLM Provider API Keys
# OPENAI_API_KEY=your_openai_api_key
# ANTHROPIC_API_KEY=your_anthropic_api_key
# GOOGLE_API_KEY=your_google_api_key
# AWS_ACCESS_KEY_ID=your_aws_access_key_id
# AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key
# AWS_REGION=your_aws_region
# AZURE_OPENAI_API_KEY=your_azure_openai_api_key
# AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
# COHERE_API_KEY=your_cohere_api_key
# MISTRAL_API_KEY=your_mistral_api_key
# TOGETHER_API_KEY=your_together_api_key
# GROQ_API_KEY=your_groq_api_key
# REPLICATE_API_TOKEN=your_replicate_api_token
# ANYSCALE_API_KEY=your_anyscale_api_key
# For Vertex AI, ensure GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path of your service account key file.

# MCP Instrumentation Connection Details

# Databases
# POSTGRES_USER=your_postgres_user
# POSTGRES_PASSWORD=your_postgres_password
# POSTGRES_HOST=your_postgres_host
# POSTGRES_PORT=5432
# POSTGRES_DB=your_postgres_db

# MYSQL_USER=your_mysql_user
# MYSQL_PASSWORD=your_mysql_password
# MYSQL_HOST=your_mysql_host
# MYSQL_PORT=3306
# MYSQL_DB=your_mysql_db

# MONGODB_URI=your_mongodb_connection_string

# Caching
# REDIS_HOST=your_redis_host
# REDIS_PORT=6379
# REDIS_PASSWORD=your_redis_password

# Message Queues
# KAFKA_BOOTSTRAP_SERVERS=your_kafka_bootstrap_servers

# Vector Databases
# PINECONE_API_KEY=your_pinecone_api_key
# PINECONE_ENVIRONMENT=your_pinecone_environment

# WEAVIATE_URL=your_weaviate_url
# WEAVIATE_API_KEY=your_weaviate_api_key

# QDRANT_URL=your_qdrant_url
# QDRANT_API_KEY=your_qdrant_api_key

# MILVUS_HOST=your_milvus_host
# MILVUS_PORT=19530
