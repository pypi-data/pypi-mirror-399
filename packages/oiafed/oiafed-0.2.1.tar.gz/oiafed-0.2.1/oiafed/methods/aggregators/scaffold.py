"""
SCAFFOLD èšåˆå™¨

ä»Ž methods/aggregators/scaffold.py è¿ç§»åˆ° src/

å®žçŽ° SCAFFOLD (Stochastic Controlled Averaging for Federated Learning) èšåˆç®—æ³•ã€‚
ä½¿ç”¨æŽ§åˆ¶å˜é‡å‡å°‘å®¢æˆ·ç«¯æ¼‚ç§»,æé«˜è”é‚¦å­¦ä¹ çš„æ”¶æ•›é€Ÿåº¦ã€‚

è®ºæ–‡: SCAFFOLD: Stochastic Controlled Averaging for Federated Learning
ä½œè€…: Sai Praneeth Karimireddy et al.
å‘è¡¨: ICML 2020

ç®—æ³•ç‰¹ç‚¹:
1. ä½¿ç”¨æŽ§åˆ¶å˜é‡çº æ­£å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çš„æ›´æ–°åå·®
2. æ›´å¥½çš„æ”¶æ•›ä¿è¯,ç‰¹åˆ«æ˜¯åœ¨æ•°æ®å¼‚æž„æƒ…å†µä¸‹
3. éœ€è¦é¢å¤–å­˜å‚¨å’Œä¼ è¾“æŽ§åˆ¶å˜é‡
4. é€‚åˆæ•°æ®åˆ†å¸ƒå·®å¼‚è¾ƒå¤§çš„è”é‚¦å­¦ä¹ åœºæ™¯
"""

import torch
from typing import List, Dict, Any, Optional
from loguru import logger

from ...core.aggregator import Aggregator
from ...core.types import ClientUpdate
from ...registry import aggregator


@aggregator(
    name='scaffold',
    description='SCAFFOLDæŽ§åˆ¶å˜é‡è”é‚¦èšåˆå™¨',
    version='1.0'
)
class SCAFFOLDAggregator(Aggregator):
    """
    SCAFFOLD èšåˆå™¨å®žçŽ°

    ç®—æ³•æ ¸å¿ƒ:
    1. ç»´æŠ¤å…¨å±€æŽ§åˆ¶å˜é‡ c
    2. æ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤æœ¬åœ°æŽ§åˆ¶å˜é‡ c_i
    3. å®¢æˆ·ç«¯æ›´æ–°è€ƒè™‘æŽ§åˆ¶å˜é‡çš„æ¢¯åº¦ä¿®æ­£
    4. æœåŠ¡å™¨èšåˆæ—¶åŒæ—¶æ›´æ–°æ¨¡åž‹å’ŒæŽ§åˆ¶å˜é‡

    å‚æ•°:
    - learning_rate: å…¨å±€å­¦ä¹ çŽ‡,é»˜è®¤1.0
    - control_lr: æŽ§åˆ¶å˜é‡å­¦ä¹ çŽ‡,é»˜è®¤None(è‡ªåŠ¨è®¡ç®—)
    - weighted: æ˜¯å¦æŒ‰æ ·æœ¬æ•°é‡åŠ æƒ,é»˜è®¤True
    - momentum: åŠ¨é‡ç³»æ•°,é»˜è®¤0.0
    """

    def __init__(self, learning_rate: float = 1.0, control_lr: Optional[float] = None,
                 weighted: bool = True, momentum: float = 0.0, **kwargs):
        """åˆå§‹åŒ–SCAFFOLDèšåˆå™¨"""
        # SCAFFOLDç‰¹å®šå‚æ•°
        self.learning_rate = learning_rate
        self.control_lr = control_lr  # è‡ªåŠ¨è®¡ç®—
        self._weighted = weighted
        self.momentum = momentum

        # è®¾å¤‡é…ç½®
        device = kwargs.get("device", "auto")
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device

        # æŽ§åˆ¶å˜é‡çŠ¶æ€
        self.global_control_variate: Optional[Dict[str, torch.Tensor]] = None
        self.client_control_variates: Dict[str, Dict[str, torch.Tensor]] = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self.round_count = 0
        self.control_variate_norm_history = []

        logger.info(f"âœ… SCAFFOLDèšåˆå™¨åˆå§‹åŒ–å®Œæˆ - LR: {self.learning_rate}, åŠ¨é‡: {self.momentum}")

    def aggregate(self, updates: List[ClientUpdate], global_model=None) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡ŒSCAFFOLDèšåˆ

        Args:
            updates: å®¢æˆ·ç«¯æ›´æ–°åˆ—è¡¨ (List[ClientUpdate])
            global_model: å…¨å±€æ¨¡åž‹ (å¯é€‰)

        Returns:
            èšåˆåŽçš„æ¨¡åž‹æƒé‡å­—å…¸
        """
        if not updates:
            raise ValueError("æ²¡æœ‰å®¢æˆ·ç«¯æ›´æ–°å¯èšåˆ")

        self.round_count += 1
        logger.debug(f"ðŸ”„ SCAFFOLDèšåˆè½®æ¬¡ {self.round_count} - {len(updates)} ä¸ªå®¢æˆ·ç«¯")

        # 1. åˆå§‹åŒ–å…¨å±€æŽ§åˆ¶å˜é‡(å¦‚æžœæ˜¯ç¬¬ä¸€æ¬¡)
        if self.global_control_variate is None:
            self._initialize_global_control_variate(updates[0])

        # 2. è®¡ç®—èšåˆæƒé‡
        weights = self._compute_aggregation_weights(updates)

        # 3. èšåˆæ¨¡åž‹å‚æ•°
        aggregated_weights = self._aggregate_model_weights(updates, weights)

        # 4. æ›´æ–°æŽ§åˆ¶å˜é‡
        control_stats = self._update_control_variates(updates, weights)

        logger.debug(f"âœ… SCAFFOLDèšåˆå®Œæˆ - æŽ§åˆ¶å˜é‡èŒƒæ•°: {control_stats.get('global_cv_norm', 0):.6f}")

        return aggregated_weights

    def _initialize_global_control_variate(self, sample_update: ClientUpdate):
        """åˆå§‹åŒ–å…¨å±€æŽ§åˆ¶å˜é‡"""
        model_weights = sample_update.weights
        self.global_control_variate = {}

        for param_name, param_value in model_weights.items():
            if isinstance(param_value, torch.Tensor):
                self.global_control_variate[param_name] = torch.zeros_like(
                    param_value, device=self.device
                )
            else:
                self.global_control_variate[param_name] = 0.0

        logger.debug("ðŸ”§ å…¨å±€æŽ§åˆ¶å˜é‡å·²åˆå§‹åŒ–")

    def _compute_aggregation_weights(self, updates: List[ClientUpdate]) -> List[float]:
        """è®¡ç®—èšåˆæƒé‡"""
        if not self._weighted:
            num_clients = len(updates)
            return [1.0 / num_clients] * num_clients

        sample_counts = [update.num_samples for update in updates]
        total_samples = sum(sample_counts)

        if total_samples == 0:
            num_clients = len(updates)
            return [1.0 / num_clients] * num_clients

        return [count / total_samples for count in sample_counts]

    def _aggregate_model_weights(self, updates: List[ClientUpdate],
                                weights: List[float]) -> Dict[str, torch.Tensor]:
        """èšåˆæ¨¡åž‹æƒé‡"""
        aggregated_weights = {}

        # èŽ·å–å‚æ•°ç»“æž„
        first_weights = updates[0].weights
        param_names = list(first_weights.keys())

        # åˆå§‹åŒ–èšåˆç»“æžœ
        for param_name in param_names:
            param_tensor = first_weights[param_name]
            if isinstance(param_tensor, torch.Tensor):
                # ä½¿ç”¨float32é¿å…Longç±»åž‹è½¬æ¢é”™è¯¯
                aggregated_weights[param_name] = torch.zeros_like(param_tensor, dtype=torch.float32, device=self.device)
            else:
                aggregated_weights[param_name] = 0.0

        # åŠ æƒèšåˆ
        for i, update in enumerate(updates):
            client_weights = update.weights
            weight = weights[i]

            for param_name in param_names:
                if param_name in client_weights:
                    param_value = client_weights[param_name]

                    if isinstance(param_value, torch.Tensor):
                        param_value = param_value.to(self.device)
                        # è½¬æ¢æ•´æ•°ç±»åž‹ä¸ºfloat
                        if param_value.dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                            param_value = param_value.float()
                        aggregated_weights[param_name] += weight * param_value
                    else:
                        aggregated_weights[param_name] += weight * param_value

        # è½¬æ¢å›žåŽŸå§‹ç±»åž‹
        for param_name in param_names:
            if isinstance(first_weights[param_name], torch.Tensor):
                if first_weights[param_name].dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                    aggregated_weights[param_name] = aggregated_weights[param_name].long()

        return aggregated_weights

    def _update_control_variates(self, updates: List[ClientUpdate],
                               weights: List[float]) -> Dict[str, float]:
        """æ›´æ–°æŽ§åˆ¶å˜é‡"""
        control_stats = {}

        # è®¡ç®—æŽ§åˆ¶å˜é‡å­¦ä¹ çŽ‡
        if self.control_lr is None:
            # è‡ªåŠ¨è®¡ç®—: åŸºäºŽå¹³å‡æœ¬åœ°epochæ•°
            avg_local_epochs = 1.0
            if hasattr(updates[0], 'metadata') and updates[0].metadata:
                local_epochs_list = [u.metadata.get("local_epochs", 1) for u in updates if u.metadata]
                if local_epochs_list:
                    avg_local_epochs = sum(local_epochs_list) / len(local_epochs_list)
            effective_lr = self.learning_rate / avg_local_epochs
        else:
            effective_lr = self.control_lr

        # æ›´æ–°å…¨å±€æŽ§åˆ¶å˜é‡
        control_variate_deltas = {}

        # èšåˆæŽ§åˆ¶å˜é‡å¢žé‡
        for param_name in self.global_control_variate.keys():
            control_variate_deltas[param_name] = torch.zeros_like(
                self.global_control_variate[param_name], device=self.device
            )

        for i, update in enumerate(updates):
            client_id = update.node_id if hasattr(update, 'node_id') else f"client_{i}"
            weight = weights[i]

            # èŽ·å–å®¢æˆ·ç«¯æŽ§åˆ¶å˜é‡å¢žé‡
            if hasattr(update, 'metadata') and update.metadata and "control_variate_delta" in update.metadata:
                cv_delta = update.metadata["control_variate_delta"]

                for param_name in control_variate_deltas.keys():
                    if param_name in cv_delta:
                        delta_value = cv_delta[param_name]
                        if isinstance(delta_value, torch.Tensor):
                            delta_value = delta_value.to(self.device)
                            control_variate_deltas[param_name] += weight * delta_value

            # æ›´æ–°å®¢æˆ·ç«¯æŽ§åˆ¶å˜é‡ç¼“å­˜
            if hasattr(update, 'metadata') and update.metadata and "control_variate" in update.metadata:
                self.client_control_variates[client_id] = update.metadata["control_variate"]

        # åº”ç”¨æŽ§åˆ¶å˜é‡æ›´æ–°
        global_cv_norm = 0.0
        for param_name, delta in control_variate_deltas.items():
            if isinstance(delta, torch.Tensor):
                # ä½¿ç”¨åŠ¨é‡æ›´æ–°
                if self.momentum > 0:
                    self.global_control_variate[param_name] = (
                        self.momentum * self.global_control_variate[param_name] +
                        (1 - self.momentum) * effective_lr * delta
                    )
                else:
                    self.global_control_variate[param_name] += effective_lr * delta

                # è®¡ç®—èŒƒæ•° - è½¬æ¢ä¸ºfloatä»¥ä¾¿è®¡ç®—norm
                cv_tensor = self.global_control_variate[param_name]
                if cv_tensor.dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                    cv_tensor = cv_tensor.float()
                global_cv_norm += torch.norm(cv_tensor).item() ** 2

        global_cv_norm = global_cv_norm ** 0.5
        self.control_variate_norm_history.append(global_cv_norm)

        control_stats = {
            "global_cv_norm": global_cv_norm,
            "effective_control_lr": effective_lr,
            "num_client_cv_updates": len([u for u in updates if hasattr(u, 'metadata') and u.metadata and "control_variate_delta" in u.metadata])
        }

        return control_stats

    def get_client_control_variate(self, client_id: str) -> Optional[Dict[str, torch.Tensor]]:
        """èŽ·å–æŒ‡å®šå®¢æˆ·ç«¯çš„æŽ§åˆ¶å˜é‡"""
        return self.client_control_variates.get(client_id)

    def get_control_variate_trend(self) -> List[float]:
        """èŽ·å–æŽ§åˆ¶å˜é‡èŒƒæ•°çš„åŽ†å²è¶‹åŠ¿"""
        return self.control_variate_norm_history.copy()

    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–èšåˆå™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "algorithm": "SCAFFOLD",
            "learning_rate": self.learning_rate,
            "control_lr": self.control_lr,
            "momentum": self.momentum,
            "total_rounds": self.round_count,
            "num_registered_clients": len(self.client_control_variates),
            "device": str(self.device)
        }

        # æ·»åŠ æŽ§åˆ¶å˜é‡ç»Ÿè®¡
        if self.control_variate_norm_history:
            stats["latest_cv_norm"] = self.control_variate_norm_history[-1]
            stats["avg_cv_norm"] = sum(self.control_variate_norm_history) / len(self.control_variate_norm_history)

            if len(self.control_variate_norm_history) > 1:
                trend = "increasing" if (self.control_variate_norm_history[-1] >
                                       self.control_variate_norm_history[0]) else "decreasing"
                stats["cv_norm_trend"] = trend

        return stats

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.round_count = 0
        self.control_variate_norm_history.clear()
        self.client_control_variates.clear()
        self.global_control_variate = None
        logger.info("ðŸ”„ SCAFFOLDèšåˆå™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

    def __repr__(self) -> str:
        return (f"SCAFFOLDAggregator(lr={self.learning_rate}, momentum={self.momentum}, "
                f"rounds={self.round_count})")
