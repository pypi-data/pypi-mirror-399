"""
FedProx èšåˆå™¨

ä»Ž methods/aggregators/fedprox.py è¿ç§»åˆ° src/

å®žçŽ° FedProx (Federated Optimization in Heterogeneous Networks) èšåˆç®—æ³•ã€‚
åœ¨FedAvgåŸºç¡€ä¸Šæ·»åŠ æ­£åˆ™åŒ–é¡¹,æ›´å¥½åœ°å¤„ç†å¼‚æž„æ•°æ®å’Œç³»ç»Ÿå¼‚æž„æ€§ã€‚

è®ºæ–‡: Federated Optimization in Heterogeneous Networks
ä½œè€…: Tian Li et al.
å‘è¡¨: MLSys 2020

ç®—æ³•ç‰¹ç‚¹:
1. åœ¨å®¢æˆ·ç«¯æœ¬åœ°ç›®æ ‡å‡½æ•°ä¸­æ·»åŠ è¿‘ç«¯é¡¹
2. çº¦æŸå®¢æˆ·ç«¯æ¨¡åž‹ä¸è¦åç¦»å…¨å±€æ¨¡åž‹å¤ªè¿œ
3. æ›´å¥½çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§
4. é€‚åˆæ•°æ®å¼‚æž„çš„è”é‚¦å­¦ä¹ åœºæ™¯
"""

import torch
from typing import List, Dict, Any, Optional
from loguru import logger

from ...core.aggregator import Aggregator
from ...core.types import ClientUpdate
from ...registry import aggregator


@aggregator(
    name='fedprox',
    description='FedProxå¸¦æ­£åˆ™åŒ–çš„è”é‚¦èšåˆå™¨',
    version='1.0'
)
class FedProxAggregator(Aggregator):
    """
    FedProx èšåˆå™¨å®žçŽ°

    åœ¨FedAvgåŸºç¡€ä¸Š,è€ƒè™‘è¿‘ç«¯é¡¹çš„å½±å“:
    å®¢æˆ·ç«¯æŸå¤±: L_k(w) + (Î¼/2)||w - w_t||Â²

    å…¶ä¸­:
    - L_k(w): å®¢æˆ·ç«¯kçš„åŽŸå§‹æŸå¤±å‡½æ•°
    - Î¼: è¿‘ç«¯é¡¹ç³»æ•°,æŽ§åˆ¶ä¸Žå…¨å±€æ¨¡åž‹çš„åç¦»ç¨‹åº¦
    - w_t: å½“å‰è½®æ¬¡çš„å…¨å±€æ¨¡åž‹å‚æ•°

    å‚æ•°:
    - mu: è¿‘ç«¯é¡¹ç³»æ•°,é»˜è®¤0.01
    - weighted: æ˜¯å¦æŒ‰æ ·æœ¬æ•°é‡åŠ æƒ,é»˜è®¤True
    - normalize_weights: æ˜¯å¦æ ‡å‡†åŒ–æƒé‡,é»˜è®¤True
    """

    def __init__(self, mu: float = 0.01, weighted: bool = True,
                 normalize_weights: bool = True, **kwargs):
        """åˆå§‹åŒ–FedProxèšåˆå™¨"""
        # FedProxç‰¹å®šå‚æ•°
        self.mu = mu  # è¿‘ç«¯é¡¹ç³»æ•°
        self._weighted = weighted
        self.normalize_weights = normalize_weights

        # è®¾å¤‡é…ç½®
        device = kwargs.get("device", "auto")
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device

        # èšåˆåŽ†å²
        self.round_count = 0
        self.convergence_history = []

        logger.info(f"âœ… FedProxèšåˆå™¨åˆå§‹åŒ–å®Œæˆ - Î¼: {self.mu}, åŠ æƒ: {self._weighted}")

    def aggregate(self, updates: List[ClientUpdate], global_model=None) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡ŒFedProxèšåˆ

        Args:
            updates: å®¢æˆ·ç«¯æ›´æ–°åˆ—è¡¨ (List[ClientUpdate])
            global_model: å…¨å±€æ¨¡åž‹ (å¯é€‰)

        Returns:
            èšåˆåŽçš„æ¨¡åž‹æƒé‡å­—å…¸
        """
        if not updates:
            raise ValueError("æ²¡æœ‰å®¢æˆ·ç«¯æ›´æ–°å¯èšåˆ")

        self.round_count += 1
        logger.debug(f"ðŸ”„ FedProxèšåˆè½®æ¬¡ {self.round_count} - {len(updates)} ä¸ªå®¢æˆ·ç«¯")

        # 1. è®¡ç®—èšåˆæƒé‡ (è€ƒè™‘è¿‘ç«¯é¡¹å½±å“)
        weights = self._compute_fedprox_weights(updates)

        # 2. æ‰§è¡ŒåŠ æƒèšåˆ
        aggregated_weights = self._fedprox_aggregation(updates, weights)

        # 3. è®¡ç®—æ”¶æ•›æŒ‡æ ‡
        convergence_metrics = self._compute_convergence_metrics(updates, aggregated_weights)
        self.convergence_history.append(convergence_metrics)

        logger.debug(f"âœ… FedProxèšåˆå®Œæˆ - æ”¶æ•›åº¦: {convergence_metrics.get('avg_divergence', 0):.6f}")

        return aggregated_weights

    def _compute_fedprox_weights(self, updates: List[ClientUpdate]) -> List[float]:
        """è®¡ç®—FedProxèšåˆæƒé‡"""
        if not self._weighted:
            num_clients = len(updates)
            return [1.0 / num_clients] * num_clients

        # åŸºç¡€æƒé‡: æŒ‰æ ·æœ¬æ•°é‡
        sample_counts = [update.num_samples for update in updates]
        total_samples = sum(sample_counts)

        if total_samples == 0:
            num_clients = len(updates)
            return [1.0 / num_clients] * num_clients

        base_weights = [count / total_samples for count in sample_counts]

        # FedProxè°ƒæ•´: è€ƒè™‘è¿‘ç«¯é¡¹çš„å½±å“
        # è¿‘ç«¯é¡¹å€¼è¶Šå°(è¶ŠæŽ¥è¿‘å…¨å±€æ¨¡åž‹),æƒé‡è¶Šå¤§
        if self.mu > 0 and hasattr(updates[0], 'metadata') and updates[0].metadata:
            adjusted_weights = []
            for i, update in enumerate(updates):
                proximal_term = update.metadata.get("proximal_term", 0.0) if update.metadata else 0.0
                # ä½¿ç”¨æŒ‡æ•°è¡°å‡è°ƒæ•´æƒé‡
                adjustment = torch.exp(torch.tensor(-self.mu * proximal_term))
                adjusted_weights.append(base_weights[i] * adjustment.item())

            # é‡æ–°æ ‡å‡†åŒ–
            if self.normalize_weights:
                total_weight = sum(adjusted_weights)
                if total_weight > 0:
                    adjusted_weights = [w / total_weight for w in adjusted_weights]

            return adjusted_weights

        return base_weights

    def _fedprox_aggregation(self, updates: List[ClientUpdate],
                            weights: List[float]) -> Dict[str, torch.Tensor]:
        """æ‰§è¡ŒFedProxèšåˆ"""
        aggregated_weights = {}

        # èŽ·å–æ¨¡åž‹ç»“æž„
        first_weights = updates[0].weights
        param_names = list(first_weights.keys())

        # åˆå§‹åŒ–èšåˆç»“æžœ
        for param_name in param_names:
            param_tensor = first_weights[param_name]
            if isinstance(param_tensor, torch.Tensor):
                # ä½¿ç”¨float32é¿å…Longç±»åž‹è½¬æ¢é”™è¯¯
                aggregated_weights[param_name] = torch.zeros_like(param_tensor, dtype=torch.float32, device=self.device)
            else:
                aggregated_weights[param_name] = 0.0

        # åŠ æƒèšåˆ
        for i, update in enumerate(updates):
            client_weights = update.weights
            weight = weights[i]

            for param_name in param_names:
                if param_name in client_weights:
                    param_value = client_weights[param_name]

                    if isinstance(param_value, torch.Tensor):
                        param_value = param_value.to(self.device)
                        # è½¬æ¢æ•´æ•°ç±»åž‹ä¸ºfloat
                        if param_value.dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                            param_value = param_value.float()
                        aggregated_weights[param_name] += weight * param_value
                    else:
                        aggregated_weights[param_name] += weight * param_value

        # è½¬æ¢å›žåŽŸå§‹ç±»åž‹
        for param_name in param_names:
            if isinstance(first_weights[param_name], torch.Tensor):
                if first_weights[param_name].dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                    aggregated_weights[param_name] = aggregated_weights[param_name].long()

        return aggregated_weights

    def _compute_convergence_metrics(self, updates: List[ClientUpdate],
                                   aggregated_weights: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """è®¡ç®—æ”¶æ•›æŒ‡æ ‡"""
        metrics = {}

        # è®¡ç®—å®¢æˆ·ç«¯æ¨¡åž‹ä¸Žèšåˆæ¨¡åž‹çš„å·®å¼‚
        divergences = []

        for update in updates:
            client_weights = update.weights
            divergence = 0.0
            param_count = 0

            for param_name in aggregated_weights.keys():
                if param_name in client_weights:
                    global_param = aggregated_weights[param_name]
                    client_param = client_weights[param_name]

                    if isinstance(global_param, torch.Tensor) and isinstance(client_param, torch.Tensor):
                        client_param = client_param.to(self.device)
                        # è½¬æ¢ä¸ºfloatä»¥ä¾¿è®¡ç®—norm
                        if global_param.dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                            global_param = global_param.float()
                        if client_param.dtype in [torch.long, torch.int, torch.int32, torch.int64]:
                            client_param = client_param.float()
                        diff = torch.norm(global_param - client_param).item()
                        divergence += diff
                        param_count += 1

            if param_count > 0:
                divergences.append(divergence / param_count)

        if divergences:
            metrics["avg_divergence"] = sum(divergences) / len(divergences)
            metrics["max_divergence"] = max(divergences)
            metrics["min_divergence"] = min(divergences)
            metrics["std_divergence"] = torch.std(torch.tensor(divergences)).item()

        # è¿‘ç«¯é¡¹ç»Ÿè®¡
        proximal_terms = []
        for update in updates:
            if hasattr(update, 'metadata') and update.metadata:
                proximal_terms.append(update.metadata.get("proximal_term", 0.0))

        if proximal_terms:
            metrics["avg_proximal_term"] = sum(proximal_terms) / len(proximal_terms)
            metrics["max_proximal_term"] = max(proximal_terms)

        return metrics

    def get_convergence_trend(self) -> Dict[str, List[float]]:
        """èŽ·å–æ”¶æ•›è¶‹åŠ¿"""
        if not self.convergence_history:
            return {}

        trends = {}
        metric_names = self.convergence_history[0].keys()

        for metric_name in metric_names:
            trends[metric_name] = [round_metrics.get(metric_name, 0.0)
                                 for round_metrics in self.convergence_history]

        return trends

    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–èšåˆå™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "algorithm": "FedProx",
            "mu": self.mu,
            "weighted": self._weighted,
            "total_rounds": self.round_count,
            "device": str(self.device)
        }

        # æ·»åŠ æ”¶æ•›ç»Ÿè®¡
        if self.convergence_history:
            latest_metrics = self.convergence_history[-1]
            stats["latest_convergence"] = latest_metrics

            # è®¡ç®—æ•´ä½“è¶‹åŠ¿
            divergences = [m.get("avg_divergence", 0) for m in self.convergence_history]
            if len(divergences) > 1:
                stats["convergence_trend"] = "improving" if divergences[-1] < divergences[0] else "degrading"

        return stats

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.round_count = 0
        self.convergence_history.clear()
        logger.info("ðŸ”„ FedProxèšåˆå™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

    def __repr__(self) -> str:
        return f"FedProxAggregator(mu={self.mu}, weighted={self._weighted}, rounds={self.round_count})"
