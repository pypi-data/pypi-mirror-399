# ==================== FedAdam: Adaptive Federated Optimization ====================

id: fedadam
name: "FedAdam: Adaptive Federated Optimization"
category: HFL
venue: "ICLR"
year: 2021
url: "https://arxiv.org/abs/2003.00295"
description: |
  将 Adam 优化器应用于联邦学习的服务器端聚合。
  服务器维护一阶和二阶动量估计，
  实现自适应学习率调整。

components:
  learner: default
  aggregator: fedadam
  trainer: default
  model: cifar10_cnn
  dataset: cifar10

defaults:
  experiment:
    num_clients: 10
    
  trainer:
    num_rounds: 100
    local_epochs: 5
    client_fraction: 0.1
    eval_interval: 10

  learner:
    learning_rate: 0.01
    batch_size: 32
    momentum: 0.9

  aggregator:
    server_lr: 0.01
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-8
    tau: 0.001

  model:
    num_classes: 10

  dataset:
    data_dir: ./data
    download: true
    partition:
      strategy: dirichlet
      alpha: 0.5

params:
  aggregator.server_lr:
    type: float
    range: [0.0001, 1.0]
    desc: "服务器端学习率"
    
  aggregator.beta1:
    type: float
    range: [0.0, 0.999]
    desc: "一阶动量系数"
    
  aggregator.beta2:
    type: float
    range: [0.0, 0.9999]
    desc: "二阶动量系数"
    
  aggregator.tau:
    type: float
    range: [0.0001, 0.1]
    desc: "适应性参数"

  learner.learning_rate:
    type: float
    range: [0.0001, 1.0]
    desc: "客户端学习率"

  trainer.num_rounds:
    type: int
    range: [1, 1000]
    desc: "联邦训练轮数"

citation: |
  @inproceedings{reddi2021adaptive,
    title={Adaptive federated optimization},
    author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
    booktitle={ICLR},
    year={2021}
  }

notes: |
  - 服务器端自适应优化，更新更稳定
  - 适合非凸优化问题
  - tau 控制适应性强度
  - 与 FedYogi 类似，但使用 Adam 更新规则
