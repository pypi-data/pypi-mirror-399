"""
Vidai vs OpenAI Benchmark Suite.

Measures:
1. Import Cost (Time & Memory)
2. Instantiation Cost (Time & Memory)
3. Request Overhead (Latency)

Run with: python examples/benchmark.py
"""

import time
import tracemalloc
import sys
import os
from contextlib import contextmanager
from typing import Callable, Tuple, Any

# Ensure we can import vidai
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

def measure_memory_variance(func: Callable) -> Tuple[Any, float, float]:
    """Measure peak memory usage of a function."""
    tracemalloc.start()
    start_current, start_peak = tracemalloc.get_traced_memory()
    
    start_time = time.perf_counter_ns()
    result = func()
    end_time = time.perf_counter_ns()
    
    end_current, end_peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    memory_diff_kb = (end_current - start_current) / 1024
    duration_ms = (end_time - start_time) / 1_000_000
    
    return result, duration_ms, memory_diff_kb

print("=" * 60)
print("üöÄ Vidai vs OpenAI Benchmark")
print("=" * 60)

# --- 1. Import Benchmarks ---
print("\n[1] Library Import Cost")

def import_openai():
    import openai
    return openai

def import_vidai():
    import vidai
    return vidai

# We verify module cache status first
if "openai" in sys.modules or "vidai" in sys.modules:
    print("‚ö†Ô∏è  API modules already loaded. Import benchmark skipped (run in fresh process).")
else:
    # Note: accurate import checks require separate processes usually, 
    # but we will do a rough check here if they aren't loaded.
    pass

# --- 2. Instantiation Benchmarks ---
import openai
import vidai
from vidai import Vidai

print("\n[2] Client Instantiation Cost")

def create_openai():
    return openai.OpenAI(api_key="sk-mock")

def create_vidai():
    return Vidai(api_key="sk-mock")

_, t_oa, m_oa = measure_memory_variance(create_openai)
print(f"OpenAI Client:     {t_oa:.4f} ms | {m_oa:.2f} KB")

# Warm up Vidai (imports lazy modules)
create_vidai() 
_, t_vidai, m_vidai = measure_memory_variance(create_vidai)
print(f"Vidai:        {t_vidai:.4f} ms | {m_vidai:.2f} KB")
print(f"Diff (Overhead):   {t_vidai - t_oa:.4f} ms | {m_vidai - m_oa:.2f} KB")


# --- 3. Request Overhead (Mocked) ---
print("\n[3] Request Processing Overhead (Pure Python / Mocked Network)")

# We inject a mock HTTP client to prevent any network calls
import httpx

class MockTransport(httpx.BaseTransport):
    def handle_request(self, request):
        return httpx.Response(
            200,
            json={
                "id": "chatcmpl-123",
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": '{"name": "Alice"}'},
                    "finish_reason": "stop"
                }],
                "created": 1234567890,
                "model": "gpt-4o",
                "object": "chat.completion"
            }
        )

# Create Client with Mock Transport
# This measures SDK overhead + HTTPX construction/serialization overhead
# but ZERO network I/O.
mock_http_client = httpx.Client(transport=MockTransport())

# Setup Clients with INJECTED http_client
oa_client = openai.OpenAI(api_key="sk-mock", http_client=mock_http_client)
w_client = Vidai(api_key="sk-mock", http_client=mock_http_client)


ITERATIONS = 1000

def run_openai_requests():
    for _ in range(ITERATIONS):
        oa_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "hi"}]
        )

# For Wizz, we test WITH and WITHOUT Structured Output to see cost
def run_vidai_requests_standard():
    for _ in range(ITERATIONS):
        w_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "hi"}]
        )

from pydantic import BaseModel
class User(BaseModel):
    name: str

def run_vidai_requests_structured():
    for _ in range(ITERATIONS):
        w_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "hi"}],
            response_format=User
        )

print(f"Running {ITERATIONS} iterations...")

start = time.perf_counter_ns()
run_openai_requests()
end = time.perf_counter_ns()
oa_avg = (end - start) / ITERATIONS / 1_000_000
print(f"OpenAI (Avg):           {oa_avg:.4f} ms")

start = time.perf_counter_ns()
run_vidai_requests_standard()
end = time.perf_counter_ns()
w_std_avg = (end - start) / ITERATIONS / 1_000_000
print(f"Vidai (Standard):  {w_std_avg:.4f} ms (Overhead: {w_std_avg - oa_avg:.4f} ms)")

start = time.perf_counter_ns()
run_vidai_requests_structured()
end = time.perf_counter_ns()
w_str_avg = (end - start) / ITERATIONS / 1_000_000
print(f"Vidai (Structured):{w_str_avg:.4f} ms (Overhead: {w_str_avg - oa_avg:.4f} ms)")

print("\n‚úÖ Benchmark Complete.")
