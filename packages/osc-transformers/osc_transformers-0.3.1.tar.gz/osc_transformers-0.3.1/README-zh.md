<div align="center">

# OSC-Transformers

**ğŸš€ åŸºäºé…ç½®æ–‡ä»¶çš„æ¨¡å—åŒ– Transformer æ¨¡å‹æ„å»ºæ¡†æ¶**

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.8%2B-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

*çµæ´»ã€é«˜æ•ˆã€å¯æ‰©å±•çš„ Transformer æ¨¡å‹æ„å»ºå·¥å…·*

</div>

## âœ¨ ç‰¹æ€§

- ğŸ”§ **é…ç½®é©±åŠ¨**: é€šè¿‡ç®€å•é…ç½®æ–‡ä»¶æ„å»º Transformer æ¨¡å‹
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**: æ”¯æŒè‡ªå®šä¹‰æ³¨å†Œå„ç±»ç»„ä»¶
- âš¡ **é«˜æ€§èƒ½**: æ”¯æŒ CUDA Graph å’Œ Paged Attention

## ğŸ› ï¸ æ”¯æŒç»„ä»¶

| ç»„ä»¶ç±»å‹ | å†…ç½®å®ç° |
|---------|---------|
| æ³¨æ„åŠ›æœºåˆ¶ | `PagedAttention` |
| å‰é¦ˆç½‘ç»œ | `SwiGLU` |
| å½’ä¸€åŒ– | `RMSNorm` |
| åµŒå…¥å±‚ | `VocabEmbedding` |
| è¾“å‡ºå¤´ | `LMHead` |

## ğŸ“¦ å®‰è£…

- å®‰è£…[æœ€æ–°ç‰ˆæœ¬pytorch](https://pytorch.org/)
- å®‰è£…[flash-attn](https://github.com/Dao-AILab/flash-attention): å»ºè®®ä¸‹è½½å®˜æ–¹æ„å»ºå¥½çš„whlåŒ…ï¼Œé¿å…ç¼–è¯‘é—®é¢˜
- å®‰è£…osc-transformers
```bash
pip install osc-transformers
```


## ğŸš€ å¿«é€Ÿå¼€å§‹


åˆ›å»º `model.cfg`(Qwen3-0.6B):
```toml
[model]
@architecture = "TransformerDecoder"
num_layers = 28
prenorm = "True"

[model.attention]
@attention = "PagedAttention"
in_dim = 1024
num_heads = 16
head_dim = 128
num_query_groups = 8
rope_base = 1000000
q_bias = "False"
k_bias = "False"
v_bias = "False"
o_bias = "False"

[model.attention.k_norm]
@normalization = "RMSNorm"
in_dim = 128
eps = 0.000001

[model.attention.q_norm]
@normalization = "RMSNorm"
in_dim = 128
eps = 0.000001

[model.embedding]
@embedding = "VocabEmbedding"
num_embeddings = 151936
embedding_dim = 1024

[model.feedforward]
@feedforward = "SwiGLU"
in_dim = 1024
hidden_dim = 3072
up_bias = "False"
gate_bias = "False"
down_bias = "False"

[model.head]
@head = "LMHead"
in_dim = 1024
out_dim = 151936
bias = "False"

[model.norm]
@normalization = "RMSNorm"
in_dim = 1024
eps = 0.000001
```
ä»£ç ç¤ºä¾‹ï¼š
```python
from osc_transformers import TransformerDecoder, Sequence, SamplingParams

# æ„å»ºæ¨¡å‹
model = TransformerDecoder.from_config("model.cfg")
model.setup(gpu_memory_utilization=0.9, max_model_len=40960, device="cuda:0")

# æ‰¹é‡æ¨ç†
seqs = [Sequence(token_ids=[1,2,3,4,5,6,7,8,9,10], sampling_params=SamplingParams(temperature=0.5, max_generate_tokens=1024))]
seqs = model.batch(seqs)

# æµå¼æ¨ç†
seq = Sequence(token_ids=[1,2,3,4,5,6,7,8,9,10], sampling_params=SamplingParams(temperature=0.5, max_generate_tokens=1024))
for token in model.stream(seq):
    pass

```

## ğŸ“š æ¨ç†æ€§èƒ½
```bash
osc-transformers bench examples/configs/qwen3-0_6B.cfg --num_seqs 64 --max_input_len 1024 --max_output_len 1024 --gpu_memory_utilization 0.9
```
| æ¶æ„ | æ¨¡å‹ |è®¾å¤‡ | ååé‡(tokens/s) |
|---------|---------|---------|---------|
| TransformerDecoder | Qwen3-0.6B | 4090 | 5400 |
| TransformerDecoder | Qwen3-0.6B | 3090 | 4000 |

## ğŸ“š è‡´è°¢

æœ¬é¡¹ç›®æ ¸å¿ƒä»£ç ä¸»è¦æ¥è‡ªäºä»¥ä¸‹é¡¹ç›®ï¼š

- [nano-vllm](https://github.com/GeeeekExplorer/nano-vllm)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [Liger-Kernel](https://github.com/linkedin/Liger-Kernel)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License