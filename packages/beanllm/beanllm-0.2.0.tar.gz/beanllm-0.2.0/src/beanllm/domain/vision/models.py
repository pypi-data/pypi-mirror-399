"""
Vision Models - 비전 태스크 모델 (2024-2025)

최신 비전 모델 래퍼:
- SAM (Segment Anything Model)
- Florence-2 (Microsoft)
- YOLO (Object Detection)

Requirements:
    pip install transformers torch pillow opencv-python ultralytics
"""

import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np

from .base_task_model import BaseVisionTaskModel

try:
    from ...utils.logger import get_logger
except ImportError:
    def get_logger(name: str):
        return logging.getLogger(name)


logger = get_logger(__name__)


class SAMWrapper(BaseVisionTaskModel):
    """
    Segment Anything Model (SAM) 래퍼 (2025년 최신)

    Meta AI의 SAM은 제로샷 이미지 segmentation 모델입니다.

    SAM 버전:
    - SAM 3 (2025년 11월): 텍스트 프롬프트, 컨셉 기반 분할, 3D 재구성
    - SAM 2: 비디오 segmentation 지원
    - SAM 1: 원본 (Point, Box, Mask prompt)

    SAM 3 주요 기능:
    - 텍스트 프롬프트로 객체 감지/분할/추적
    - 이미지/비디오에서 컨셉의 모든 인스턴스 찾기
    - 단일 이미지에서 3D 재구성 (SAM 3D)
    - 2x 성능 향상 (vs SAM 2)

    Example:
        ```python
        from beanllm.domain.vision import SAMWrapper

        # SAM 3 사용 (최신, 텍스트 프롬프트)
        sam = SAMWrapper(model_type="sam3_hiera_large")

        # 텍스트 프롬프트로 분할
        masks = sam.segment_by_text(
            image="photo.jpg",
            text_prompt="person wearing red shirt"
        )

        # SAM 2 사용 (비디오)
        sam = SAMWrapper(model_type="sam2_hiera_large")

        # 이미지에서 객체 분할
        masks = sam.segment(
            image="photo.jpg",
            points=[[500, 375]],  # 클릭 포인트
            labels=[1]  # 1=foreground, 0=background
        )

        # 모든 객체 자동 분할
        all_masks = sam.segment_everything("photo.jpg")
        ```

    References:
        - SAM 3: https://ai.meta.com/sam3/
        - GitHub: https://github.com/facebookresearch/sam3
        - Paper: https://about.fb.com/news/2025/11/new-sam-models-detect-objects-create-3d-reconstructions/
    """

    def __init__(
        self,
        model_type: str = "sam3_hiera_large",
        device: Optional[str] = None,
        **kwargs,
    ):
        """
        Args:
            model_type: SAM 모델 타입
                - "sam3_hiera_large": SAM 3 Large (최신, 권장, 텍스트 프롬프트)
                - "sam3_hiera_base": SAM 3 Base
                - "sam3_hiera_small": SAM 3 Small
                - "sam2_hiera_large": SAM 2 Large (비디오)
                - "sam2_hiera_base_plus": SAM 2 Base+
                - "sam2_hiera_small": SAM 2 Small
                - "sam2_hiera_tiny": SAM 2 Tiny
                - "sam_vit_h": SAM ViT-H (원본)
                - "sam_vit_l": SAM ViT-L
                - "sam_vit_b": SAM ViT-B
            device: 디바이스 (cuda/cpu/mps)
            **kwargs: 추가 설정
        """
        self.model_type = model_type
        self.kwargs = kwargs

        # Device 설정
        if device is None:
            import torch
            if torch.cuda.is_available():
                self.device = "cuda"
            elif torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device

        # Lazy loading
        self._model = None
        self._predictor = None

    def _load_model(self):
        """모델 로딩 (lazy loading)"""
        if self._model is not None:
            return

        try:
            if self.model_type.startswith("sam3"):
                # SAM 3 (최신)
                from sam3.build_sam import build_sam3
                from sam3.sam3_predictor import SAM3Predictor

                checkpoint = self._get_sam3_checkpoint()
                config = self._get_sam3_config()

                self._model = build_sam3(config, checkpoint, device=self.device)
                self._predictor = SAM3Predictor(self._model)

            elif self.model_type.startswith("sam2"):
                # SAM 2
                from sam2.build_sam import build_sam2
                from sam2.sam2_image_predictor import SAM2ImagePredictor

                checkpoint = self._get_sam2_checkpoint()
                config = self._get_sam2_config()

                self._model = build_sam2(config, checkpoint, device=self.device)
                self._predictor = SAM2ImagePredictor(self._model)
            else:
                # SAM (원본)
                from segment_anything import sam_model_registry, SamPredictor

                checkpoint = self._get_sam_checkpoint()
                self._model = sam_model_registry[self.model_type](checkpoint=checkpoint)
                self._model.to(device=self.device)
                self._predictor = SamPredictor(self._model)

            logger.info(f"SAM model loaded: {self.model_type} on {self.device}")

        except ImportError:
            raise ImportError(
                "segment-anything, sam2, or sam3 required. "
                "Install with: pip install git+https://github.com/facebookresearch/segment-anything.git "
                "or pip install git+https://github.com/facebookresearch/sam2.git "
                "or pip install git+https://github.com/facebookresearch/sam3.git"
            )

    def _get_sam3_checkpoint(self) -> str:
        """SAM 3 체크포인트 경로"""
        checkpoint_map = {
            "sam3_hiera_large": "checkpoints/sam3_hiera_large.pt",
            "sam3_hiera_base": "checkpoints/sam3_hiera_base.pt",
            "sam3_hiera_small": "checkpoints/sam3_hiera_small.pt",
        }
        return checkpoint_map.get(self.model_type, checkpoint_map["sam3_hiera_large"])

    def _get_sam3_config(self) -> str:
        """SAM 3 config 경로"""
        config_map = {
            "sam3_hiera_large": "sam3_hiera_l.yaml",
            "sam3_hiera_base": "sam3_hiera_b.yaml",
            "sam3_hiera_small": "sam3_hiera_s.yaml",
        }
        return config_map.get(self.model_type, config_map["sam3_hiera_large"])

    def _get_sam2_checkpoint(self) -> str:
        """SAM 2 체크포인트 경로"""
        checkpoint_map = {
            "sam2_hiera_large": "checkpoints/sam2_hiera_large.pt",
            "sam2_hiera_base_plus": "checkpoints/sam2_hiera_base_plus.pt",
            "sam2_hiera_small": "checkpoints/sam2_hiera_small.pt",
            "sam2_hiera_tiny": "checkpoints/sam2_hiera_tiny.pt",
        }
        return checkpoint_map.get(self.model_type, checkpoint_map["sam2_hiera_large"])

    def _get_sam2_config(self) -> str:
        """SAM 2 config 경로"""
        config_map = {
            "sam2_hiera_large": "sam2_hiera_l.yaml",
            "sam2_hiera_base_plus": "sam2_hiera_b+.yaml",
            "sam2_hiera_small": "sam2_hiera_s.yaml",
            "sam2_hiera_tiny": "sam2_hiera_t.yaml",
        }
        return config_map.get(self.model_type, config_map["sam2_hiera_large"])

    def _get_sam_checkpoint(self) -> str:
        """SAM 체크포인트 경로"""
        checkpoint_map = {
            "sam_vit_h": "checkpoints/sam_vit_h_4b8939.pth",
            "sam_vit_l": "checkpoints/sam_vit_l_0b3195.pth",
            "sam_vit_b": "checkpoints/sam_vit_b_01ec64.pth",
        }
        return checkpoint_map.get(self.model_type, checkpoint_map["sam_vit_h"])

    def segment(
        self,
        image: Union[str, Path, np.ndarray],
        points: Optional[List[List[int]]] = None,
        labels: Optional[List[int]] = None,
        boxes: Optional[List[List[int]]] = None,
        multimask_output: bool = True,
    ) -> Dict[str, Any]:
        """
        이미지 segmentation

        Args:
            image: 이미지 (경로 또는 numpy array)
            points: 포인트 프롬프트 [[x, y], ...]
            labels: 포인트 레이블 [1=foreground, 0=background]
            boxes: 박스 프롬프트 [[x1, y1, x2, y2], ...]
            multimask_output: 여러 마스크 출력 여부

        Returns:
            {"masks": np.ndarray, "scores": List[float], "logits": np.ndarray}
        """
        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image_pil = Image.open(image).convert("RGB")
            image = np.array(image_pil)

        # 이미지 설정
        self._predictor.set_image(image)

        # Prompt 설정
        point_coords = np.array(points) if points else None
        point_labels = np.array(labels) if labels else None
        box_coords = np.array(boxes) if boxes else None

        # 예측
        masks, scores, logits = self._predictor.predict(
            point_coords=point_coords,
            point_labels=point_labels,
            box=box_coords[0] if box_coords is not None and len(box_coords) == 1 else None,
            multimask_output=multimask_output,
        )

        return {
            "masks": masks,
            "scores": scores.tolist(),
            "logits": logits,
        }

    def segment_everything(
        self,
        image: Union[str, Path, np.ndarray],
    ) -> List[Dict[str, Any]]:
        """
        자동으로 모든 객체 분할

        Args:
            image: 이미지

        Returns:
            [{"segmentation": mask, "area": int, "bbox": [x, y, w, h], "predicted_iou": float}, ...]
        """
        self._load_model()

        from segment_anything import SamAutomaticMaskGenerator

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image_pil = Image.open(image).convert("RGB")
            image = np.array(image_pil)

        # Mask generator
        mask_generator = SamAutomaticMaskGenerator(self._model)

        # 예측
        masks = mask_generator.generate(image)

        logger.info(f"SAM generated {len(masks)} masks")

        return masks

    def segment_by_text(
        self,
        image: Union[str, Path, np.ndarray],
        text_prompt: str,
        confidence_threshold: float = 0.5,
    ) -> Dict[str, Any]:
        """
        텍스트 프롬프트로 객체 분할 (SAM 3 only)

        SAM 3의 새로운 기능으로, 텍스트 설명으로 객체를 찾고 분할합니다.

        Args:
            image: 이미지 (경로 또는 numpy array)
            text_prompt: 텍스트 프롬프트 (예: "person wearing red shirt", "all cars")
            confidence_threshold: 신뢰도 임계값 (기본: 0.5)

        Returns:
            {
                "masks": np.ndarray,  # Shape: (N, H, W)
                "boxes": List[List[int]],  # [[x1, y1, x2, y2], ...]
                "scores": List[float],  # Confidence scores
                "labels": List[str],  # Text labels
            }

        Example:
            ```python
            sam = SAMWrapper(model_type="sam3_hiera_large")

            # 특정 객체 찾기
            result = sam.segment_by_text(
                image="photo.jpg",
                text_prompt="person wearing red shirt"
            )

            # 모든 인스턴스 찾기
            result = sam.segment_by_text(
                image="photo.jpg",
                text_prompt="all dogs"
            )
            ```
        """
        if not self.model_type.startswith("sam3"):
            raise ValueError(
                f"Text prompting is only supported in SAM 3. "
                f"Current model: {self.model_type}. "
                f"Please use model_type='sam3_hiera_large' or similar."
            )

        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image_pil = Image.open(image).convert("RGB")
            image = np.array(image_pil)

        # SAM 3 텍스트 기반 예측
        # Note: 실제 SAM 3 API에 따라 조정 필요
        try:
            # SAM 3의 텍스트 프롬프트 API 사용
            predictions = self._predictor.predict_with_text(
                image=image,
                text_prompt=text_prompt,
                confidence_threshold=confidence_threshold,
            )

            logger.info(
                f"SAM 3 text prediction completed: "
                f"prompt='{text_prompt}', found={len(predictions['masks'])} objects"
            )

            return predictions

        except AttributeError:
            # Fallback: SAM 3 API가 다를 경우
            logger.warning(
                "SAM 3 text prompt API not available. "
                "Using automatic masking with text filtering."
            )

            # 대안: 자동 마스크 생성 후 필터링
            all_masks = self.segment_everything(image)

            # TODO: 텍스트 필터링 로직 추가 (CLIP 등 사용)
            # 현재는 모든 마스크 반환
            return {
                "masks": np.array([m["segmentation"] for m in all_masks]),
                "boxes": [m["bbox"] for m in all_masks],
                "scores": [m.get("predicted_iou", 0.0) for m in all_masks],
                "labels": [text_prompt] * len(all_masks),
            }

    # BaseVisionTaskModel 추상 메서드 구현

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        points: Optional[List[List[int]]] = None,
        labels: Optional[List[int]] = None,
        boxes: Optional[List[List[int]]] = None,
        multimask_output: bool = True,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        예측 실행 (BaseVisionTaskModel 인터페이스)

        기본적으로 segment() 메서드를 호출합니다.

        Args:
            image: 이미지
            points: 포인트 프롬프트 (optional)
            labels: 포인트 레이블 (optional)
            boxes: 박스 프롬프트 (optional)
            multimask_output: 여러 마스크 출력 여부
            **kwargs: 추가 파라미터

        Returns:
            {"masks": np.ndarray, "scores": List[float], "logits": np.ndarray}
        """
        return self.segment(
            image=image,
            points=points,
            labels=labels,
            boxes=boxes,
            multimask_output=multimask_output,
        )

    def __repr__(self) -> str:
        return f"SAMWrapper(model_type={self.model_type}, device={self.device})"


class Florence2Wrapper(BaseVisionTaskModel):
    """
    Florence-2 모델 래퍼 (Microsoft)

    Microsoft의 Florence-2는 통합 비전-언어 모델입니다.

    Florence-2 특징:
    - Vision-Language 통합 모델
    - Object Detection, Segmentation, Captioning, VQA 통합
    - 0.2B/0.7B 파라미터 옵션
    - 오픈소스 (MIT License)

    Example:
        ```python
        from beanllm.domain.vision import Florence2Wrapper

        # Florence-2 모델 로드
        florence = Florence2Wrapper(model_size="large")

        # Image Captioning
        caption = florence.caption("image.jpg")
        print(caption)  # "A cat sitting on a couch"

        # Object Detection
        objects = florence.detect_objects("image.jpg")
        print(objects)  # [{"label": "cat", "box": [x1, y1, x2, y2], "score": 0.95}]

        # Visual Question Answering
        answer = florence.vqa("image.jpg", "What is the cat doing?")
        print(answer)  # "sitting"
        ```
    """

    def __init__(
        self,
        model_size: str = "large",
        device: Optional[str] = None,
        **kwargs,
    ):
        """
        Args:
            model_size: 모델 크기 (base/large)
                - "base": Florence-2-base (0.2B)
                - "large": Florence-2-large (0.7B)
            device: 디바이스
            **kwargs: 추가 설정
        """
        self.model_size = model_size
        self.kwargs = kwargs

        # Device 설정
        if device is None:
            import torch
            if torch.cuda.is_available():
                self.device = "cuda"
            else:
                self.device = "cpu"
        else:
            self.device = device

        # Lazy loading
        self._model = None
        self._processor = None

    def _load_model(self):
        """모델 로딩 (lazy loading)"""
        if self._model is not None:
            return

        try:
            from transformers import AutoModelForCausalLM, AutoProcessor
            import torch

            model_map = {
                "base": "microsoft/Florence-2-base",
                "large": "microsoft/Florence-2-large",
            }
            model_name = model_map.get(self.model_size, model_map["large"])

            logger.info(f"Loading Florence-2: {model_name}")

            self._model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True,
            ).to(self.device)

            self._processor = AutoProcessor.from_pretrained(
                model_name,
                trust_remote_code=True
            )

            logger.info("Florence-2 loaded successfully")

        except ImportError:
            raise ImportError("transformers required. Install with: pip install transformers")

    def _run_task(
        self,
        task: str,
        image: Union[str, Path, np.ndarray],
        text_input: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Florence-2 태스크 실행

        Args:
            task: 태스크 이름 (e.g., "<CAPTION>", "<DETAILED_CAPTION>")
            image: 이미지
            text_input: 추가 텍스트 입력

        Returns:
            결과 딕셔너리
        """
        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image = Image.open(image).convert("RGB")

        # 입력 준비
        if text_input:
            prompt = f"{task} {text_input}"
        else:
            prompt = task

        inputs = self._processor(text=prompt, images=image, return_tensors="pt").to(self.device)

        # 추론
        generated_ids = self._model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=1024,
            num_beams=3,
        )

        # 디코드
        generated_text = self._processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

        # 파싱
        parsed = self._processor.post_process_generation(
            generated_text,
            task=task,
            image_size=(image.width, image.height)
        )

        return parsed

    def caption(
        self,
        image: Union[str, Path, np.ndarray],
        detailed: bool = False,
    ) -> str:
        """
        Image captioning

        Args:
            image: 이미지
            detailed: 상세 캡션 생성 여부

        Returns:
            캡션 텍스트
        """
        task = "<MORE_DETAILED_CAPTION>" if detailed else "<CAPTION>"
        result = self._run_task(task, image)
        return result.get(task, "")

    def detect_objects(
        self,
        image: Union[str, Path, np.ndarray],
    ) -> List[Dict[str, Any]]:
        """
        Object detection

        Args:
            image: 이미지

        Returns:
            [{"label": str, "box": [x1, y1, x2, y2], "score": float}, ...]
        """
        result = self._run_task("<OD>", image)
        return result.get("<OD>", {}).get("bboxes", [])

    def vqa(
        self,
        image: Union[str, Path, np.ndarray],
        question: str,
    ) -> str:
        """
        Visual Question Answering

        Args:
            image: 이미지
            question: 질문

        Returns:
            답변
        """
        result = self._run_task("<VQA>", image, text_input=question)
        return result.get("<VQA>", "")

    # BaseVisionTaskModel 추상 메서드 구현

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        task: str = "caption",
        **kwargs,
    ) -> Union[str, List[Dict[str, Any]]]:
        """
        예측 실행 (BaseVisionTaskModel 인터페이스)

        Args:
            image: 이미지
            task: 태스크 종류 (caption/detect/vqa)
            **kwargs: 태스크별 추가 파라미터
                - caption: detailed=False
                - vqa: question (필수)

        Returns:
            태스크별 결과
            - caption: str
            - detect: List[Dict]
            - vqa: str

        Example:
            ```python
            # Caption
            caption = model.predict(image="photo.jpg", task="caption")

            # Object detection
            objects = model.predict(image="photo.jpg", task="detect")

            # VQA
            answer = model.predict(
                image="photo.jpg",
                task="vqa",
                question="What is this?"
            )
            ```
        """
        if task == "caption":
            return self.caption(image, **kwargs)
        elif task == "detect":
            return self.detect_objects(image)
        elif task == "vqa":
            if "question" not in kwargs:
                raise ValueError("VQA task requires 'question' parameter")
            return self.vqa(image, kwargs["question"])
        else:
            raise ValueError(
                f"Unknown task: {task}. "
                f"Available: caption, detect, vqa"
            )

    def __repr__(self) -> str:
        return f"Florence2Wrapper(model_size={self.model_size}, device={self.device})"


class YOLOWrapper(BaseVisionTaskModel):
    """
    YOLO (You Only Look Once) 래퍼 (2025년 최신)

    Ultralytics의 YOLO object detection 모델.

    YOLO 버전:
    - YOLOv12 (2025년 2월): Attention-centric architecture, 40.6% mAP
    - YOLOv11 (2024): Improved efficiency
    - YOLOv10: Dual label assignment
    - YOLOv8: Baseline

    YOLO 특징:
    - 실시간 object detection
    - Detection, Segmentation, Pose, Classification 지원
    - 다양한 모델 크기 (n/s/m/l/x)

    YOLOv12 주요 개선:
    - 2.1%/1.2% mAP 향상 (vs v10/v11)
    - Attention-centric architecture
    - 더욱 빠른 추론 속도
    - 40.6% mAP on COCO val2017

    Example:
        ```python
        from beanllm.domain.vision import YOLOWrapper

        # YOLOv12 사용 (최신, 권장)
        yolo = YOLOWrapper(version="12", model_size="m")

        # Object detection
        results = yolo.detect("image.jpg")
        for obj in results:
            print(f"{obj['class']}: {obj['confidence']:.2f}, box: {obj['box']}")

        # Segmentation
        yolo = YOLOWrapper(version="12", task="segment")
        results = yolo.segment("image.jpg")
        ```

    References:
        - YOLOv12: NeurIPS 2025
        - GitHub: https://github.com/ultralytics/ultralytics
    """

    def __init__(
        self,
        version: str = "12",
        model_size: str = "m",
        task: str = "detect",
        **kwargs,
    ):
        """
        Args:
            version: YOLO 버전
                - "12": YOLOv12 (최신, 권장, 2025년 2월)
                - "11": YOLOv11 (2024)
                - "10": YOLOv10
                - "8": YOLOv8
            model_size: 모델 크기 (n/s/m/l/x)
                - n: Nano (가장 빠름)
                - s: Small
                - m: Medium (균형, 권장)
                - l: Large
                - x: XLarge (가장 정확)
            task: 태스크 (detect/segment/pose/classify)
            **kwargs: 추가 설정
        """
        self.version = version
        self.model_size = model_size
        self.task = task
        self.kwargs = kwargs

        # Lazy loading
        self._model = None

    def _load_model(self):
        """모델 로딩 (lazy loading)"""
        if self._model is not None:
            return

        try:
            from ultralytics import YOLO

            # 모델 이름 생성
            model_name = f"yolo{self.version}{self.model_size}"
            if self.task != "detect":
                model_name += f"-{self.task}"
            model_name += ".pt"

            logger.info(f"Loading YOLO: {model_name}")

            self._model = YOLO(model_name)

            logger.info("YOLO loaded successfully")

        except ImportError:
            raise ImportError("ultralytics required. Install with: pip install ultralytics")

    def detect(
        self,
        image: Union[str, Path, np.ndarray],
        conf: float = 0.25,
        iou: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """
        Object detection

        Args:
            image: 이미지
            conf: 신뢰도 임계값
            iou: IoU 임계값

        Returns:
            [{"class": str, "confidence": float, "box": [x1, y1, x2, y2]}, ...]
        """
        self._load_model()

        # 추론
        results = self._model(image, conf=conf, iou=iou)

        # 결과 파싱
        detections = []
        for result in results:
            for box in result.boxes:
                detections.append({
                    "class": result.names[int(box.cls)],
                    "confidence": float(box.conf),
                    "box": box.xyxy[0].tolist(),  # [x1, y1, x2, y2]
                })

        logger.info(f"YOLO detected {len(detections)} objects")

        return detections

    def segment(
        self,
        image: Union[str, Path, np.ndarray],
        conf: float = 0.25,
        iou: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """
        Instance segmentation

        Args:
            image: 이미지
            conf: 신뢰도 임계값
            iou: IoU 임계값

        Returns:
            [{"class": str, "confidence": float, "box": [...], "mask": np.ndarray}, ...]
        """
        if self.task != "segment":
            logger.warning("YOLOWrapper task is not 'segment'. Switching to segment.")
            self.task = "segment"
            self._model = None  # 모델 재로드

        self._load_model()

        # 추론
        results = self._model(image, conf=conf, iou=iou)

        # 결과 파싱
        segments = []
        for result in results:
            if result.masks is None:
                continue

            for i, (box, mask) in enumerate(zip(result.boxes, result.masks)):
                segments.append({
                    "class": result.names[int(box.cls)],
                    "confidence": float(box.conf),
                    "box": box.xyxy[0].tolist(),
                    "mask": mask.data.cpu().numpy(),
                })

        logger.info(f"YOLO segmented {len(segments)} objects")

        return segments

    # BaseVisionTaskModel 추상 메서드 구현

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        conf: float = 0.25,
        iou: float = 0.7,
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """
        예측 실행 (BaseVisionTaskModel 인터페이스)

        태스크에 따라 detect() 또는 segment()를 호출합니다.

        Args:
            image: 이미지
            conf: 신뢰도 임계값
            iou: IoU 임계값
            **kwargs: 추가 파라미터

        Returns:
            Detection 또는 Segmentation 결과

        Example:
            ```python
            # Detection
            detections = model.predict("photo.jpg", conf=0.5)

            # Segmentation (task="segment"로 초기화된 경우)
            segments = model.predict("photo.jpg", conf=0.5)
            ```
        """
        if self.task == "segment":
            return self.segment(image=image, conf=conf, iou=iou)
        else:
            # detect가 기본
            return self.detect(image=image, conf=conf, iou=iou)

    def __repr__(self) -> str:
        return f"YOLOWrapper(version={self.version}, size={self.model_size}, task={self.task})"


class Qwen3VLWrapper(BaseVisionTaskModel):
    """
    Qwen3-VL - Alibaba의 최신 Vision-Language Model (2025년)

    Qwen3-VL 특징:
    - 멀티모달 이해 (이미지 + 텍스트)
    - Visual Question Answering (VQA)
    - Image Captioning
    - OCR (광학 문자 인식)
    - 다국어 지원 (영어, 중국어, 일본어, 한국어 등)

    지원 모델:
    - Qwen/Qwen3-VL: 메인 모델
    - Qwen/Qwen3-VL-Chat: 대화형 모델

    Example:
        ```python
        from beanllm.domain.vision import Qwen3VLWrapper

        # Qwen3-VL 초기화
        model = Qwen3VLWrapper(model_size="7B")

        # 이미지 질문 응답 (VQA)
        answer = model.answer_question(
            image="photo.jpg",
            question="What is in this image?"
        )

        # 이미지 캡셔닝
        caption = model.generate_caption(image="photo.jpg")
        ```

    References:
        - https://huggingface.co/Qwen/Qwen3-VL
        - https://qwenlm.github.io/
    """

    def __init__(
        self,
        model_size: str = "7B",
        device: Optional[str] = None,
        **kwargs,
    ):
        """
        Args:
            model_size: 모델 크기 (7B, 14B 등)
            device: 디바이스 (cuda/cpu)
            **kwargs: 추가 파라미터
        """
        self.model_size = model_size
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.kwargs = kwargs

        # Lazy loading
        self._model = None
        self._processor = None

    def _load_model(self):
        """모델 로딩 (lazy loading)"""
        if self._model is not None:
            return

        try:
            from transformers import AutoModelForCausalLM, AutoProcessor
        except ImportError:
            raise ImportError(
                "transformers required for Qwen3-VL. "
                "Install with: pip install transformers"
            )

        model_name = f"Qwen/Qwen3-VL-{self.model_size}"

        logger.info(f"Loading Qwen3-VL: {model_name} on {self.device}")

        self._processor = AutoProcessor.from_pretrained(model_name)
        self._model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map=self.device,
        )

        logger.info("Qwen3-VL loaded successfully")

    def answer_question(
        self,
        image: Union[str, Path, np.ndarray],
        question: str,
        max_tokens: int = 512,
    ) -> str:
        """
        Visual Question Answering (VQA)

        Args:
            image: 이미지
            question: 질문
            max_tokens: 최대 생성 토큰 수

        Returns:
            답변 텍스트
        """
        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image = Image.open(image).convert("RGB")

        # 프롬프트 생성
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": question},
                ],
            }
        ]

        # 입력 준비
        text = self._processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = self._processor(
            text=[text], images=[image], return_tensors="pt"
        ).to(self.device)

        # 생성
        generated_ids = self._model.generate(**inputs, max_new_tokens=max_tokens)
        output = self._processor.batch_decode(
            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        logger.info(f"Qwen3-VL VQA: question={question[:30]}...")

        return output

    def generate_caption(
        self,
        image: Union[str, Path, np.ndarray],
        max_tokens: int = 128,
    ) -> str:
        """
        이미지 캡셔닝

        Args:
            image: 이미지
            max_tokens: 최대 생성 토큰 수

        Returns:
            캡션 텍스트
        """
        return self.answer_question(
            image=image,
            question="Describe this image in detail.",
            max_tokens=max_tokens,
        )

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        prompt: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        예측 실행 (BaseVisionTaskModel 인터페이스)

        Args:
            image: 이미지
            prompt: 프롬프트 (None이면 캡셔닝)
            **kwargs: 추가 파라미터

        Returns:
            예측 결과
        """
        if prompt:
            answer = self.answer_question(image=image, question=prompt, **kwargs)
            return {"answer": answer, "prompt": prompt}
        else:
            caption = self.generate_caption(image=image, **kwargs)
            return {"caption": caption}

    def __repr__(self) -> str:
        return f"Qwen3VLWrapper(model_size={self.model_size}, device={self.device})"


class EVACLIPWrapper(BaseVisionTaskModel):
    """
    EVA-CLIP - 향상된 Vision-Language 표현 학습 (2024-2025)

    EVA-CLIP 특징:
    - CLIP의 개선 버전
    - 더 나은 zero-shot 성능
    - 대규모 이미지-텍스트 매칭
    - 1B+ 파라미터 모델

    Example:
        ```python
        from beanllm.domain.vision import EVACLIPWrapper

        # EVA-CLIP 초기화
        model = EVACLIPWrapper()

        # 이미지-텍스트 유사도
        similarity = model.compute_similarity(
            image="photo.jpg",
            texts=["a dog", "a cat", "a car"]
        )

        # Zero-shot 분류
        label = model.classify_zero_shot(
            image="photo.jpg",
            labels=["dog", "cat", "car"]
        )
        ```

    References:
        - https://github.com/baaivision/EVA/tree/master/EVA-CLIP
    """

    def __init__(
        self,
        model_name: str = "EVA02-CLIP-L-14-336",
        device: Optional[str] = None,
        **kwargs,
    ):
        """
        Args:
            model_name: EVA-CLIP 모델 이름
            device: 디바이스 (cuda/cpu)
            **kwargs: 추가 파라미터
        """
        self.model_name = model_name
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.kwargs = kwargs

        # Lazy loading
        self._model = None
        self._processor = None

    def _load_model(self):
        """모델 로딩 (lazy loading)"""
        if self._model is not None:
            return

        try:
            from transformers import AutoModel, AutoProcessor
        except ImportError:
            raise ImportError(
                "transformers required. Install with: pip install transformers"
            )

        logger.info(f"Loading EVA-CLIP: {self.model_name} on {self.device}")

        self._processor = AutoProcessor.from_pretrained(f"BAAI/{self.model_name}")
        self._model = AutoModel.from_pretrained(f"BAAI/{self.model_name}")
        self._model.to(self.device)
        self._model.eval()

        logger.info("EVA-CLIP loaded successfully")

    def compute_similarity(
        self,
        image: Union[str, Path, np.ndarray],
        texts: List[str],
    ) -> List[float]:
        """
        이미지-텍스트 유사도 계산

        Args:
            image: 이미지
            texts: 텍스트 리스트

        Returns:
            유사도 점수 리스트
        """
        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image = Image.open(image).convert("RGB")

        # 입력 처리
        inputs = self._processor(text=texts, images=image, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # 추론
        with torch.no_grad():
            outputs = self._model(**inputs)
            image_embeds = outputs.image_embeds
            text_embeds = outputs.text_embeds

            # 유사도 계산
            image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
            similarity = (image_embeds @ text_embeds.T).squeeze(0)

        logger.info(f"EVA-CLIP similarity computed for {len(texts)} texts")

        return similarity.cpu().tolist()

    def classify_zero_shot(
        self,
        image: Union[str, Path, np.ndarray],
        labels: List[str],
    ) -> Dict[str, Any]:
        """
        Zero-shot 분류

        Args:
            image: 이미지
            labels: 분류 레이블 리스트

        Returns:
            분류 결과
        """
        similarities = self.compute_similarity(image=image, texts=labels)

        # 가장 높은 유사도 찾기
        max_idx = similarities.index(max(similarities))

        return {
            "label": labels[max_idx],
            "confidence": similarities[max_idx],
            "all_scores": dict(zip(labels, similarities)),
        }

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        texts: Optional[List[str]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        예측 실행 (BaseVisionTaskModel 인터페이스)

        Args:
            image: 이미지
            texts: 텍스트 리스트
            **kwargs: 추가 파라미터

        Returns:
            예측 결과
        """
        if texts:
            similarities = self.compute_similarity(image=image, texts=texts)
            return {"similarities": dict(zip(texts, similarities))}
        else:
            return {"error": "Please provide texts for similarity computation"}

    def __repr__(self) -> str:
        return f"EVACLIPWrapper(model={self.model_name}, device={self.device})"


class DINOv2Wrapper(BaseVisionTaskModel):
    """
    DINOv2 - Self-supervised Vision Transformer (2024-2025)

    DINOv2 특징:
    - Self-supervised learning (라벨 없이 학습)
    - 강력한 visual features
    - Zero-shot 분류, 검색, 세그멘테이션
    - ViT 기반 아키텍처

    Example:
        ```python
        from beanllm.domain.vision import DINOv2Wrapper

        # DINOv2 초기화
        model = DINOv2Wrapper(model_size="large")

        # 이미지 임베딩 추출
        embedding = model.extract_features("photo.jpg")

        # 두 이미지 간 유사도
        sim = model.compute_image_similarity("img1.jpg", "img2.jpg")
        ```

    References:
        - https://github.com/facebookresearch/dinov2
        - https://arxiv.org/abs/2304.07193
    """

    def __init__(
        self,
        model_size: str = "large",
        device: Optional[str] = None,
        **kwargs,
    ):
        """
        Args:
            model_size: 모델 크기 (small, base, large, giant)
            device: 디바이스 (cuda/cpu)
            **kwargs: 추가 파라미터
        """
        self.model_size = model_size
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.kwargs = kwargs

        # Lazy loading
        self._model = None
        self._transform = None

    def _load_model(self):
        """모델 로딩 (lazy loading)"""
        if self._model is not None:
            return

        try:
            from transformers import AutoImageProcessor, AutoModel
        except ImportError:
            raise ImportError(
                "transformers required. Install with: pip install transformers"
            )

        model_name = f"facebook/dinov2-{self.model_size}"

        logger.info(f"Loading DINOv2: {model_name} on {self.device}")

        self._transform = AutoImageProcessor.from_pretrained(model_name)
        self._model = AutoModel.from_pretrained(model_name)
        self._model.to(self.device)
        self._model.eval()

        logger.info("DINOv2 loaded successfully")

    def extract_features(
        self,
        image: Union[str, Path, np.ndarray],
    ) -> np.ndarray:
        """
        이미지 특징 추출

        Args:
            image: 이미지

        Returns:
            특징 벡터 (numpy array)
        """
        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image = Image.open(image).convert("RGB")

        # 전처리
        inputs = self._transform(images=image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # 추론
        with torch.no_grad():
            outputs = self._model(**inputs)
            features = outputs.last_hidden_state[:, 0]  # CLS token

        logger.info(f"DINOv2 features extracted: shape={features.shape}")

        return features.cpu().numpy()[0]

    def compute_image_similarity(
        self,
        image1: Union[str, Path, np.ndarray],
        image2: Union[str, Path, np.ndarray],
    ) -> float:
        """
        두 이미지 간 유사도 계산

        Args:
            image1: 첫 번째 이미지
            image2: 두 번째 이미지

        Returns:
            코사인 유사도 (0-1)
        """
        feat1 = self.extract_features(image1)
        feat2 = self.extract_features(image2)

        # 코사인 유사도
        similarity = np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))

        return float(similarity)

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        **kwargs,
    ) -> Dict[str, Any]:
        """
        예측 실행 (BaseVisionTaskModel 인터페이스)

        Args:
            image: 이미지
            **kwargs: 추가 파라미터

        Returns:
            예측 결과
        """
        features = self.extract_features(image)

        return {
            "features": features.tolist(),
            "feature_dim": len(features),
        }

    def __repr__(self) -> str:
        return f"DINOv2Wrapper(model_size={self.model_size}, device={self.device})"


class Qwen3VLWrapper(BaseVisionTaskModel):
    """
    Qwen3-VL (Vision-Language Model) 래퍼 (2025년 최신)

    Alibaba의 Qwen3-VL은 최신 멀티모달 모델입니다.

    Qwen3-VL 특징:
    - 이미지 이해 + 텍스트 생성
    - 128K 컨텍스트 윈도우
    - 29개 언어 지원 (한국어 포함)
    - 다양한 이미지 크기 처리
    - 최대 1시간 동영상 처리 가능

    모델 크기:
    - 2B: 경량, 빠른 추론
    - 4B: 균형잡힌 성능
    - 8B: 고성능
    - 32B: 최고 성능

    주요 기능:
    - Image Captioning: 이미지 설명 생성
    - VQA: 이미지에 대한 질문 답변
    - OCR: 이미지 내 텍스트 인식
    - Document Understanding: 문서 이해
    - Chart/Table Analysis: 차트/표 분석

    Example:
        ```python
        from beanllm.domain.vision import Qwen3VLWrapper

        # 모델 초기화
        qwen = Qwen3VLWrapper(model_size="8B")

        # 이미지 캡셔닝
        caption = qwen.caption("image.jpg")

        # VQA (Visual Question Answering)
        answer = qwen.vqa(
            image="image.jpg",
            question="이 이미지에서 무엇을 볼 수 있나요?"
        )

        # OCR
        text = qwen.ocr("document.jpg")

        # 다중 이미지 대화
        response = qwen.chat(
            images=["img1.jpg", "img2.jpg"],
            prompt="두 이미지의 차이점을 설명해주세요."
        )
        ```

    References:
        - GitHub: https://github.com/QwenLM/Qwen3-VL
        - HuggingFace: Qwen/Qwen3-VL-*
        - Blog: https://qwenlm.github.io/blog/qwen3-vl/
    """

    def __init__(
        self,
        model_size: str = "8B",
        device: Optional[str] = None,
        trust_remote_code: bool = True,
        **kwargs,
    ):
        """
        Args:
            model_size: 모델 크기
                - "2B": Qwen3-VL-2B (경량)
                - "4B": Qwen3-VL-4B (권장)
                - "8B": Qwen3-VL-8B (고성능, 기본값)
                - "32B": Qwen3-VL-32B (최고 성능)
            device: 디바이스 (cuda/cpu/mps)
            trust_remote_code: 원격 코드 신뢰 (HuggingFace)
            **kwargs: 추가 파라미터
        """
        super().__init__(**kwargs)

        self.model_size = model_size
        self.trust_remote_code = trust_remote_code

        # 디바이스 설정
        if device is None:
            import torch
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        self.device = device

        self._model = None
        self._processor = None

        logger.info(
            f"Qwen3VLWrapper initialized: model_size={model_size}, device={device}"
        )

    def _load_model(self):
        """모델 지연 로딩"""
        if self._model is not None:
            return

        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            import torch
        except ImportError as e:
            raise ImportError(
                "transformers and torch are required. "
                "Install with: pip install transformers torch"
            ) from e

        # 모델 이름 매핑
        model_names = {
            "2B": "Qwen/Qwen3-VL-2B-Instruct",
            "4B": "Qwen/Qwen3-VL-4B-Instruct",
            "8B": "Qwen/Qwen3-VL-8B-Instruct",
            "32B": "Qwen/Qwen3-VL-32B-Instruct",
        }

        if self.model_size not in model_names:
            raise ValueError(
                f"Invalid model_size: {self.model_size}. "
                f"Choose from: {list(model_names.keys())}"
            )

        model_name = model_names[self.model_size]

        logger.info(f"Loading Qwen3-VL model: {model_name}")

        # 모델 로드
        self._model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=self.trust_remote_code,
        )

        if self.device != "cuda":
            self._model = self._model.to(self.device)

        self._model.eval()

        # Processor 로드
        self._processor = AutoProcessor.from_pretrained(
            model_name,
            trust_remote_code=self.trust_remote_code,
        )

        logger.info("Qwen3-VL model loaded successfully")

    def caption(
        self,
        image: Union[str, Path, np.ndarray],
        prompt: str = "Describe this image in detail.",
        max_new_tokens: int = 256,
        **kwargs,
    ) -> str:
        """
        이미지 캡셔닝 (이미지 설명 생성)

        Args:
            image: 이미지 (경로 또는 배열)
            prompt: 프롬프트 (기본: "Describe this image in detail.")
            max_new_tokens: 최대 생성 토큰 수
            **kwargs: 추가 생성 파라미터

        Returns:
            생성된 캡션

        Example:
            ```python
            caption = qwen.caption("photo.jpg")
            # "A beautiful sunset over the ocean with orange and pink clouds..."
            ```
        """
        self._load_model()

        # 이미지 로드
        if isinstance(image, (str, Path)):
            from PIL import Image
            image = Image.open(image).convert("RGB")
        elif isinstance(image, np.ndarray):
            from PIL import Image
            image = Image.fromarray(image).convert("RGB")

        # 메시지 구성
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        # 입력 처리
        text = self._processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = self._processor(
            text=[text],
            images=[image],
            videos=None,
            padding=True,
            return_tensors="pt",
        )
        image_inputs = image_inputs.to(self.device)

        # 생성
        import torch
        with torch.no_grad():
            generated_ids = self._model.generate(
                **image_inputs,
                max_new_tokens=max_new_tokens,
                **kwargs,
            )

        # 디코딩
        output_text = self._processor.batch_decode(
            generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False,
        )[0]

        logger.info(f"Caption generated: {len(output_text)} characters")

        return output_text

    def vqa(
        self,
        image: Union[str, Path, np.ndarray],
        question: str,
        max_new_tokens: int = 256,
        **kwargs,
    ) -> str:
        """
        Visual Question Answering (이미지에 대한 질문 답변)

        Args:
            image: 이미지
            question: 질문
            max_new_tokens: 최대 생성 토큰 수
            **kwargs: 추가 생성 파라미터

        Returns:
            답변 텍스트

        Example:
            ```python
            answer = qwen.vqa(
                image="photo.jpg",
                question="How many people are in this image?"
            )
            # "There are 3 people in this image."
            ```
        """
        return self.caption(image=image, prompt=question, max_new_tokens=max_new_tokens, **kwargs)

    def ocr(
        self,
        image: Union[str, Path, np.ndarray],
        prompt: str = "Extract all text from this image.",
        max_new_tokens: int = 512,
        **kwargs,
    ) -> str:
        """
        OCR (이미지 내 텍스트 인식)

        Args:
            image: 이미지
            prompt: 프롬프트
            max_new_tokens: 최대 생성 토큰 수
            **kwargs: 추가 생성 파라미터

        Returns:
            인식된 텍스트

        Example:
            ```python
            text = qwen.ocr("document.jpg")
            # "Invoice\nDate: 2025-01-15\nAmount: $1,234.56..."
            ```
        """
        return self.caption(image=image, prompt=prompt, max_new_tokens=max_new_tokens, **kwargs)

    def chat(
        self,
        images: Union[List[Union[str, Path, np.ndarray]], Union[str, Path, np.ndarray]],
        prompt: str,
        max_new_tokens: int = 512,
        **kwargs,
    ) -> str:
        """
        다중 이미지 대화

        Args:
            images: 이미지 또는 이미지 리스트
            prompt: 프롬프트
            max_new_tokens: 최대 생성 토큰 수
            **kwargs: 추가 생성 파라미터

        Returns:
            응답 텍스트

        Example:
            ```python
            response = qwen.chat(
                images=["img1.jpg", "img2.jpg"],
                prompt="Compare these two images."
            )
            ```
        """
        self._load_model()

        # 단일 이미지를 리스트로 변환
        if not isinstance(images, list):
            images = [images]

        # 이미지 로드
        loaded_images = []
        for img in images:
            if isinstance(img, (str, Path)):
                from PIL import Image
                loaded_images.append(Image.open(img).convert("RGB"))
            elif isinstance(img, np.ndarray):
                from PIL import Image
                loaded_images.append(Image.fromarray(img).convert("RGB"))
            else:
                loaded_images.append(img)

        # 메시지 구성 (다중 이미지)
        content = []
        for img in loaded_images:
            content.append({"type": "image", "image": img})
        content.append({"type": "text", "text": prompt})

        messages = [{"role": "user", "content": content}]

        # 입력 처리
        text = self._processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = self._processor(
            text=[text],
            images=loaded_images,
            videos=None,
            padding=True,
            return_tensors="pt",
        )
        image_inputs = image_inputs.to(self.device)

        # 생성
        import torch
        with torch.no_grad():
            generated_ids = self._model.generate(
                **image_inputs,
                max_new_tokens=max_new_tokens,
                **kwargs,
            )

        # 디코딩
        output_text = self._processor.batch_decode(
            generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False,
        )[0]

        logger.info(f"Chat response generated: {len(output_text)} characters")

        return output_text

    def predict(
        self,
        image: Union[str, Path, np.ndarray],
        task: str = "caption",
        **kwargs,
    ) -> Union[str, Dict[str, Any]]:
        """
        태스크별 예측 실행 (BaseVisionTaskModel 인터페이스)

        Args:
            image: 이미지
            task: 태스크 타입
                - "caption": 이미지 캡셔닝
                - "vqa": Visual Question Answering
                - "ocr": 텍스트 인식
            **kwargs: 태스크별 추가 파라미터

        Returns:
            태스크별 결과

        Example:
            ```python
            # Caption
            caption = qwen.predict(image="photo.jpg", task="caption")

            # VQA
            answer = qwen.predict(
                image="photo.jpg",
                task="vqa",
                question="What is this?"
            )

            # OCR
            text = qwen.predict(image="document.jpg", task="ocr")
            ```
        """
        if task == "caption":
            return self.caption(image, **kwargs)
        elif task == "vqa":
            if "question" not in kwargs:
                raise ValueError("VQA task requires 'question' parameter")
            return self.vqa(image, kwargs["question"], **kwargs)
        elif task == "ocr":
            return self.ocr(image, **kwargs)
        else:
            raise ValueError(
                f"Unknown task: {task}. "
                f"Available: caption, vqa, ocr"
            )

    def __repr__(self) -> str:
        return f"Qwen3VLWrapper(model_size={self.model_size}, device={self.device})"
