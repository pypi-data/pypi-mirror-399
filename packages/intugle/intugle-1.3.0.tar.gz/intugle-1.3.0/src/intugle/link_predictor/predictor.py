import itertools
import json
import logging
import os

from typing import Any, Dict, List

import pandas as pd
import yaml

from intugle.analysis.models import DataSet
from intugle.core import settings
from intugle.core.console import console, warning_style
from intugle.core.pipeline.link_prediction.agent import MultiLinkPredictionAgent
from intugle.libs.smart_query_generator.utils.join import Join

from .models import LinkPredictionResult, PredictedLink

log = logging.getLogger(__name__)


class NoLinksFoundError(Exception):
    """Custom exception raised when no links are found to save."""
    pass


class LinkPredictor:
    """
    Analyzes a collection of datasets to predict column links between all
    possible pairs, ensuring that key identification has been performed on
    each dataset first.
    """

    def __init__(self, data_input: Dict[str, Any] | List[DataSet]):
        """
        Initializes the LinkPredictor with either a dictionary of raw dataframes
        or a list of pre-initialized DataSet objects.

        Args:
            data_input: Either a dictionary of {name: dataframe} or a list
                        of DataSet objects.
        """

        if isinstance(data_input, (dict, list)):
            if len(data_input) < 2:
                raise ValueError("LinkPredictor requires at least two datasets to compare.")
        else:
            raise TypeError("Input must be a dictionary of named dataframes or a list of DataSet objects.")

        self.datasets: Dict[str, DataSet] = {}
        self.links: list[PredictedLink] = []

        if isinstance(data_input, dict):
            self._initialize_from_dict(data_input)
        elif isinstance(data_input, list):
            self._initialize_from_list(data_input)

        print(f"LinkPredictor initialized with datasets: {list(self.datasets.keys())}")

        self.already_executed_combo = set()

    def _run_prerequisites(self, dataset: DataSet):
        """Runs the prerequisite analysis steps on a given DataSet."""
        dataset.profile().identify_datatypes().identify_keys()

    def _initialize_from_dict(self, data_dict: Dict[str, Any]):
        """Creates and processes DataSet objects from a dictionary of raw dataframes."""
        for name, df in data_dict.items():
            dataset = DataSet(df, name=name)
            print(f"Running prerequisite analysis for new dataset: '{name}'...")
            self._run_prerequisites(dataset)
            self.datasets[name] = dataset

    def _initialize_from_list(self, data_list: List[DataSet]):
        """Processes a list of existing DataSet objects, running analysis if needed."""
        for dataset in data_list:
            if not dataset.name:
                raise ValueError("DataSet objects provided in a list must have a 'name' attribute.")
            if dataset.source.table.key is None:
                print(f"Dataset '{dataset.name}' is missing key identification. Running prerequisite analysis...")
                self._run_prerequisites(dataset)
            else:
                print(f"Dataset '{dataset.name}' already processed. Skipping analysis.")
            self.datasets[dataset.name] = dataset

    def _create_table_combination_id(self, table_a: str, table_b: str) -> str:
        assets = [table_a, table_b]
        assets.sort()
        return f"{assets[0]}--{assets[1]}"

    def _is_duplicate_combination(self, table_combination: str) -> bool:
        """
        Checks if a table combination has already been executed.

        Args:
            table_combination (str): The table combination identifier.

        Returns:
            bool: True if the combination was already executed, False otherwise.
        """
        if table_combination in self.already_executed_combo:
            log.warning(f"[!] Skipping already executed combination: {table_combination}")
            return True
        return False

    def _invoke_agent(self, dataset_a: DataSet, dataset_b: DataSet) -> Dict[str, Any]:
        """
        Invokes the multi-link prediction agent to analyze two datasets.

        Args:
            dataset_a (DataSet): The first dataset to analyze.
            dataset_b (DataSet): The second dataset to analyze.

        Returns:
            Dict[str, Any]: The agent's output containing predicted links.
        """
        agent = MultiLinkPredictionAgent(
            table1_dataset=dataset_a,
            table2_dataset=dataset_b,
        )
        return agent()

    def _create_predicted_link(self, link_data: Dict[str, Any]) -> PredictedLink:
        """
        Creates a PredictedLink object from agent output data.

        Args:
            link_data (Dict[str, Any]): A dictionary containing link information.

        Returns:
            PredictedLink: A constructed PredictedLink object.
        """
        from_columns = [link['column1'] for link in link_data['links']]
        to_columns = [link['column2'] for link in link_data['links']]
        
        # Assuming table names are consistent across all parts of a composite key
        from_dataset = link_data['links'][0]['table1']
        to_dataset = link_data['links'][0]['table2']

        return PredictedLink(
            from_dataset=from_dataset,
            from_columns=from_columns,
            to_dataset=to_dataset,
            to_columns=to_columns,
            intersect_count=link_data.get("intersect_count"),
            intersect_ratio_from_col=link_data.get("intersect_ratio_col1"),
            intersect_ratio_to_col=link_data.get("intersect_ratio_col2"),
            from_uniqueness_ratio=link_data.get("from_uniqueness_ratio"),
            to_uniqueness_ratio=link_data.get("to_uniqueness_ratio"),
            accuracy=max(
                link_data.get("intersect_ratio_col1", 0) or 0,
                link_data.get("intersect_ratio_col2", 0) or 0
            ),
        )

    def _parse_agent_output(self, llm_result: Dict[str, Any]) -> List[PredictedLink]:
        """
        Parses the agent's output and constructs a list of PredictedLink objects.

        Args:
            llm_result (Dict[str, Any]): The output from the prediction agent.

        Returns:
            List[PredictedLink]: A list of predicted links between datasets.
        """
        pair_links: List[PredictedLink] = []
        if llm_result and llm_result.get("links"):
            # llm_result["links"] is a list of OutputSchema-like dictionaries
            for link_data in llm_result["links"]:
                if link_data.get('links'):
                    pair_links.append(self._create_predicted_link(link_data))
        return pair_links

    def _predict_for_pair(
        self,
        name_a: str,
        dataset_a: DataSet,
        name_b: str,
        dataset_b: DataSet,
    ) -> List[PredictedLink]:
        """
        Contains the core logic for finding links between TWO dataframes.
        This method can now safely assume that key identification has been run.
        """
        table_combination = self._create_table_combination_id(name_a, name_b)
        if self._is_duplicate_combination(table_combination):
            return []

        llm_result = self._invoke_agent(dataset_a, dataset_b)
        return self._parse_agent_output(llm_result)

    def predict(self, filename: str = None, save: bool = False, force_recreate: bool = False) -> 'LinkPredictor':
        """
        Iterates through all unique pairs of datasets, predicts the links for
        each pair, and returns the aggregated results.
        """
        if filename is None:
            filename = settings.RELATIONSHIPS_FILE

        relationships_file = os.path.join(settings.MODELS_DIR, filename)

        if not force_recreate and os.path.exists(relationships_file):
            is_stale = False
            relationships_mtime = os.path.getmtime(relationships_file)
            for dataset in self.datasets.values():
                dataset_yml = os.path.join(settings.MODELS_DIR, f"{dataset.name}.yml")
                if os.path.exists(dataset_yml) and os.path.getmtime(dataset_yml) > relationships_mtime:
                    is_stale = True
                    break
            
            if not is_stale:
                console.print("Link predictions are up-to-date. Loading from cache.", style="green")
                self.load_from_yaml(relationships_file)
                return self

        all_links: List[PredictedLink] = []
        dataset_names = list(self.datasets.keys())

        for name_a, name_b in itertools.combinations(dataset_names, 2):
            print(f"\n--- Comparing '{name_a}' <=> '{name_b}' ---")
            dataset_a = self.datasets[name_a]
            dataset_b = self.datasets[name_b]

            links_for_pair = self._predict_for_pair(name_a, dataset_a, name_b, dataset_b)

            if links_for_pair:
                print(f"Found {len(links_for_pair)} potential link(s).")
                all_links.extend(links_for_pair)
            else:
                print("No links found for this pair.")

        self.links = all_links

        if len(self.links) == 0:
            console.print("No links found between any datasets.", style=warning_style)
            return self

        if save:
            self.save_yaml(file_path=filename)

        return self
    
    def get_links_df(self) -> pd.DataFrame:
        """Returns the predicted links as a pandas DataFrame."""
        if not self.links:
            return pd.DataFrame()
        return pd.DataFrame([link.model_dump() for link in self.links])
    
    def show_graph(self):
        links = [link.relationship.link for link in self.links]
        join = Join(links, [])
        assets = {dataset.name for dataset in self.datasets.values()}

        graph = join.generate_graph(list(assets), only_connected=False)

        join.plot_graph(graph)

    def save_yaml(self, file_path: str) -> None:
        os.makedirs(settings.MODELS_DIR, exist_ok=True)
        file_path = os.path.join(settings.MODELS_DIR, file_path)

        if len(self.links) == 0:
            raise NoLinksFoundError("No links found to save.")

        relationships = {"relationships": [json.loads(link.relationship.model_dump_json()) for link in self.links]}

        # Save the relationships to a YAML file
        with open(file_path, "w") as file:
            yaml.dump(relationships, file, sort_keys=False, default_flow_style=False)

    def load_from_yaml(self, file_path: str) -> None:
        """Loads link predictions from a YAML file."""
        with open(file_path, "r") as f:
            data = yaml.safe_load(f)
        
        relationships = data.get("relationships", [])
        loaded_links = []
        for rel in relationships:
            metrics = rel.get("profiling_metrics", {}) or {}
            link = PredictedLink(
                from_dataset=rel["source"]["table"],
                from_columns=rel["source"]["columns"],
                to_dataset=rel["target"]["table"],
                to_columns=rel["target"]["columns"],
                intersect_count=metrics.get("intersect_count"),
                intersect_ratio_from_col=metrics.get("intersect_ratio_from_col"),
                intersect_ratio_to_col=metrics.get("intersect_ratio_to_col"),
                from_uniqueness_ratio=metrics.get("from_uniqueness_ratio"),
                to_uniqueness_ratio=metrics.get("to_uniqueness_ratio"),
                accuracy=metrics.get("accuracy"),
            )
            loaded_links.append(link)
        self.links = loaded_links


class LinkPredictionSaver:
    @classmethod
    def save_yaml(cls, result: LinkPredictionResult, file_path: str) -> None:
        os.makedirs(settings.MODELS_DIR, exist_ok=True)
        file_path = os.path.join(settings.MODELS_DIR, file_path)

        links = result.links

        if len(links) == 0:
            raise ValueError("No links found to save.")

        relationships = [link.relationship for link in links]
        relationships_data = {"relationships": [json.loads(r.model_dump_json()) for r in relationships]}

        # Save the relationships to a YAML file
        with open(file_path, "w") as file:
            yaml.dump(relationships_data, file, sort_keys=False, default_flow_style=False)
