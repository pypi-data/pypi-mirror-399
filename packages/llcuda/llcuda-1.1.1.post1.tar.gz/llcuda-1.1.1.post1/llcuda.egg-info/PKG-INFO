Metadata-Version: 2.4
Name: llcuda
Version: 1.1.1.post1
Summary: PyTorch-style CUDA-accelerated LLM inference with hybrid bootstrap architecture. 51 KB package auto-downloads binaries for all NVIDIA GPUs (SM 5.0-8.9). Works on local GPUs, Google Colab, and Kaggle.
Author-email: Waqas Muhammad <waqasm86@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/waqasm86/llcuda
Project-URL: Documentation, https://waqasm86.github.io/
Project-URL: Repository, https://github.com/waqasm86/llcuda
Project-URL: Bug Tracker, https://github.com/waqasm86/llcuda/issues
Project-URL: Changelog, https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md
Keywords: llm,cuda,gpu,inference,deep-learning,llama,jupyter,jupyterlab,chat,embeddings,semantic-search,gguf,colab,kaggle,t4,turing,ampere
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Framework :: Jupyter
Classifier: Framework :: Jupyter :: JupyterLab
Classifier: Environment :: GPU :: NVIDIA CUDA :: 12
Classifier: Environment :: GPU :: NVIDIA CUDA :: 11
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.20.0
Requires-Dist: requests>=2.20.0
Requires-Dist: huggingface_hub>=0.10.0
Requires-Dist: tqdm>=4.60.0
Provides-Extra: jupyter
Requires-Dist: ipywidgets>=7.6.0; extra == "jupyter"
Requires-Dist: tqdm>=4.60.0; extra == "jupyter"
Requires-Dist: IPython>=7.0.0; extra == "jupyter"
Requires-Dist: matplotlib>=3.5.0; extra == "jupyter"
Requires-Dist: pandas>=1.3.0; extra == "jupyter"
Provides-Extra: embeddings
Requires-Dist: scikit-learn>=1.0.0; extra == "embeddings"
Provides-Extra: models
Requires-Dist: huggingface_hub>=0.10.0; extra == "models"
Requires-Dist: gguf>=0.1.0; extra == "models"
Provides-Extra: all
Requires-Dist: ipywidgets>=7.6.0; extra == "all"
Requires-Dist: tqdm>=4.60.0; extra == "all"
Requires-Dist: IPython>=7.0.0; extra == "all"
Requires-Dist: matplotlib>=3.5.0; extra == "all"
Requires-Dist: pandas>=1.3.0; extra == "all"
Requires-Dist: scikit-learn>=1.0.0; extra == "all"
Requires-Dist: huggingface_hub>=0.10.0; extra == "all"
Requires-Dist: gguf>=0.1.0; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=3.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"
Requires-Dist: sphinx>=4.5.0; extra == "dev"
Dynamic: license-file

# llcuda: CUDA-Accelerated LLM Inference for Python

**Effortless, zero-configuration LLM inference with CUDA acceleration. Compatible with all modern NVIDIA GPUs, Google Colab, Kaggle, and JupyterLab. PyTorch-inspired API for seamless integration.**

> Ideal for:  
> - Google Colab and Kaggle notebooks  
> - Local development on GPUs from GTX 940M to RTX 4090  
> - Production-grade performance without manual setup  

---

## What's New in Version 1.1.1

**Universal GPU Compatibility and Cloud-Optimized Design**

In previous versions (1.0.x), compatibility was limited, often leading to errors like "no kernel image available" on older architectures such as Tesla T4 in Colab/Kaggle.

With v1.1.1, we've introduced a **hybrid bootstrap architecture** for broader support:

- **Ultra-Light PyPI Package**: Only 51 KB (down from 327 MB) – pure Python code.  
- **Auto-Download System**: Binaries (~700 MB) and models (~770 MB) fetch automatically on first import, based on your GPU.  
- **Expanded GPU Support**: All NVIDIA architectures with compute capability 5.0+ (Maxwell to Ada Lovelace).  
- **Full Cloud Integration**: Seamless on Google Colab (T4, P100, V100, A100) and Kaggle (T4).  
- **No Breaking Changes**: Backward-compatible with v1.0.x APIs.  

Example Upgrade:

```python
# Previously (v1.0.x) – Error on T4/P100
!pip install llcuda==1.0.0
engine = llcuda.InferenceEngine()
engine.load_model("gemma-3-1b-Q4_K_M")  # Fails on incompatible GPUs

# Now (v1.1.1) – Works everywhere
!pip install llcuda
engine = llcuda.InferenceEngine()
engine.load_model("gemma-3-1b-Q4_K_M")  # Auto-configures and runs
