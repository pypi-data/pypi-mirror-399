# ğŸ§  SAARA: Autonomous Document-to-LLM Data Engine

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Ollama](https://img.shields.io/badge/Ollama-Local_AI-orange.svg)](https://ollama.com/)
[![License](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE)

**SAARA** is an end-to-end autonomous data pipeline designed to transform raw, unstructured documents (PDFs, research papers) into high-quality, instruction-tuned datasets for fine-tuning Large Language Models (LLMs).

> **Why this exists**: Creating high-quality datasets is the bottleneck in training domain-specific AI. This tool automates the "boring stuff"â€”OCR, chunking, labeling, and cleaningâ€”allowing you to go from PDF to fine-tuned model in hours, not weeks.

---

## ğŸš€ Key Features

### 1. ğŸ‘ï¸ SOTA Vision-LLM OCR
- **No more Garbled Text**: Uses **Moondream** and **Qwen2.5-VL** (Vision-Language Models) to "read" PDFs visually.
- Handles complex double-column layouts, tables, and scientific diagrams that traditional OCR (Tesseract) fails on.
- **Hybrid Fallback**: Automatically switches between PyMuPDF (fast) and Vision OCR (accurate) based on page extractability.

### 2. ğŸ¤– Autonomous Data Labeling
- Uses local LLMs (Granite 4.0, Llama 3) to generate diverse training tasks:
    - **Instruction Tuning**: "How do I treat X using Ayurveda?"
    - **Q&A Pairs**: Fact-based extraction.
    - **Summarization**: TL;DRs of complex sections.
    - **Classification**: Topic tagging.

### 3. ğŸ§ª Data Distillation & Hygiene
- **Self-Cleaning**: The `distill` module removes low-quality generations, duplicates, and confabulations.
- **ShareGPT Formatting**: Automatically converts raw data into the industry-standard conversation format.

### 4. ğŸ—ï¸ Pre-training from Scratch *(NEW)*
- **Build Your Own LLM**: Create custom models from 15M to 3B parameters.
- **Custom Tokenizers**: Train domain-specific BPE tokenizers on your data.
- **Full Pipeline**: Pre-train â†’ Fine-tune â†’ Evaluate â†’ Deploy.
- Production-ready LLaMA-style architectures.

### 5. ğŸ“ Native Fine-Tuning Support
- **One-Command Training**: Built-in training loop using `SFTTrainer` (QLoRA) to fine-tune any HuggingFace model.
- **Multi-Format Support**: Automatically handles ShareGPT, Alpaca, and Raw Text formats.
- **Checkpoint Resume**: Continue training from any checkpoint.
- **Iterative Fine-tuning**: Fine-tune your already fine-tuned models to keep improving.
- Optimized for consumer GPUs (supports 4-bit quantization).

### 6. ğŸ§ª Model Evaluation & Self-Improvement
- **Granite 4 as Judge**: Test your fine-tuned model with automatic quality scoring.
- **Self-Improvement Loop**: Low-scoring responses are corrected and used for next training round.
- **Iterative Enhancement**: Train â†’ Evaluate â†’ Improve â†’ Repeat.

### 7. ğŸš€ Model Deployment
- **Local Chat**: Interactive terminal testing with your model.
- **Ollama Export**: Convert to GGUF format for Ollama usage.
- **HuggingFace Hub**: Push your model to share with the community.
- **Cloud Deployment**: Docker + Google Cloud Run ready.

---

## ğŸ› ï¸ Architecture

```mermaid
graph LR
    A[Raw PDF] --> B(Vision OCR / Extractor)
    B --> C{Chunker Strategy}
    C --> D[Synthetic Labeling Agent]
    D --> E[Raw Dataset JSONL]
    E --> F(Data Distiller)
    F --> G[Clean ShareGPT Dataset]
    G --> H{Training Path}
    H -->|Pre-train| I[Build New Model]
    H -->|Fine-tune| J[Adapt Existing Model]
    I --> K[Model Evaluation]
    J --> K
    K --> L{Score < 7?}
    L -->|Yes| M[Generate Corrections]
    M --> J
    L -->|No| N((Deploy Model))
```

---

## ğŸ“¦ Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/nikhil49023/Data-engine.git
    cd Data-engine
    ```

2.  **Install the CLI**:
    ```bash
    pip install -e .
    ```

3.  **Setup Ollama**:
    - Install [Ollama](https://ollama.ai)
    - The setup wizard will help you install models automatically

### Quick Start

**First-time setup (recommended):**
```bash
saara setup
```

The setup wizard will:
1. âœ… Detect your hardware (GPU, VRAM, RAM)
2. âœ… Recommend optimal models for your system
3. âœ… Install selected vision and analyzer models
4. âœ… Save configuration

---

## âš¡ Usage

### ğŸ¯ Interactive Wizard (Recommended)

```bash
saara run
```

This launches a beautiful CLI wizard with 5 workflows:

| Option | Mode | Description |
|--------|------|-------------|
| 1 | ğŸ“„ Dataset Creation | Extract data from PDFs â†’ Generate training datasets |
| 2 | ğŸ§  Model Training | Fine-tune LLMs on your prepared data |
| 3 | ğŸ§ª Model Evaluation | Test & improve models with Granite 4 |
| 4 | ğŸš€ Model Deployment | Deploy locally (Ollama) or to cloud |
| 5 | ğŸ—ï¸ Pre-training | Build & train a model from scratch |

---

### ğŸ—ï¸ Pre-training from Scratch *(NEW)*

Build your own language model from the ground up:

```bash
saara pretrain
```

**Available Architectures:**

| Name | Parameters | VRAM | Use Case |
|------|-----------|------|----------|
| Nano | ~15M | 2GB+ | Testing, learning (CPU trainable) |
| Micro | ~50M | 4GB+ | Experimentation |
| Mini | ~125M | 6GB+ | Domain-specific pre-training |
| Small | ~350M | 8GB+ | Specialized tasks |
| Base | ~1B | 16GB+ | Production models |
| Large | ~3B | 24GB+ | High-capacity models |

**Pre-training Sub-menu:**
1. ğŸ“š Create Pre-training Dataset
2. ğŸ—ï¸ Build & Train New Model
3. ğŸ”¤ Train Custom Tokenizer
4. ğŸ§ª Test Pre-trained Model
5. ğŸ“‹ List Pre-trained Models

**Pre-training Dataset Creation:**
- Extracts raw text from PDFs, markdown, and text files
- Cleans OCR artifacts and normalizes unicode
- Chunks text into optimal sizes for language modeling
- **LLM-Enhanced Processing (Optional):**
  - Uses local LLM (Granite 4, Llama 3, Qwen) to clean and improve text
  - Fixes OCR errors and expands abbreviations
  - LLM-based quality scoring for more accurate filtering
- Quality filtering (removes low-quality/incoherent text)
- Deduplication (prevents model memorization)
- Outputs in JSONL format ready for training
- Optional train/validation split

**Workflow:**
```
Create Dataset â†’ Train Tokenizer (optional) â†’ Pre-train Model â†’ Test â†’ Fine-tune â†’ Deploy
```

---


### ğŸ“„ Dataset Creation Flow

1. Select input PDF folder and output directory
2. Choose Vision OCR model (Moondream/Qwen) - auto-detects available models
3. Choose Analyzer model (Granite 4/Llama 3/Qwen 2.5/Mistral)
4. Configure advanced options (chunk size, Q&A density)
5. Pipeline automatically generates:
   - `*_instruction.jsonl` - Instruction tuning data
   - `*_qa.jsonl` - Q&A pairs
   - `*_sharegpt.jsonl` - Chat format (best for training)
   - `*_summarization.jsonl` - Summarization tasks

---

### ğŸ§  Model Training Flow

The training wizard now supports:
- **Base Models**: HuggingFace models (Gemma, Llama, Qwen, etc.)
- **Custom Pre-trained**: Your own pre-trained models
- **Fine-tuned Adapters**: Continue training existing adapters

**Supported Base Models:**
| Model | Size | Best For |
|-------|------|----------|
| sarvamai/sarvam-1 | 2B | Indian Languages |
| google/gemma-2b | 2B | General Purpose |
| TinyLlama/TinyLlama-1.1B | 1.1B | Fast Testing |
| meta-llama/Llama-3.2-1B | 1B | English Tasks |
| Qwen/Qwen2.5-7B | 7B | Complex Reasoning |

**Output:** `models/{model-name}-finetuned/final_adapter/`

---

### ğŸ§ª Model Evaluation Flow

Uses **Granite 4** to evaluate your fine-tuned model:

1. Runs test prompts through your model
2. Scores each response (1-10)
3. Generates improved responses for low scores
4. Creates correction data for next training round

**Self-Improvement Cycle:**
```
Train Model â†’ Evaluate (Granite 4) â†’ Generate Corrections â†’ Retrain â†’ Repeat
```

---

### ğŸš€ Model Deployment Flow

| Option | Platform | Description |
|--------|----------|-------------|
| 1 | Local Chat | Interactive terminal chat |
| 2 | Ollama Export | Convert to GGUF format |
| 3 | HuggingFace | Push to HF Hub |
| 4 | Cloud Deploy | Docker + Google Cloud Run |
| 5 | Merge Model | Merge adapter with base |

---

## ğŸ“Ÿ CLI Commands

### Core Commands

| Command | Description |
|---------|-------------|
| `saara run` | Start interactive wizard |
| `saara pretrain` | Build & train model from scratch |
| `saara setup` | First-time hardware detection & model setup |
| `saara version` | Show version information |

### Data Processing

| Command | Description |
|---------|-------------|
| `saara process <file>` | Process a single PDF file |
| `saara batch <dir>` | Process all PDFs in directory |
| `saara distill <input>` | Generate synthetic training data |

### Model Operations

| Command | Description |
|---------|-------------|
| `saara train` | Fine-tune a model (interactive) |
| `saara deploy` | Deploy a trained model |
| `saara evaluate <base> <adapter>` | Evaluate model quality |

### Model Management

| Command | Description |
|---------|-------------|
| `saara models list` | List all available models |
| `saara models install <name>` | Install an Ollama model |
| `saara models remove <name>` | Remove a model |
| `saara models status` | Show hardware & model status |

### Server

| Command | Description |
|---------|-------------|
| `saara serve` | Start REST API server |

---

## ğŸ“ Project Structure

```
Data-engine/
â”œâ”€â”€ setup.py                # Package setup
â”œâ”€â”€ config.yaml             # Configuration settings
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ saara/                  # Source code
â”‚   â”œâ”€â”€ cli.py             # CLI entry point
â”‚   â”œâ”€â”€ pipeline.py         # Core data pipeline
â”‚   â”œâ”€â”€ pretrain.py         # Pre-training module (NEW)
â”‚   â”œâ”€â”€ train.py            # LLM fine-tuning module
â”‚   â”œâ”€â”€ evaluator.py        # Model evaluation
â”‚   â”œâ”€â”€ deployer.py         # Deployment utilities
â”‚   â”œâ”€â”€ distiller.py        # Data cleaning
â”‚   â”œâ”€â”€ model_manager.py    # Ollama model management
â”‚   â””â”€â”€ splash.py           # SAARA splash screen
â”œâ”€â”€ models/                 # Saved models (pre-trained & fine-tuned)
â”œâ”€â”€ datasets/               # Generated datasets
â”œâ”€â”€ tokenizers/             # Custom tokenizers
â”œâ”€â”€ evaluations/            # Evaluation results
â””â”€â”€ exports/                # Deployment artifacts
```

---

## ğŸ”® Roadmap

- [x] Vision-LLM OCR (Moondream, Qwen)
- [x] Autonomous data labeling
- [x] Multi-format dataset generation
- [x] Native fine-tuning with QLoRA
- [x] Model evaluation with Granite 4
- [x] Self-improvement training loop
- [x] Local & cloud deployment
- [x] Pre-training from scratch
- [x] Custom tokenizer training
- [x] Iterative adapter fine-tuning
- [ ] Multi-modal dataset generation (images + text)
- [ ] RAG-based factual verification
- [ ] Web UI dashboard

---

## ğŸ“„ License

**Proprietary License** - Copyright Â© 2024-2025 Kilani Sai Nikhil. All Rights Reserved.

This software is provided under a proprietary license with the following terms:

âœ… **Permitted:**
- Use the software for personal, educational, or commercial purposes
- Reference in academic/educational contexts with attribution

âŒ **Not Permitted:**
- Modify, alter, or create derivative works
- Reproduce, copy, or duplicate the software
- Distribute, sublicense, or sell the software
- Reverse engineer or decompile the software

See the [LICENSE](LICENSE) file for full details.

---

## ğŸ‘¤ Author

**Kilani Sai Nikhil** - [GitHub](https://github.com/nikhil49023)

---

*Built with â¤ï¸ for the AI community*
