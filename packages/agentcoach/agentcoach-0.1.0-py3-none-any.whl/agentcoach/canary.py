"""Canary test suite generation from failures."""

import json
from pathlib import Path
from typing import Any, Union

from agentcoach.models import Finding, Trace


def generate_canary_suite(
    report_path: Union[str, Path],
    output_dir: Union[str, Path],
) -> dict[str, Path]:
    """Generate canary test suite from analysis report.
    
    Args:
        report_path: Path to report.json
        output_dir: Directory to write canary suite
        
    Returns:
        Dictionary mapping file type to path
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load report
    with open(report_path) as f:
        report = json.load(f)
    
    # Generate test cases from findings
    test_cases = _generate_test_cases(report)
    
    # Write cases.jsonl
    cases_path = output_dir / "cases.jsonl"
    with open(cases_path, "w") as f:
        for case in test_cases:
            f.write(json.dumps(case) + "\n")
    
    # Write test_canary.py
    test_file_path = output_dir / "test_canary.py"
    test_code = _generate_test_code()
    with open(test_file_path, "w") as f:
        f.write(test_code)
    
    return {
        "cases": cases_path,
        "test_file": test_file_path,
    }


def _generate_test_cases(report: dict[str, Any]) -> list[dict[str, Any]]:
    """Generate test cases from report findings."""
    test_cases = []
    
    findings = report.get("findings", [])
    
    # Group findings by category
    by_category: dict[str, list[dict]] = {}
    for finding in findings:
        cat = finding["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(finding)
    
    # Generate test case for each category with issues
    for category, cat_findings in by_category.items():
        if category == "consistency":
            continue  # Skip consistency (MVP stub)
        
        test_case = {
            "id": f"canary_{category}",
            "category": category,
            "description": f"Regression test for {category} issues",
            "input": "Sample input that triggered this issue",
            "constraints": [],
            "expected_schema": None,
            "banned_phrases": [],
            "require_citations": False,
        }
        
        # Add constraints based on category
        if category == "schema":
            test_case["expected_schema"] = {
                "type": "object",
                "required": ["answer"],
            }
        
        elif category == "grounding":
            test_case["require_citations"] = True
            test_case["constraints"].append("Must cite evidence sources")
        
        elif category == "tool_use":
            test_case["constraints"].append("All tool calls must succeed")
            test_case["constraints"].append("Tool outputs must be used in answer")
        
        elif category == "loops":
            test_case["constraints"].append("No repeated tool calls")
            test_case["constraints"].append("Maximum 10 total tool calls")
        
        elif category == "state":
            # Extract lost constraints from findings
            for finding in cat_findings:
                lost = finding.get("details", {}).get("lost_constraints", [])
                test_case["constraints"].extend(lost[:3])
        
        elif category == "policy_tone":
            # Extract banned phrases
            for finding in cat_findings:
                banned = finding.get("details", {}).get("banned_phrases_found", [])
                test_case["banned_phrases"].extend(banned)
        
        test_cases.append(test_case)
    
    return test_cases


def _generate_test_code() -> str:
    """Generate pytest test file code."""
    return '''"""Canary regression tests generated by AgentCoach."""

import json
from pathlib import Path
from typing import Any, Callable, Union

import pytest


def load_test_cases() -> list[dict[str, Any]]:
    """Load test cases from cases.jsonl."""
    cases = []
    cases_file = Path(__file__).parent / "cases.jsonl"
    
    with open(cases_file) as f:
        for line in f:
            if line.strip():
                cases.append(json.loads(line))
    
    return cases


# User must provide this function
def run_agent(input_text: str) -> dict[str, Any]:
    """Run your agent and return the result.
    
    This function must be implemented by the user.
    It should return a dict with:
    - output: str (the final answer)
    - trace: dict (optional trace data)
    - tool_calls: list (optional tool call info)
    
    Example:
        def run_agent(input_text: str) -> dict[str, Any]:
            result = my_agent.run(input_text)
            return {
                "output": result.final_answer,
                "trace": result.trace_data,
                "tool_calls": result.tool_calls,
            }
    """
    raise NotImplementedError(
        "You must implement run_agent() function to execute your agent"
    )


@pytest.mark.parametrize("test_case", load_test_cases(), ids=lambda tc: tc["id"])
def test_canary(test_case: dict[str, Any]) -> None:
    """Run canary test case."""
    # Run agent
    result = run_agent(test_case["input"])
    output = result.get("output", "")
    
    # Check schema if required
    if test_case.get("expected_schema"):
        try:
            output_data = json.loads(output)
            schema = test_case["expected_schema"]
            
            # Check required fields
            for field in schema.get("required", []):
                assert field in output_data, f"Missing required field: {field}"
        except json.JSONDecodeError:
            pytest.fail("Output is not valid JSON")
    
    # Check banned phrases
    for phrase in test_case.get("banned_phrases", []):
        assert phrase.lower() not in output.lower(), f"Banned phrase found: {phrase}"
    
    # Check citations if required
    if test_case.get("require_citations"):
        has_citation = any(
            marker in output.lower()
            for marker in ["[", "source:", "according to", "based on"]
        )
        assert has_citation, "Output lacks required citations"
    
    # Check constraints
    for constraint in test_case.get("constraints", []):
        # Simple keyword check
        if "must" in constraint.lower() or "no" in constraint.lower():
            # These are harder to validate automatically
            # User should add custom validation
            pass
    
    # Check tool calls if constraints mention them
    tool_calls = result.get("tool_calls", [])
    if any("tool" in c.lower() for c in test_case.get("constraints", [])):
        # Verify no repeated tool calls
        signatures = [(tc.get("name"), str(tc.get("args"))) for tc in tool_calls]
        assert len(signatures) == len(set(signatures)), "Repeated tool calls detected"
        
        # Verify reasonable number of calls
        assert len(tool_calls) <= 10, f"Too many tool calls: {len(tool_calls)}"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
'''
