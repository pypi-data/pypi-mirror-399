# Lorekeeper Client Configuration

# Local LLM endpoint (llama.cpp server)
# llama.cpp: http://localhost:8080 (default)
# Ollama: http://localhost:11434
# LM Studio: http://localhost:1234
LLM_ENDPOINT=http://localhost:8080

# Backend WebSocket URL (workers endpoint)
LOREKEEPER_BACKEND=wss://api.lorekeeper.ai/workers

# Worker authentication (REQUIRED for backend connection)
# Generate a token using the backend CLI:
#   cd /path/to/lorekeeper
#   go run cmd/token/main.go generate -local -worker-id my-worker -model-id qwen3-1.7b
#
# The CLI will output values to paste here:
WORKER_ID=
WORKER_TOKEN=
MODEL_ID=default

# Client bridge server settings
HOST=127.0.0.1
PORT=8081

# Context limits (in characters, ~4 chars per token)
# MAX_MESSAGE_LENGTH: Max size of a single message (default 100KB ~25K tokens)
# MAX_TOTAL_CONTEXT: Max total context size (default 500KB ~125K tokens)
# Set based on your model's context window (e.g., 32K model = ~128KB)
MAX_MESSAGE_LENGTH=100000
MAX_TOTAL_CONTEXT=500000
MAX_TIMEOUT=300.0

# Context compaction: if true, truncate old messages instead of erroring on overflow
# When enabled, older messages are removed to fit within MAX_TOTAL_CONTEXT
# System prompt and most recent messages are always preserved
CONTEXT_COMPACTION=true

# Development mode (enables hot reload)
DEV=false
