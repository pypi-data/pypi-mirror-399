"""
mem1 + LangChain é›†æˆç¤ºä¾‹

ä¸‰å±‚è®°å¿†æ¶æ„ï¼š
- Tier 1 (çŸ­æœŸ): LangChain ç®¡ç†çš„å½“å‰ä¼šè¯
- Tier 2 (ç”»åƒ): mem1 ç”¨æˆ·ç”»åƒï¼Œæ³¨å…¥ system prompt
- Tier 3 (é•¿æœŸ): ES å­˜å‚¨çš„å†å²å¯¹è¯
"""
import os
import logging
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory

from mem1 import Mem1Memory, Mem1Config, LLMConfig

load_dotenv()
logging.basicConfig(level=logging.INFO)

config = Mem1Config(
    llm=LLMConfig(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com"
    )
)
config.memory.auto_update_profile = False

USER_ID = "langchain_demo_user"


def demo_manual_integration():
    """æ–¹å¼1: æ‰‹åŠ¨é›†æˆï¼ˆæ›´çµæ´»ï¼‰"""
    print("\n=== æ‰‹åŠ¨é›†æˆ mem1 åˆ° LangChain ===")
    
    memory = Mem1Memory(config)
    
    # è·å–ç”¨æˆ·ç”»åƒ (Tier 2)
    ctx = memory.get_context(user_id=USER_ID, query="å¸®æˆ‘å†™æŠ¥å‘Š")
    
    # æ„å»º system prompt
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚

## ç”¨æˆ·ç”»åƒ
{ctx['import_content']}

## å½“å‰æ—¶é—´
{ctx['current_time']}
"""
    
    # LangChain LLM
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com"
    )
    
    # Tier 1: å½“å‰ä¼šè¯
    messages = [SystemMessage(content=system_prompt)]
    conversation_to_save = []
    
    # å¤šè½®å¯¹è¯
    user_inputs = ["ä½ å¥½", "å¸®æˆ‘å†™ä¸ªç®€å•çš„æŠ¥å‘Š"]
    for user_input in user_inputs:
        print(f"\nğŸ‘¤ ç”¨æˆ·: {user_input}")
        messages.append(HumanMessage(content=user_input))
        
        response = llm.invoke(messages)
        print(f"ğŸ¤– åŠ©æ‰‹: {response.content[:100]}...")
        
        messages.append(response)
        conversation_to_save.append({"role": "user", "content": user_input})
        conversation_to_save.append({"role": "assistant", "content": response.content})
    
    # ä¿å­˜åˆ° Tier 3
    memory.add_conversation(
        messages=conversation_to_save,
        user_id=USER_ID,
        metadata={"session": "manual_demo"}
    )
    print("\nâœ“ ä¼šè¯å·²ä¿å­˜åˆ° ES")


def demo_chain_integration():
    """æ–¹å¼2: ä½¿ç”¨ LangChain Chain"""
    print("\n=== LangChain Chain + mem1 ===")
    
    memory = Mem1Memory(config)
    ctx = memory.get_context(user_id=USER_ID, query="")
    
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com"
    )
    
    # Tier 1: LangChain çŸ­æœŸè®°å¿†
    chat_history = InMemoryChatMessageHistory()
    
    # æ³¨å…¥ Tier 2 ç”»åƒ
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚\n\n## ç”¨æˆ·ç”»åƒ\n{ctx['import_content']}"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ])
    
    chain = prompt | llm
    
    # å¯¹è¯
    query = "ä½ å¥½ï¼Œè®°å¾—æˆ‘çš„åå¥½å—ï¼Ÿ"
    print(f"\nğŸ‘¤ ç”¨æˆ·: {query}")
    
    result = chain.invoke({"input": query, "history": chat_history.messages})
    print(f"ğŸ¤– åŠ©æ‰‹: {result.content[:100]}...")
    
    # æ›´æ–°çŸ­æœŸè®°å¿†
    chat_history.add_user_message(query)
    chat_history.add_ai_message(result.content)
    
    # ä¿å­˜åˆ° Tier 3
    memory.add_conversation(
        messages=[
            {"role": "user", "content": query},
            {"role": "assistant", "content": result.content}
        ],
        user_id=USER_ID,
        metadata={"session": "chain_demo"}
    )
    print("âœ“ ä¼šè¯å·²ä¿å­˜åˆ° ES")


if __name__ == "__main__":
    demo_manual_integration()
    demo_chain_integration()
