"""Built-in rules for detecting prompt injection vulnerabilities."""

# Built-in rules for detecting prompt injection vulnerabilities
BUILTIN_RULES = [
    {
        "id": "PS001",
        "message": "User input directly concatenated into prompt string",
        "severity": "critical",
        "category": "prompt-injection",
        "description": "Untrusted user input is directly concatenated into a prompt string without sanitization. This can lead to prompt injection attacks.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Sanitize user input before including in prompts, or use structured message formats with separate system and user roles.",
        "references": [
            "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
            "https://simonwillison.net/2022/Sep/12/prompt-injection/"
        ],
    },
    {
        "id": "PS002",
        "message": "User input in f-string prompt template",
        "severity": "critical",
        "category": "prompt-injection",
        "description": "User-controlled variables are interpolated into prompt f-strings without validation.",
        "pattern_type": "ast",
        "languages": ["python"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Use parameterized prompts or validate/sanitize user input before interpolation.",
    },
    {
        "id": "PS003",
        "message": "Hardcoded API key detected near LLM code",
        "severity": "high",
        "category": "secrets",
        "description": "API keys or secrets appear to be hardcoded in the source code near LLM API calls.",
        "pattern_type": "regex",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-798",
        "fix_suggestion": "Use environment variables or a secrets manager for API keys.",
    },
    {
        "id": "PS004",
        "message": "System prompt accepts user-controlled content",
        "severity": "critical",
        "category": "prompt-injection",
        "description": "The system prompt includes user-controlled content, which could allow attackers to override system instructions.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Keep system prompts static and separate from user input. Use the user message role for user content.",
    },
    {
        "id": "PS005",
        "message": "Missing input validation before LLM call",
        "severity": "medium",
        "category": "input-validation",
        "description": "User input is passed to LLM API without apparent validation or length limits.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-20",
        "fix_suggestion": "Validate and sanitize user input. Consider implementing length limits and content filtering.",
    },
    {
        "id": "PS006",
        "message": "LLM output used directly without validation",
        "severity": "medium",
        "category": "output-validation",
        "description": "LLM response is used directly in code execution, database queries, or system commands without validation.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-94",
        "fix_suggestion": "Validate and sanitize LLM outputs before using in sensitive operations.",
    },
    {
        "id": "PS007",
        "message": "Unsafe string concatenation in LangChain prompt",
        "severity": "high",
        "category": "prompt-injection",
        "description": "User input is concatenated into LangChain PromptTemplate or ChatPromptTemplate without proper escaping.",
        "pattern_type": "ast",
        "languages": ["python"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Use LangChain's input variables and pass user input as parameters rather than concatenating.",
    },
    {
        "id": "PS008",
        "message": "Unsafe .format() call on prompt string",
        "severity": "high",
        "category": "prompt-injection",
        "description": "Using .format() with user-controlled content in prompt strings can lead to injection.",
        "pattern_type": "ast",
        "languages": ["python"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Validate format string placeholders or use parameterized approaches.",
    },
    {
        "id": "PS009",
        "message": "Request/form data flows to LLM without sanitization",
        "severity": "critical",
        "category": "prompt-injection",
        "description": "HTTP request data (query params, form data, body) flows to LLM API calls without sanitization.",
        "pattern_type": "taint",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Sanitize request data before including in prompts. Implement input validation middleware.",
    },
    {
        "id": "PS010",
        "message": "Database content used in prompt without escaping",
        "severity": "medium",
        "category": "prompt-injection",
        "description": "Data retrieved from database is used in prompts without escaping, which could enable stored prompt injection.",
        "pattern_type": "taint",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-77",
        "fix_suggestion": "Treat database content as untrusted and sanitize before including in prompts.",
    },
    {
        "id": "PS011",
        "message": "Jailbreak pattern detected in prompt",
        "severity": "info",
        "category": "prompt-quality",
        "description": "The prompt contains patterns commonly used in jailbreak attempts that should be filtered.",
        "pattern_type": "regex",
        "languages": ["python", "javascript", "typescript"],
        "fix_suggestion": "Review this prompt for potential bypass instructions. Consider adding jailbreak detection.",
    },
    {
        "id": "PS012",
        "message": "OpenAI API call without error handling",
        "severity": "low",
        "category": "reliability",
        "description": "OpenAI API calls should have proper error handling for rate limits, timeouts, and API errors.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "fix_suggestion": "Wrap API calls in try/except and handle OpenAI-specific exceptions.",
    },
    {
        "id": "PS013",
        "message": "Anthropic API call without error handling",
        "severity": "low",
        "category": "reliability",
        "description": "Anthropic API calls should have proper error handling for rate limits and API errors.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "fix_suggestion": "Wrap API calls in try/except and handle Anthropic-specific exceptions.",
    },
    {
        "id": "PS014",
        "message": "Unsafe tool/function calling pattern",
        "severity": "high",
        "category": "prompt-injection",
        "description": "LLM tool/function calling is configured with potentially unsafe patterns that could be exploited.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-94",
        "fix_suggestion": "Validate tool call parameters before execution. Use allowlists for permitted operations.",
    },
    {
        "id": "PS015",
        "message": "Sensitive data may leak to LLM context",
        "severity": "medium",
        "category": "data-leakage",
        "description": "Variables containing potentially sensitive data (passwords, tokens, PII) are included in LLM prompts.",
        "pattern_type": "ast",
        "languages": ["python", "javascript", "typescript"],
        "cwe_id": "CWE-200",
        "fix_suggestion": "Avoid sending sensitive data to external LLM APIs. Redact or mask sensitive information.",
    },
]
