"""
Report Generator

Creates comprehensive reports comparing pipeline performance with
publication-quality visualizations.

Author: Ayeh Bolouki
Email: ayehbolouki1988@gmail.com
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path
from typing import Dict, Optional
import logging
from jinja2 import Template

logger = logging.getLogger(__name__)

# Set plot style
sns.set_style("whitegrid")
sns.set_palette("husl")


class ReportGenerator:
    """
    Generate comparison reports from benchmark results.
    
    Creates HTML reports with visualizations comparing pipeline performance
    across multiple metrics.
    
    Parameters
    ----------
    results_dir : str
        Directory containing benchmark results
    
    Examples
    --------
    >>> reporter = ReportGenerator('benchmark_results/')
    >>> reporter.generate_report('report.html')
    """
    
    def __init__(self, results_dir: str):
        """Initialize report generator."""
        self.results_dir = Path(results_dir)
        
        if not self.results_dir.exists():
            raise FileNotFoundError(f"Results directory not found: {results_dir}")
        
        logger.info(f"Initialized report generator: {results_dir}")
    
    def generate_report(
        self,
        output_file: str = 'raptor_report.html',
        format: str = 'html'
    ):
        """
        Generate comparison report.
        
        Parameters
        ----------
        output_file : str
            Output file path
        format : str
            Report format: 'html', 'pdf', or 'markdown'
        """
        logger.info(f"Generating {format} report...")
        
        # Load results
        results = self._load_results()
        
        # Generate visualizations
        figures = self._create_visualizations(results)
        
        # Generate report based on format
        if format == 'html':
            self._generate_html_report(results, figures, output_file)
        elif format == 'pdf':
            self._generate_pdf_report(results, figures, output_file)
        elif format == 'markdown':
            self._generate_markdown_report(results, figures, output_file)
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        logger.info(f"Report generated: {output_file}")
    
    def _load_results(self) -> Dict:
        """Load benchmark results from directory."""
        import json
        
        results_file = self.results_dir / 'benchmark_results.json'
        
        if not results_file.exists():
            raise FileNotFoundError(f"Results file not found: {results_file}")
        
        with open(results_file, 'r') as f:
            results = json.load(f)
        
        return results
    
    def _create_visualizations(self, results: Dict) -> Dict:
        """
        Create comparison visualizations.
        
        Parameters
        ----------
        results : dict
            Benchmark results
        
        Returns
        -------
        dict
            Dictionary of figure paths
        """
        figures = {}
        vis_dir = self.results_dir / 'visualizations'
        vis_dir.mkdir(exist_ok=True)
        
        # Runtime comparison
        fig_path = vis_dir / 'runtime_comparison.png'
        self._plot_runtime_comparison(results, fig_path)
        figures['runtime'] = str(fig_path)
        
        # Memory comparison
        fig_path = vis_dir / 'memory_comparison.png'
        self._plot_memory_comparison(results, fig_path)
        figures['memory'] = str(fig_path)
        
        # Accuracy comparison (if available)
        if self._has_accuracy_data(results):
            fig_path = vis_dir / 'accuracy_comparison.png'
            self._plot_accuracy_comparison(results, fig_path)
            figures['accuracy'] = str(fig_path)
        
        return figures
    
    def _plot_runtime_comparison(self, results: Dict, output_path: Path):
        """Plot runtime comparison."""
        # Extract runtime data
        pipelines = []
        runtimes = []
        
        for pid, data in results.items():
            if isinstance(data, dict) and 'runtime' in data:
                pipelines.append(data.get('pipeline', f'Pipeline {pid}'))
                runtimes.append(data['runtime'] / 60)  # Convert to minutes
        
        # Create plot
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.barh(pipelines, runtimes, color=sns.color_palette("husl", len(pipelines)))
        
        ax.set_xlabel('Runtime (minutes)', fontsize=12)
        ax.set_title('Pipeline Runtime Comparison', fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)
        
        # Add value labels
        for i, (bar, runtime) in enumerate(zip(bars, runtimes)):
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height()/2,
                   f'{runtime:.1f} min',
                   ha='left', va='center', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Created runtime comparison: {output_path}")
    
    def _plot_memory_comparison(self, results: Dict, output_path: Path):
        """Plot memory usage comparison."""
        # For now, create placeholder
        # In actual implementation, would extract memory data from results
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        pipelines = [f'Pipeline {pid}' for pid in results.keys()]
        memory = [np.random.uniform(8, 32) for _ in pipelines]  # Placeholder
        
        bars = ax.barh(pipelines, memory, color=sns.color_palette("husl", len(pipelines)))
        ax.set_xlabel('Peak Memory (GB)', fontsize=12)
        ax.set_title('Pipeline Memory Usage', fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Created memory comparison: {output_path}")
    
    def _plot_accuracy_comparison(self, results: Dict, output_path: Path):
        """Plot accuracy metrics comparison."""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Placeholder accuracy data
        pipelines = [f'Pipeline {pid}' for pid in results.keys()]
        f1_scores = [np.random.uniform(0.7, 0.95) for _ in pipelines]
        
        bars = ax.barh(pipelines, f1_scores, color=sns.color_palette("husl", len(pipelines)))
        ax.set_xlabel('F1 Score', fontsize=12)
        ax.set_xlim(0, 1.0)
        ax.set_title('Pipeline Accuracy (F1 Score)', fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Created accuracy comparison: {output_path}")
    
    def _has_accuracy_data(self, results: Dict) -> bool:
        """Check if results contain accuracy data."""
        # Placeholder - would check for actual accuracy metrics
        return False
    
    def _generate_html_report(self, results: Dict, figures: Dict, output_file: str):
        """Generate HTML report."""
        html_template = """
<!DOCTYPE html>
<html>
<head>
    <title>RAPTOR Benchmark Report</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        h1 { margin: 0; font-size: 2.5em; }
        h2 { color: #333; border-bottom: 2px solid #667eea; padding-bottom: 10px; }
        .section {
            background: white;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .figure {
            text-align: center;
            margin: 20px 0;
        }
        .figure img {
            max-width: 100%;
            border-radius: 5px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #667eea;
            color: white;
        }
        tr:hover { background-color: #f5f5f5; }
        .footer {
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>ðŸ¦– RAPTOR Benchmark Report</h1>
        <p>RNA-seq Analysis Pipeline Comparison</p>
    </div>
    
    <div class="section">
        <h2>Summary</h2>
        <p>Comparison of {{ n_pipelines }} RNA-seq analysis pipelines.</p>
        <p>Generated on: {{ date }}</p>
    </div>
    
    <div class="section">
        <h2>Runtime Comparison</h2>
        <div class="figure">
            <img src="{{ runtime_fig }}" alt="Runtime Comparison">
        </div>
    </div>
    
    <div class="section">
        <h2>Memory Usage</h2>
        <div class="figure">
            <img src="{{ memory_fig }}" alt="Memory Comparison">
        </div>
    </div>
    
    <div class="section">
        <h2>Results Summary</h2>
        <table>
            <tr>
                <th>Pipeline</th>
                <th>Status</th>
                <th>Runtime</th>
            </tr>
            {% for pid, data in results.items() %}
            <tr>
                <td>Pipeline {{ pid }}</td>
                <td>{{ data.status }}</td>
                <td>{{ "%.1f"|format(data.runtime / 60) }} min</td>
            </tr>
            {% endfor %}
        </table>
    </div>
    
    <div class="footer">
        <p>Generated by RAPTOR v2.0 | University of Namur, Belgium</p>
        <p><a href="https://github.com/AyehBlk/RAPTOR">github.com/AyehBlk/RAPTOR</a></p>
    </div>
</body>
</html>
        """
        
        import datetime
        
        template = Template(html_template)
        html_content = template.render(
            n_pipelines=len(results),
            date=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            runtime_fig=figures.get('runtime', ''),
            memory_fig=figures.get('memory', ''),
            results=results
        )
        
        with open(output_file, 'w') as f:
            f.write(html_content)
        
        logger.info(f"HTML report saved: {output_file}")
    
    def _generate_pdf_report(self, results: Dict, figures: Dict, output_file: str):
        """Generate PDF report (placeholder)."""
        logger.warning("PDF generation not yet implemented. Use HTML format.")
    
    def _generate_markdown_report(self, results: Dict, figures: Dict, output_file: str):
        """Generate Markdown report."""
        import datetime
        
        md_content = f"""# RAPTOR Benchmark Report ðŸ¦–

Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary

Comparison of {len(results)} RNA-seq analysis pipelines.

## Runtime Comparison

![Runtime Comparison]({figures.get('runtime', '')})

## Memory Usage

![Memory Usage]({figures.get('memory', '')})

## Results

| Pipeline | Status | Runtime |
|----------|--------|---------|
"""
        
        for pid, data in results.items():
            if isinstance(data, dict):
                status = data.get('status', 'unknown')
                runtime = data.get('runtime', 0) / 60
                md_content += f"| Pipeline {pid} | {status} | {runtime:.1f} min |\n"
        
        md_content += "\n---\n\nGenerated by RAPTOR v2.0 | University of Namur, Belgium\n"
        
        with open(output_file, 'w') as f:
            f.write(md_content)
        
        logger.info(f"Markdown report saved: {output_file}")


# =============================================================================
# Profile Report Generation
# =============================================================================

def generate_profile_report(profile: Dict, recommendation: Dict, output_file: str):
    """
    Generate profile and recommendation report.
    
    Parameters
    ----------
    profile : dict
        Data profile
    recommendation : dict
        Pipeline recommendation
    output_file : str
        Output HTML file
    """
    html_template = """
<!DOCTYPE html>
<html>
<head>
    <title>RAPTOR Data Profile & Recommendation</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        h1 { margin: 0; font-size: 2.5em; }
        h2 { color: #333; border-bottom: 2px solid #667eea; padding-bottom: 10px; }
        .section {
            background: white;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .recommendation {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            font-size: 1.2em;
        }
        .recommendation h2 { color: white; border-bottom: 2px solid white; }
        .metric { margin: 10px 0; }
        .metric-label { font-weight: bold; color: #667eea; }
        .reasoning { margin: 5px 0; padding-left: 20px; }
        ul { list-style-type: none; padding: 0; }
        li:before { content: "âœ… "; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ðŸ¦– RAPTOR Data Profile</h1>
        <p>Intelligent Pipeline Recommendation</p>
    </div>
    
    <div class="recommendation">
        <h2>ðŸŽ¯ Recommended Pipeline</h2>
        <h3>{{ rec.primary.pipeline_name }}</h3>
        <p>Score: {{ "%.1f"|format(rec.primary.score) }}/200</p>
        <h4>Reasoning:</h4>
        <ul>
        {% for reason in rec.primary.reasoning[:5] %}
            <li>{{ reason }}</li>
        {% endfor %}
        </ul>
    </div>
    
    <div class="section">
        <h2>Data Characteristics</h2>
        
        <div class="metric">
            <span class="metric-label">Dataset:</span> 
            {{ profile.design.n_genes }} genes Ã— {{ profile.design.n_samples }} samples
        </div>
        
        <div class="metric">
            <span class="metric-label">Library Size CV:</span> 
            {{ "%.2f"|format(profile.library_stats.cv) }}
        </div>
        
        <div class="metric">
            <span class="metric-label">Zero Inflation:</span> 
            {{ "%.1f"|format(profile.count_distribution.zero_pct) }}%
        </div>
        
        <div class="metric">
            <span class="metric-label">BCV:</span> 
            {{ "%.2f"|format(profile.biological_variation.bcv) }}
        </div>
        
        <div class="metric">
            <span class="metric-label">Sequencing Depth:</span> 
            {{ profile.sequencing.depth_category }}
        </div>
        
        <div class="metric">
            <span class="metric-label">Data Difficulty:</span> 
            {{ profile.summary.difficulty }}
        </div>
    </div>
    
    <div class="footer">
        <p>Generated by RAPTOR v2.0 | University of Namur, Belgium</p>
    </div>
</body>
</html>
    """
    
    template = Template(html_template)
    html_content = template.render(profile=profile, rec=recommendation)
    
    with open(output_file, 'w') as f:
        f.write(html_content)
    
    logger.info(f"Profile report saved: {output_file}")


if __name__ == '__main__':
    print("RAPTOR Report Generator")
    print("======================")
    print("\nGenerates comparison reports with visualizations.")
    print("Use via CLI: raptor report --results benchmark_results/")
