# SPDX-License-Identifier: MIT
# Copyright © 2025 Bijan Mousavi

"""Builds the documentation page for testing artifacts.

This module defines `TestArtifactPage`, which implements the `StandardArtifactPage`
base class to parse and display results from pytest, coverage.py,
pytest-benchmark, and Hypothesis.

Module Constants:
    TEST_DIR: The directory where test artifact logs are stored.
    ORDER: A list defining the presentation order of artifacts on the page.
    BLURBS: A dictionary mapping artifact filenames to their descriptions
        and a summary of a "good" result.
"""

from __future__ import annotations

import json
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Iterable

from scripts.docs_builder.artifacts_pages.base import StandardArtifactPage
from scripts.docs_builder.artifacts_pages.base import anchor_for
from scripts.docs_builder.artifacts_pages.base import Bullet
from scripts.docs_builder.helpers import copy_tree_into_docs
from scripts.docs_builder.helpers import indent

TEST_DIR = Path("artifacts/test")


# ORDER = ["junit.xml", "htmlcov", "benchmarks", "hypothesis"]
ORDER = ["junit.xml", "htmlcov"]

BLURBS = {
    "junit.xml": ("JUnit-style test report (pytest).", "0 failures / 0 errors; all tests passed."),
    "htmlcov": ("HTML coverage report (coverage.py).", "Meets coverage threshold; no critical files missing."),
    "benchmarks": ("Benchmark outputs (pytest-benchmark, file storage).", "No regressions beyond thresholds."),
    "hypothesis": ("Hypothesis database (saved examples/health).", "No falsifying examples saved."),
}


def _parse_junit(fp: Path) -> dict:
    """Safely parses a JUnit XML file to extract test suite statistics.

    Args:
        fp: The path to the `junit.xml` file.

    Returns:
        A dictionary containing test statistics: 'tests', 'failures', 'errors',
        'skipped', 'time', and a boolean 'ok' status. Returns a dictionary
        with zero values if parsing fails for any reason.
    """
    out = {"tests": 0, "failures": 0, "errors": 0, "skipped": 0, "time": 0.0, "ok": False}
    try:
        root = ET.parse(fp).getroot()
        suites = root.findall(".//test") if root.tag.endswith("test") else [root]
        for s in suites:
            out["tests"] += int(s.attrib.get("tests", "0"))
            out["failures"] += int(s.attrib.get("failures", "0"))
            out["errors"] += int(s.attrib.get("errors", "0"))
            out["skipped"] += int(s.attrib.get("skipped", "0"))
            try:
                out["time"] += float(s.attrib.get("time", "0"))
            except ValueError:
                pass
        out["ok"] = out["failures"] == 0 and out["errors"] == 0 and out["tests"] > 0
    except (ET.ParseError, FileNotFoundError):
        pass
    return out


def _read_cov_pct(htmlcov_dir: Path) -> float | None:
    """Reads the total coverage percentage from a `status.json` file.

    This file is generated by `coverage.py` in the HTML report directory.

    Args:
        htmlcov_dir: The path to the `htmlcov` directory.

    Returns:
        The total coverage percentage as a float, or `None` if the file
        cannot be read or parsed.
    """
    try:
        data = json.loads((htmlcov_dir / "status.json").read_text(encoding="utf-8"))
        totals = data.get("totals") or {}
        pc = totals.get("percent_covered") or totals.get("percent_covered_display")
        return float(pc) if pc is not None else None
    except (OSError, json.JSONDecodeError):
        return None


def _count_files(root: Path, exts: tuple[str, ...] | None = None) -> int:
    """Recursively counts files in a directory, with an optional extension filter.

    Args:
        root: The directory to search.
        exts: An optional tuple of lowercase file extensions to filter by
            (e.g., ('.json', '.txt')).

    Returns:
        The total count of matching files. Returns 0 if the root path
        does not exist.
    """
    if not root.is_dir():
        return 0
    total = 0
    for p in root.rglob("*"):
        if p.is_file():
            if exts is None or p.suffix.lower() in exts:
                total += 1
    return total


class TestArtifactPage(StandardArtifactPage):
    """Generates the documentation page for test artifacts."""

    out_md = Path("artifacts/test.md")

    def title(self) -> str:
        """Returns the main title for the test artifacts page."""
        return "Test Artifacts"

    def intro(self) -> str:
        """Returns the introductory text for the test artifacts page."""
        return (
            "Outputs from test run (kept under `artifacts/test/` to avoid polluting the repo root).\n\n"
            "- **junit.xml** — produced by `pytest --junit-xml`\n"
            "- **htmlcov/** — produced by `coverage.py` via `--cov-report=html`\n"
            "- **benchmarks/** — created when `pytest-benchmark` is available and `ENABLE_BENCH=1`\n"
            "- **hypothesis/** — Hypothesis DB via `HYPOTHESIS_DATABASE_DIRECTORY`\n"
        )

    def iter_items(self) -> Iterable[tuple[str, Path]]:
        """Yields the test artifact files to be documented."""
        return [(name, TEST_DIR / name) for name in ORDER]

    def bullet_for(self, label: str, path: Path, content: str) -> Bullet:
        """Builds a detailed summary bullet for a specific test artifact.

        This method uses helper functions to parse the artifact (or its
        directory) and format a summary metric, such as test counts,
        coverage percentage, or number of stored files.

        Args:
            label: The name of the artifact (e.g., "junit.xml").
            path: The path to the artifact file or directory.
            content: The raw string content of the artifact file (unused for dirs).

        Returns:
            A `Bullet` object populated with a title, summary, and usage guidance.
        """
        desc, good_template = BLURBS.get(label, ("Report", ""))
        extra = ""

        if label == "junit.xml":
            s = _parse_junit(path)
            extra = f" — {s['tests']} tests; {s['failures']} failures; {s['errors']} errors; {s['skipped']} skipped"
        elif label == "htmlcov":
            pct = _read_cov_pct(TEST_DIR / "htmlcov")
            extra = f" — total coverage ~{pct:.1f}%" if pct is not None else " — coverage % unavailable"
        elif label == "benchmarks":
            n = _count_files(TEST_DIR / "benchmarks", exts=(".json",))
            if n == 0:
                n = _count_files(TEST_DIR / "benchmarks")
            extra = f" — stored runs: {n}" if n else " — no runs stored"
        elif label == "hypothesis":
            n = _count_files(TEST_DIR / "hypothesis")
            extra = f" — files: {n}" if n else " — empty DB"

        title = f"[{label}](#{anchor_for(label)}) — {desc}"
        good_line = f"{good_template}{extra}"

        usage = {
            "junit.xml": "Feed to CI/badges; inspect failing tests and timings.",
            "htmlcov": "Open the HTML report; set/validate a coverage threshold in CI.",
            "benchmarks": "Compare against previous runs; fail CI on regressions.",
            "hypothesis": "Review saved counterexamples; refine strategies or invariants.",
        }.get(label)

        return Bullet(title=title, good=good_line, usage=usage)

    def detail_for(self, label: str, path: Path, content: str) -> str:
        """Creates the detailed description for a given test artifact.

        This method generates a Markdown 'info' block with details about the
        artifact. It also performs special actions for certain artifacts:
        - For 'htmlcov', it copies the entire HTML report into the built
          documentation site and adds a direct link to the `index.html`.
        - For 'junit.xml', it includes a formatted summary of the test results.
        - For 'benchmarks' and 'hypothesis', it provides context on how these
          artifacts are configured and used.

        Args:
            label: The name of the artifact (e.g., "junit.xml").
            path: The path to the artifact file or directory.
            content: The raw string content of the artifact file (unused for dirs).

        Returns:
            A formatted Markdown string for the artifact's detail section.
        """
        desc, good = BLURBS.get(label, ("Report", ""))
        lines = [f"**What it is:** {desc}", f"**What good looks like:** {good}"]

        if label == "junit.xml":
            s = _parse_junit(path)
            lines.append(
                f"**Summary:** total {s['tests']}, failures {s['failures']}, "
                f"errors {s['errors']}, skipped {s['skipped']}, time ~{s['time']:.2f}s"
            )
        elif label == "htmlcov":
            copy_tree_into_docs(TEST_DIR / "htmlcov", "artifacts/test/htmlcov")
            if (TEST_DIR / "htmlcov" / "index.html").exists():
                lines.append('[Open HTML coverage report](test/htmlcov/index.html){: target="_blank" rel="noopener" }')
        elif label == "benchmarks":
            lines.append(
                "Stored by `pytest-benchmark` when available and `ENABLE_BENCH=1` "
                "(see Makefile). Files are saved under `artifacts/test/benchmarks/` "
                "using file storage."
            )
        elif label == "hypothesis":
            lines.append(
                "Hypothesis uses `HYPOTHESIS_DATABASE_DIRECTORY=artifacts/test/hypothesis` "
                "so examples and health data stay contained."
            )

        return '!!! info "About this artifact"\n\n' + indent("\n".join(lines) + "\n")
