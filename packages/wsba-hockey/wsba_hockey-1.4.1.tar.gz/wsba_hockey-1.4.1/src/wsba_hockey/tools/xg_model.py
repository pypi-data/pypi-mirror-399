import os
import pandas as pd
import numpy as np
import xgboost as xgb
import scipy.sparse as sp
import wsba_hockey.wsba_main as wsba
import wsba_hockey.tools.scraping as scraping
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.metrics import roc_curve, auc
from typing import Literal, Union

### XG_MODEL FUNCTIONS ###
# Provided in this file are functions vital to the goal prediction model in the WSBA Hockey Python package. #

## GLOBAL VARIABLES ##

target = "is_goal"
continuous = ['event_distance',
            'event_angle',
            'distance_from_last',
            'angle_from_last',
            'seconds_since_last',
            'speed_from_last',
            'speed_of_angle_from_last'
            ]
boolean = ['wrist',
        'deflected',
        'tip-in',
        'slap',
        'backhand',
        'snap',
        'wrap-around',
        'poke',
        'bat',
        'cradle',
        'between-legs',
        'other-shot',
        'prior_same',
        'prior_shot-on-goal',
        'prior_missed-shot',
        'prior_blocked-shot',
        'prior_giveaway',
        'prior_takeaway',
        'prior_hit',
        'prior_faceoff',
        'strength_3v3',
        'strength_3v4',
        'strength_3v5',
        'strength_4v3',
        'strength_4v4',
        'strength_4v5',
        'strength_4v6',
        'strength_5v3',
        'strength_5v4',
        'strength_5v5',
        'strength_5v6',
        'strength_6v4',
        'strength_6v5',
        'empty_net',
        'offwing',
        'rush',
        'rebound'
    ]

events = ['faceoff','hit','giveaway','takeaway','blocked-shot','missed-shot','shot-on-goal','goal']
shot_types = ['wrist','deflected','tip-in','slap','backhand','snap','wrap-around','poke','bat','cradle','between-legs']
fenwick_events = ['missed-shot','shot-on-goal','goal']
strengths = ['3v3',
            '3v4',
            '3v5',
            '4v3',
            '4v4',
            '4v5',
            '4v6',
            '5v3',
            '5v4',
            '5v5',
            '5v6',
            '6v4',
            '6v5']

dir = os.path.dirname(os.path.realpath(__file__))
roster_path = os.path.join(dir,'rosters\\nhl_rosters.csv')
xg_model_path = os.path.join(dir,'xg_model\\wsba_xg.json')
test_path = os.path.join(dir,'xg_model\\testing\\xg_model_training_runs.csv')
cv_path = os.path.join(dir,'xg_model\\testing\\xg_model_cv_runs.csv')
metric_path = os.path.join(dir,'xg_model\\metrics')

def fix_players(pbp):
    #Add/fix player info for shooters and goaltenders

    #Find players that don't have a handness
    try:
        find = pbp.loc[(pbp['event_type'].isin(fenwick_events))&(pbp['event_player_1_hand'].isna()),'event_player_1_id'].drop_duplicates().to_list()
    except:
        #If this fails then the column doesn't exist and all players are included in the search
        find = pbp['event_player_1_id'].drop_duplicates().to_list()
    
    #Load roster and locate players
    if not find:
        pass
    else:
        print('Adding player info to pbp...')
        roster = pd.read_csv(roster_path)
        roster = roster.loc[roster['player_id'].isin(find)].drop_duplicates(['player_id'])[['player_name','player_id','handedness']]

        #Some players are missing from the roster file (generally in newer seasons); add these manually
        miss = pbp.loc[((pbp['event_player_1_id'].isin(find))&(~pbp['event_player_1_id'].isin(roster['player_id']))),'event_player_1_id'].drop_duplicates().dropna().to_list()
        if miss:
            add = wsba.nhl_scrape_player_info(miss)[['player_name','player_id','handedness']]
            roster = pd.concat([roster,add]).reset_index(drop=True)

        #Conversion dict
        roster['player_id'] = roster['player_id'].astype(str)
        roster_dict = roster.set_index('player_id').to_dict()['handedness']

        #Fix event goalies
        pbp['event_goalie_id'] = np.where(pbp['event_team_venue']=='away',pbp['home_goalie_id'],pbp['away_goalie_id'])
            
        #Add hands
        pbp['event_player_1_hand'] = pbp['event_player_1_id'].astype(str).str.replace('.0','').replace(roster_dict).replace('nan',None)

    return pbp

def prep_xG_data(data):
    #Prep data for xG training and calculation
    data = fix_players(data)

    #Informal groupby
    data.sort_values(by=['season','game_id','period','seconds_elapsed','event_num'],inplace=True)

    #Recalibrate times series data with current data
    data['seconds_since_last'] = data['seconds_elapsed'] - data['seconds_elapsed'].shift(1) 
    #Prevent leaking between games by setting value to zero when no time has occured in game
    data["seconds_since_last"] = np.where(data['seconds_elapsed']==0,0,data['seconds_since_last'])

    #Create last event columns
    data["event_team_last"] = data['event_team_abbr'].shift(1)
    data["event_type_last"] = data['event_type'].shift(1)
    data["x_adj_last"] = data['x_adj'].shift(1)
    data["y_adj_last"] = data['y_adj'].shift(1)
    data["zone_code_last"] = data['zone_code'].shift(1)

    data.sort_values(['season','game_id','period','seconds_elapsed','event_num'],inplace=True)    

    #Contextual Data (for score state minimize the capture to four goals)
    data['score_state'] = np.where(data['away_team_abbr']==data['event_team_abbr'],data['away_score']-data['home_score'],data['home_score']-data['away_score'])
    data['score_state'] = np.where(data['score_state']>4,4,data['score_state'])
    data['score_state'] = np.where(data['score_state']<-4,-4,data['score_state'])

    data['strength_diff'] = np.where(data['away_team_abbr']==data['event_team_abbr'],data['away_skaters']-data['home_skaters'],data['home_skaters']-data['away_skaters'])
    data['strength_state_venue'] = data['away_skaters'].astype(str)+'v'+data['home_skaters'].astype(str)
    data['distance_from_last'] = np.sqrt((data['x_adj'] - data['x_adj_last'])**2 + (data['y_adj'] - data['y_adj_last'])**2)
    data['angle_from_last'] = np.degrees(np.arctan2(abs(data['y_adj'] - data['y_adj_last']), abs(89 - (data['x_adj']-data['x_adj_last']))))

    #Event speeds
    data['speed_from_last'] = np.where(data['seconds_since_last']==0,0,data['distance_from_last']/data['seconds_since_last'])
    data['speed_of_angle_from_last'] = np.where(data['seconds_since_last']==0,0,data['angle_from_last']/data['seconds_since_last'])

    #Rush and rebounds are labelled
    data['rush'] = np.where((data['event_type'].isin(fenwick_events))&(data['zone_code_last'].isin(['N','D']))&(data['x_adj']>25)&(data['seconds_since_last']<=5),1,0)
    data['rebound'] = np.where((data['event_type'].isin(fenwick_events))&(data['event_type_last'].isin(fenwick_events))&(data['seconds_since_last']<=2),1,0)

    #Create boolean variables
    data["is_goal"]=(data['event_type']=='goal').astype(int)
    data["is_home"]=(data['home_team_abbr']==data['event_team_abbr']).astype(int)

    #Boolean variables for shot types and prior events
    for shot in shot_types:
        data[shot] = (data['shot_type']==shot).astype(int)
    for event in events[:-1]:
        data[f'prior_{event}'] = (data['event_type_last']==event).astype(int)
    
    data['other-shot'] = (~data['shot_type'].isin(shot_types)).astype(int)
    data['prior_same'] = (data['event_team_last']==data['event_team_abbr']).astype(int)
    
    #Strength boolean (used instead of 'strength_diff' in order to more usefully distinguish between strength states)
    for strength in strengths:
        data[f'strength_{strength}'] = (data['strength_state']==strength).astype(int)

    #Misc variables
    data['empty_net'] = np.where((data['event_type'].isin(fenwick_events))&(data['event_goalie_id'].isna()),1,0)
    data['offwing'] = np.where(((data['y_adj']<0)&(data['event_player_1_hand']=='L'))|((data['y_adj']>=0)&(data['event_player_1_hand']=='R')),1,0)
    
    #Return: pbp data prepared to train and calculate the xG model
    return data

def wsba_xG(pbp, model_type: Literal['bayesian', 'frequentist'] = 'frequentist', states = False, hypertune = False, train = False, test_path = test_path, cv_path = cv_path, model_path = xg_model_path, train_runs = 20, cv_runs = 20):
    #Train and calculate the WSBA Expected Goals model
    
    #Add index for future merging
    pbp['event_index'] = pbp.index

    #Initialize xG column for all events
    pbp['xG'] = 0.0

    #Recalibrate coordinates
    pbp = scraping.adjust_coords(pbp)

    #Recalculate stat states if speciifed
    if states:
        for venue in ['away','home']:
            pbp[f'{venue}_score'] = ((pbp['event_team_venue']==venue)&(pbp['event_type']=='goal')).groupby(pbp['game_id']).cumsum().shift(1)
            pbp[f'{venue}_corsi'] = ((pbp['event_team_venue']==venue)&(pbp['event_type'].isin(['blocked-shot','missed-shot','shot-on-goal','goal']))).groupby(pbp['game_id']).cumsum().shift(1)
            pbp[f'{venue}_fenwick'] = ((pbp['event_team_venue']==venue)&(pbp['event_type'].isin(['missed-shot','shot-on-goal','goal']))).groupby(pbp['game_id']).cumsum().shift(1)
            pbp[f'{venue}_penalties'] = ((pbp['event_team_venue']==venue)&(pbp['event_type']=='penalty')).groupby(pbp['game_id']).cumsum().shift(1)

    #Fix strengths
    pbp['strength_state'] = np.where((pbp['season_type']==3)&(pbp['period']>4),
                                    (np.where(pbp['event_team_abbr']==pbp['away_team_abbr'],
                                            pbp['away_skaters'].astype(str)+"v"+pbp['home_skaters'].astype(str),
                                            pbp['home_skaters'].astype(str)+"v"+pbp['away_skaters'].astype(str))),
                                    pbp['strength_state'])

    #Prep data and filter shot events
    data = prep_xG_data(pbp.loc[(pbp['event_type'].isin(events))&(pbp['strength_state'].isin(strengths))&(pbp['x'].notna())&(pbp['y'].notna())])
    data = data.loc[data['event_type'].isin(fenwick_events)]
    
    if model_type == 'bayesian':
        NotImplementedError('PyMC Model in Development...')
    else:
        dfs = []
        for empty_net in [False, True]:
            #Two sub-models: Those on a goaltender and those on an empty net
            training = data.loc[data['empty_net']==1 if empty_net else data['empty_net']==0]

            #Calibrate paths
            if empty_net:
                test_path = test_path.replace('runs','runs_en')
                cv_path = cv_path.replace('runs','runs_en')
                model_path = model_path.replace('wsba_xg.json', 'wsba_xg_en.json')
            else:
                test_path = test_path
                cv_path = cv_path
                model_path = model_path

            #Convert to sparse
            data_sparse = sp.csr_matrix(training[[target]+continuous+boolean])
            is_goal_vect = data_sparse[:,0].A
            predictors = data_sparse[:,1:]

            #XGB DataModel
            xgb_matrix = xgb.DMatrix(data=predictors,label=is_goal_vect,feature_names=(continuous+boolean))

            if train:
                print('### XGBOOST MODEL TRAINING ###')
                if hypertune:
                    # Number of runs
                    run_num = train_runs

                    # DataFrames to store results
                    best_df = pd.DataFrame(columns=["max_depth", "eta", "gamma", "subsample", "colsample_bytree", "min_child_weight", "max_delta_step"])
                    best_ll = pd.DataFrame(columns=["ll", "ll_rounds", "auc", "auc_rounds", "seed"])

                    print('### HYPERTUNING ###')
                    # Loop
                    for i in range(run_num):
                        print(f"## LOOP: {i+1} ##")
                        
                        param = {
                            "objective": "binary:logistic",
                            "eval_metric": ["logloss", "auc"],
                            "max_depth": 6,
                            "eta": np.random.uniform(0.06, 0.11),
                            "gamma": np.random.uniform(0.06, 0.12),
                            "subsample": np.random.uniform(0.76, 0.84),
                            "colsample_bytree": np.random.uniform(0.76, 0.8),
                            "min_child_weight": np.random.randint(5, 23),
                            "max_delta_step": np.random.randint(4, 9)
                        }
                        
                        # Cross-validation
                        seed = np.random.randint(0, 10000)
                        np.random.seed(seed)
                        
                        cv_results = xgb.cv(
                            params=param,
                            dtrain=xgb_matrix,
                            num_boost_round=1000,
                            nfold=5,
                            early_stopping_rounds=25,
                            metrics=["logloss", "auc"],
                            seed=seed
                        )
                        
                        # Record results
                        best_df.loc[i] = param
                        best_ll.loc[i] = [
                            cv_results["test-logloss-mean"].min(),
                            cv_results["test-logloss-mean"].idxmin(),
                            cv_results["test-auc-mean"].max(),
                            cv_results["test-auc-mean"].idxmax(),
                            seed
                        ]

                    # Combine results
                    best_all = pd.concat([best_df, best_ll], axis=1).dropna()

                    # Arrange to get best run
                    best_all = best_all.sort_values(by="auc", ascending=False)

                    best_all.to_csv(test_path,index=False)

                    # Final parameters
                    param_7_EV = {
                        "objective": "binary:logistic",
                        "eval_metric": ["logloss", "auc"],
                        "gamma": best_all['gamma'].iloc[0],
                        "subsample": best_all['subsample'].iloc[0],
                        "max_depth": best_all['max_depth'].iloc[0],
                        "colsample_bytree": best_all['colsample_bytree'].iloc[0],
                        "min_child_weight": best_all['min_child_weight'].iloc[0],
                        "max_delta_step": best_all['max_delta_step'].iloc[0],
                    }

                    # CV rounds Loop
                    run_num = cv_runs
                    cv_test = pd.DataFrame(columns=["AUC_rounds", "AUC", "LL_rounds", "LL", "seed"])

                    print('### CROSS-VALIDATION ###')
                    for i in range(run_num):
                        print(f"## LOOP: {i+1} ##")
                        
                        seed = np.random.randint(0, 10000)
                        np.random.seed(seed)
                        
                        cv_rounds = xgb.cv(
                            params=param_7_EV,
                            dtrain=xgb_matrix,
                            num_boost_round=1000,
                            nfold=5,
                            early_stopping_rounds=25,
                            metrics=["logloss", "auc"],
                            seed=seed
                        )
                        
                        # Record results
                        cv_test.loc[i] = [
                            cv_rounds["test-auc-mean"].idxmax(),
                            cv_rounds["test-auc-mean"].max(),
                            cv_rounds["test-logloss-mean"].idxmin(),
                            cv_rounds["test-logloss-mean"].min(),
                            seed
                        ]

                    # Clean results and sort to find the number of rounds to use and seed
                    cv_final = cv_test.sort_values(by="AUC", ascending=False)
                    cv_final.to_csv(cv_path,index=False)
                else:
                    # Load previous parameters
                    best_all = pd.read_csv(test_path)
                    cv_final = pd.read_csv(cv_path)

                    print('Loaded hyperparameters...')
                    # Final parameters
                    param_7_EV = {
                        "objective": "binary:logistic",
                        "eval_metric": ["logloss", "auc"],
                        "gamma": best_all['gamma'].iloc[0],
                        "subsample": best_all['subsample'].iloc[0],
                        "max_depth": best_all['max_depth'].iloc[0],
                        "colsample_bytree": best_all['colsample_bytree'].iloc[0],
                        "min_child_weight": best_all['min_child_weight'].iloc[0],
                        "max_delta_step": best_all['max_delta_step'].iloc[0],
                    }

                print('Training model...')
                seed = int(cv_final['seed'].iloc[0])
                np.random.seed(seed)
                model = xgb.train(
                    params=param_7_EV,
                    dtrain=xgb_matrix,
                    num_boost_round=int(cv_final['AUC_rounds'].iloc[0]),
                    verbose_eval=2,
                )
                
                #Save model
                model.save_model(model_path)
                
            else:
                #Load model
                model = xgb.Booster()
                model.load_model(model_path)

                if len(training) > 0:
                    #Predict xG for fenwick shots
                    training['xG'] = model.predict(xgb_matrix)

                dfs.append(training)

        if not train:
            xg_data = pd.concat(dfs)

            #Add xG columns
            for col in xg_data.columns:
                if col not in pbp.columns:
                    pbp[col] = np.nan

            pbp.loc[xg_data.index, xg_data.columns] = xg_data

            #Return: PBP dataframe with xG columns
            pbp_xg = pbp.sort_values(by=['event_index','season','game_id','period','seconds_elapsed','event_num'])

            return pbp_xg

def feature_importance(model_path = xg_model_path):
    print('Feature importance for WSBA xG Model...')
    
    for en in [False, True]:
        model = xgb.Booster()
        model.load_model(model_path if not en else model_path.replace('.json', '_en.json'))

        fi = pd.DataFrame(model.get_score(importance_type='gain').items(), columns=['feature','gain']).sort_values("gain", ascending=False)
        fi['gain'] /= fi['gain'].sum()

        fig, ax = plt.subplots(figsize=(10, 7))
        ax.barh(fi['feature'], fi['gain'])
        ax.invert_yaxis()

        ax.set_xlabel('Gain')
        ax.set_title(f'WSBA xG {'(Empty Net) ' if en else ''}Feature Importance')

        plt.savefig(os.path.join(metric_path,f'feature_importance{'_en' if en else ''}.png'),bbox_inches='tight')

def roc_auc_curve(pbp):
    print('ROC-AUC Curve for WSBA xG Model...')

    if 'xG' in pbp.columns:
        data = pbp.loc[(pbp['event_type'].isin(fenwick_events))&(pbp['strength_state'].isin(strengths))&(pbp['x'].notna())&(pbp['y'].notna())]
          
        fpr, tpr, _ = roc_curve(data['is_goal'], data['xG'])
        roc_auc = auc(fpr,tpr)
        
        plt.figure()
        plt.plot(fpr,tpr,label=f"ROC (AUC = {roc_auc:.4f})")
        plt.plot([0, 1], [0, 1], linestyle="--")
        plt.title(f"WSBA xG ROC Curve")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.legend(loc="lower right")
        plt.savefig(os.path.join(metric_path, f'roc_auc_curve.png'), bbox_inches='tight')
    else:
        print('No xG found for provided play-by-play data.  Apply xG model to the play-by-play data first.')

def reliability(pbp):
    print('Reliability for WSBA xG Model...')

    if 'xG' in pbp.columns: 
        data = pbp.loc[(pbp['event_type'].isin(fenwick_events))&(pbp['strength_state'].isin(strengths))&(pbp['x'].notna())&(pbp['y'].notna())]
        fop, mpv = calibration_curve(data['is_goal'], data['xG'], strategy='uniform')

        plt.figure()
        plt.plot(mpv, fop, "s-", label="Model")
        plt.plot([0, 1], [0, 1], linestyle="--", label="Perfect calibration")
        plt.title(f"WSBA xG Reliability")
        plt.xlabel("Predicted Probability (mean)")
        plt.ylabel("Fraction of positives")
        plt.legend(loc="best")
        plt.savefig(os.path.join(metric_path, f'reliability.png'), bbox_inches='tight')

    else:
        print('No xG found for provided play-by-play data.  Apply xG model to the play-by-play data first.')
