# v0.4 Release Action Checklist

**Quick Reference:** Immediate tasks to complete before v0.4 release

---

## ðŸ”´ CRITICAL (Must Complete - Blocks Release)

### 1. Create Test Suite (2-3 days)

```bash
# Install pytest if not already
pip install pytest pytest-cov

# Create test files
touch tests/test_caching.py
touch tests/test_structured.py
touch tests/test_resilience.py
touch tests/test_observability.py
```

#### tests/test_caching.py
- [ ] Test `cache_control="ephemeral"` on SystemMessage
- [ ] Test `cache_control="ephemeral"` on UserMessage
- [ ] Test cache metrics: `cache_creation_input_tokens`, `cache_read_input_tokens`
- [ ] Test backward compatibility (without cache_control)

#### tests/test_structured.py
- [ ] Test `strict=True` with OpenAI (native response_format)
- [ ] Test `strict=False` (prompt injection fallback)
- [ ] Test schema validation success
- [ ] Test schema validation failure

#### tests/test_resilience.py
- [ ] Test CircuitBreaker CLOSED â†’ OPEN transition
- [ ] Test CircuitBreaker OPEN â†’ HALF_OPEN â†’ CLOSED recovery
- [ ] Test RateLimiter blocks when limit reached
- [ ] Test RateLimiter allows when under limit
- [ ] Test thread safety

#### tests/test_observability.py
- [ ] Test TracingMiddleware logs requests
- [ ] Test TracingMiddleware logs responses
- [ ] Test on_error() hook is called
- [ ] Test OpenTelemetryMiddleware (basic)

**Run tests:**
```bash
pytest tests/ -v --cov=aiclient --cov-report=html
```

---

### 2. Fix Async Error Handling (30 minutes)

**File:** `aiclient/models/chat.py`

**Line 146-150:** Add middleware error notification:

```python
# BEFORE:
except Exception as e:
    if attempt < self.max_retries and should_retry(e):
        wait_time = self.retry_delay * (2 ** attempt)
        await asyncio.sleep(wait_time)
    else:
        raise e

# AFTER:
except Exception as e:
    # Notify middleware of error
    for mw in self.middlewares:
        mw.on_error(e, self.model_name)

    if attempt < self.max_retries and should_retry(e):
        wait_time = self.retry_delay * (2 ** attempt)
        await asyncio.sleep(wait_time)
    else:
        raise e
```

**Also fix:** `stream()` and `stream_async()` methods

---

### 3. Complete OpenTelemetryMiddleware (4-6 hours)

**File:** `aiclient/observability.py`

**Current (incomplete):**
```python
def before_request(self, model: str, prompt: ...) -> ...:
    if self.tracer:
        pass  # âŒ Not implemented
    return prompt
```

**Fix to:**
```python
def before_request(self, model: str, prompt: ...) -> ...:
    if self.tracer:
        # Use context variables for thread-safety
        from contextvars import ContextVar
        import threading

        span = self.tracer.start_span("llm.generate")
        span.set_attribute("llm.model", model)
        span.set_attribute("llm.provider", "aiclient")

        # Store span in thread-local or context var
        # (Implementation details needed)

    return prompt

def after_response(self, response: ModelResponse) -> ModelResponse:
    if self.tracer and self._current_span:
        span = self._current_span
        span.set_attribute("llm.tokens.input", response.usage.input_tokens)
        span.set_attribute("llm.tokens.output", response.usage.output_tokens)
        span.set_attribute("llm.tokens.total", response.usage.total_tokens)
        span.end()
    return response
```

**Test with:**
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

# Setup
provider = TracerProvider()
provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
trace.set_tracer_provider(provider)

# Use
client = Client()
client.add_middleware(OpenTelemetryMiddleware(service_name="aiclient-test"))
```

---

### 4. Update Documentation (4-6 hours)

#### docs/features.md

Add section:

```markdown
## Prompt Caching (Cost Optimization) ðŸ’°

Reduce API costs by up to 90% using Anthropic's prompt caching.

### Usage

```python
from aiclient.data_types import SystemMessage, UserMessage

# Cache expensive system prompts
messages = [
    SystemMessage(
        content="<long_system_prompt>",
        cache_control="ephemeral"  # Cache for 5 minutes
    ),
    UserMessage(content="user query")
]

response = client.chat("claude-3-sonnet").generate(messages)

# Check cache usage
print(f"Cache read tokens: {response.usage.cache_read_input_tokens}")
print(f"New cache tokens: {response.usage.cache_creation_input_tokens}")
```

**Cost Savings:**
- First request: Pays full price, creates cache
- Subsequent requests: 90% cheaper for cached content

---

## Native Structured Outputs ðŸ“¦

Guaranteed JSON schema adherence with OpenAI's native API.

### Usage

```python
from pydantic import BaseModel

class WeatherReport(BaseModel):
    location: str
    temperature: float
    conditions: str

# 100% schema adherence (OpenAI native)
report = client.chat("gpt-4o").generate(
    "What's the weather in SF?",
    response_model=WeatherReport,
    strict=True  # â† Use native structured outputs
)

# Always valid WeatherReport, no parsing errors
assert isinstance(report, WeatherReport)
```

**Modes:**
- `strict=True`: Native API (100% adherence, OpenAI only)
- `strict=False`: Prompt injection (works with all providers)
```

---

#### docs/middleware.md

Add sections:

```markdown
## Resilience Middleware

### CircuitBreaker

Prevents cascading failures by stopping requests after threshold.

```python
from aiclient.resilience import CircuitBreaker

breaker = CircuitBreaker(
    failure_threshold=5,    # Open after 5 failures
    recovery_timeout=60.0   # Wait 60s before retry
)

client = Client()
client.add_middleware(breaker)

# After 5 failures, all requests fail immediately
# After 60s, one request is allowed (half-open state)
# If successful, circuit closes; if fails, reopens
```

**States:**
- `CLOSED`: Normal operation
- `OPEN`: Blocking all requests
- `HALF_OPEN`: Testing recovery

---

### RateLimiter

Controls request throughput to respect API rate limits.

```python
from aiclient.resilience import RateLimiter

limiter = RateLimiter(requests_per_minute=60)

client = Client()
client.add_middleware(limiter)

# Automatically sleeps when limit reached
# Uses sliding window algorithm
```

---

## Observability Middleware

### TracingMiddleware

Basic request/response logging.

```python
from aiclient.observability import TracingMiddleware
import logging

logging.basicConfig(level=logging.INFO)

tracer = TracingMiddleware()
client.add_middleware(tracer)

# Logs:
# INFO: Trace[abc-123]: Request to gpt-4o
# INFO: Trace[...]: Response from openai - Tokens: 150
```

---

### OpenTelemetryMiddleware

Enterprise-grade distributed tracing.

```python
from aiclient.observability import OpenTelemetryMiddleware
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Setup OpenTelemetry
provider = TracerProvider()
provider.add_span_processor(
    BatchSpanProcessor(OTLPSpanExporter(endpoint="http://localhost:4317"))
)
trace.set_tracer_provider(provider)

# Use with aiclient
client = Client()
client.add_middleware(OpenTelemetryMiddleware(service_name="my-app"))

# Traces appear in Jaeger, Zipkin, etc.
```

**Span Attributes:**
- `llm.model`: Model name
- `llm.provider`: Provider name
- `llm.tokens.input`: Input token count
- `llm.tokens.output`: Output token count
- `llm.tokens.total`: Total tokens
```

---

#### README.md

Update "Key Features" section:

```markdown
## Key Features (v0.4)

- ðŸ¦„ **Unified Interface**: Swap between OpenAI, Anthropic, Google, xAI, and Ollama seamlessly.
- âš¡ **Async & Sync**: Native asyncio support for high-performance apps.
- ðŸ›¡ï¸ **Resilient**: Automatic retries, circuit breakers, rate limiting.
- ðŸ’° **Cost Optimized**: Prompt caching reduces costs by up to 90%.
- ðŸ“¦ **Structured Outputs**: Native JSON schema with 100% adherence.
- ðŸ” **Observable**: OpenTelemetry integration for distributed tracing.
- ðŸ¤– **Agent Primitives**: Built-in ReAct loop for tool-using agents.
```

---

### 5. Create CHANGELOG.md (1 hour)

```markdown
# Changelog

All notable changes to this project will be documented in this file.

## [0.4.0] - 2025-12-XX

### Added

#### Prompt Caching ðŸ’°
- Support for Anthropic's prompt caching to reduce costs by up to 90%
- `cache_control="ephemeral"` parameter on `SystemMessage` and `UserMessage`
- Cache metrics: `cache_creation_input_tokens`, `cache_read_input_tokens` in `Usage` object
- Automatic cache header injection for Anthropic provider

#### Native Structured Outputs ðŸ“¦
- OpenAI's native `response_format` API with guaranteed JSON schema adherence
- `strict=True` parameter for native structured outputs (100% schema compliance)
- Automatic fallback to prompt injection when `strict=False` (default)
- Backward compatible with existing code

#### Resilience Patterns ðŸ›¡ï¸
- `CircuitBreaker` middleware to prevent cascading failures
- `RateLimiter` middleware for request throttling
- Configurable failure thresholds and recovery timeouts
- Thread-safe implementations

#### Observability ðŸ”
- `TracingMiddleware` for basic request/response logging
- `OpenTelemetryMiddleware` for distributed tracing integration
- `on_error()` hook in middleware protocol
- Automatic span creation and attribute tracking

### Changed
- Middleware protocol now includes `on_error(error, model)` method
- `Usage` type extended with cache-related token counts
- Provider `prepare_request()` signature updated with `response_schema` and `strict` parameters

### Fixed
- Error handling now properly notifies middleware in synchronous generation

## [0.3.0] - 2024-12-XX

### Added
- Local LLM support via Ollama
- `provider:model` syntax for explicit provider selection

## [0.2.0] - 2024-11-XX

### Added
- Basic agent implementation with ReAct loop
- Tool/function calling support

## [0.1.0] - 2024-10-XX

### Added
- Initial release with OpenAI, Anthropic, Google providers
- Basic middleware support
- Streaming and async support
```

---

## ðŸŸ¡ IMPORTANT (Should Complete)

### 6. Create Examples (2-3 hours)

```bash
mkdir -p examples
```

#### examples/prompt_caching.py
```python
"""
Demonstrates 90% cost savings with prompt caching.
"""
from aiclient import Client
from aiclient.data_types import SystemMessage, UserMessage

client = Client()

# Large system prompt (expensive!)
system_prompt = """You are an expert Python developer...""" * 100

messages = [
    SystemMessage(content=system_prompt, cache_control="ephemeral"),
    UserMessage(content="Write a function to reverse a string")
]

# First call: Creates cache
response1 = client.chat("claude-3-sonnet").generate(messages)
print(f"First call - Created cache: {response1.usage.cache_creation_input_tokens} tokens")

# Second call: Uses cache (90% cheaper!)
response2 = client.chat("claude-3-sonnet").generate(messages)
print(f"Second call - Read cache: {response2.usage.cache_read_input_tokens} tokens")
print(f"Cost savings: ~90%")
```

#### examples/structured_outputs.py
#### examples/resilience_patterns.py
#### examples/observability.py

---

### 7. Setup CI/CD (2 hours)

#### .github/workflows/test.yml
```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        pip install -e ".[dev]"

    - name: Run tests
      run: |
        pytest tests/ -v --cov=aiclient --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

---

## ðŸŸ¢ NICE-TO-HAVE (Can Defer)

- [ ] Add Google prompt caching support
- [ ] Add Anthropic native structured outputs
- [ ] Improve TracingMiddleware with request ID correlation
- [ ] Create video tutorial
- [ ] Blog post announcement

---

## Verification Checklist

Before releasing v0.4.0 to PyPI:

- [ ] All tests passing: `pytest tests/ -v`
- [ ] Test coverage >80%: `pytest --cov=aiclient --cov-report=term-missing`
- [ ] Type checking passes: `mypy aiclient`
- [ ] Linting passes: `ruff check aiclient`
- [ ] Documentation complete and accurate
- [ ] CHANGELOG.md updated
- [ ] Version bumped to 0.4.0 in `pyproject.toml` (already done âœ…)
- [ ] Examples working
- [ ] CI/CD passing

---

## Quick Start (Do These First)

1. **Install dev dependencies:**
   ```bash
   pip install -e ".[dev]"
   ```

2. **Create test files:**
   ```bash
   touch tests/test_caching.py tests/test_structured.py tests/test_resilience.py tests/test_observability.py
   ```

3. **Fix async error handling:**
   - Edit `aiclient/models/chat.py` line 146

4. **Run existing tests to ensure no regressions:**
   ```bash
   pytest tests/test_client.py -v
   ```

5. **Start writing new tests** (see section 1 above)

---

## Timeline

- **Day 1:** Tests (caching, structured outputs)
- **Day 2:** Tests (resilience, observability) + Fix async
- **Day 3:** Complete OpenTelemetry + Documentation
- **Day 4:** Examples + CI/CD + Final verification

**Target Release Date:** 3-4 days from now

---

## Questions or Issues?

Refer to `V0.4_REVIEW.md` for detailed analysis and context.
