[Length-MAX] vocab_size=32000
[Length-MAX] longest_tokens(top 20):
  #01 len= 47 id=  362 tok='1930ĠrenumberingĠofĠstateĠ...
  #02 len= 47 id= 2278 tok='CourseĠonĠtheĠRiverĠThames...
  #03 len= 47 id= 3907 tok='InternationalĠUnionĠforĠCo...
  #04 len= 46 id= 8596 tok='airedĠonĠtheĠFoxĠnetworkĠi...
  #05 len= 45 id=26473 tok='whichĠassignsĠaĠnormalized...
  #06 len= 43 id= 8829 tok='amongĠadultsĠbetweenĠtheĠa...
  #07 len= 43 id=25168 tok='transitionedĠintoĠanĠextra...
  #08 len= 43 id=26552 tok='whoĠworkĠonĠcasesĠlinkedĠt...
  #09 len= 43 id=26708 tok='wonĠtheĠtossĠandĠelectedĠt...
  #10 len= 42 id=12060 tok='crewsĠfromĠtheĠUniversitie...
  #11 len= 41 id=11001 tok='centersĠonĠFBIĠspecialĠage...
  #12 len= 41 id=26362 tok='wentĠintoĠtheĠraceĠasĠreig...
  #13 len= 40 id= 2299 tok='CreditsĠadaptedĠfromĠtheĠl...
  #14 len= 40 id=21628 tok='ratedĠshowĠonĠtheĠFoxĠnetw...
  #15 len= 38 id=10751 tok='byĠtheĠRecordingĠIndustryĠ...
  #16 len= 38 id=13857 tok='erĠofĠtheĠOrderĠofĠtheĠBri...
  #17 len= 37 id= 5226 tok='NationalĠRegisterĠofĠHisto...
  #18 len= 36 id= 6739 tok='SiteĠofĠSpecialĠScientific...
  #19 len= 35 id= 3971 tok='InĠitsĠoriginalĠAmericanĠb...
  #20 len= 34 id=  972 tok='AllĠsongsĠwrittenĠandĠcomp...

[BPE] vocab_size=32000
[BPE] longest_tokens(top 20):
  #01 len= 17 id=22946 tok='ĠIntercontinental'
  #02 len= 17 id=19840 tok='Ġautobiographical'
  #03 len= 17 id=21024 tok='Ġcharacterization'
  #04 len= 17 id=28098 tok='Ġdisqualification'
  #05 len= 17 id=25734 tok='Ġextraterrestrial'
  #06 len= 17 id=15227 tok='Ġresponsibilities'
  #07 len= 17 id=23429 tok='Ġunconstitutional'
  #08 len= 16 id=25484 tok='ĠCharacteristics'
  #09 len= 16 id=23399 tok='ĠGloucestershire'
  #10 len= 16 id=11190 tok='ĠRepresentatives'
  #11 len= 16 id=19476 tok='Ġaccomplishments'
  #12 len= 16 id= 7258 tok='Ġcharacteristics'
  #13 len= 16 id=23276 tok='Ġcinematographer'
  #14 len= 16 id=24037 tok='Ġclassifications'
  #15 len= 16 id=26563 tok='Ġdissatisfaction'
  #16 len= 16 id=21151 tok='Ġelectromagnetic'
  #17 len= 16 id=22722 tok='Ġexperimentation'
  #18 len= 16 id=13036 tok='Ġinstrumentation'
  #19 len= 16 id=17370 tok='Ġintensification'
  #20 len= 16 id=12616 tok='Ġinternationally'

[load] loading tokenizers ...
[load] done in 0.23s  len_rust_active=True


[done]
lines        = 1165029
chars        = 534799177
len_tokens   = 101171898  tpc=0.189177  chars/token=5.286
bpe_tokens   = 113327064  tpc=0.211906  chars/token=4.719
elapsed      = 90.2s
