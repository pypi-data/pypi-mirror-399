[Length-MAX] vocab_size=32000
[Length-MAX] longest_tokens(top 20):
  #01 len= 47 id= 3488 tok='InternationalĠUnionĠforĠCo...
  #02 len= 46 id= 2105 tok='CourseĠonĠtheĠRiverĠThames...
  #03 len= 46 id= 7651 tok='airedĠonĠtheĠFoxĠnetworkĠi...
  #04 len= 46 id=11511 tok='crewsĠfromĠtheĠUniversitie...
  #05 len= 45 id=21393 tok='ratingĠoutĠofĠ100ĠtoĠrevie...
  #06 len= 43 id=24750 tok='transitionedĠintoĠanĠextra...
  #07 len= 42 id= 1258 tok='AustralianĠRecordingĠIndus...
  #08 len= 42 id= 7909 tok='amongĠadultsĠbetweenĠtheĠa...
  #09 len= 42 id=26145 tok='whoĠworkĠonĠcasesĠlinkedĠt...
  #10 len= 41 id=10221 tok='byĠtheĠRecordingĠIndustryĠ...
  #11 len= 41 id=10473 tok='centersĠonĠFBIĠspecialĠage...
  #12 len= 41 id=25954 tok='wentĠintoĠtheĠraceĠasĠreig...
  #13 len= 40 id= 2122 tok='CreditsĠadaptedĠfromĠtheĠl...
  #14 len= 40 id=13491 tok='etterĠUpdatedĠUnofficialĠS...
  #15 len= 38 id=  333 tok='1930ĠrenumberingĠofĠstateĠ...
  #16 len= 38 id=13332 tok='erĠofĠtheĠOrderĠofĠtheĠBri...
  #17 len= 37 id= 4658 tok='NationalĠRegisterĠofĠHisto...
  #18 len= 37 id= 7517 tok='affirĠ–ĠSimpsonĠhurricaneĠ...
  #19 len= 36 id= 3659 tok="IĠCanĠ'tĠBelieveĠItĠ'sĠaĠB...
  #20 len= 35 id= 3559 tok='InĠitsĠoriginalĠAmericanĠb...

[BPE] vocab_size=32000
[BPE] longest_tokens(top 20):
  #01 len= 17 id=22946 tok='ĠIntercontinental'
  #02 len= 17 id=19840 tok='Ġautobiographical'
  #03 len= 17 id=21024 tok='Ġcharacterization'
  #04 len= 17 id=28098 tok='Ġdisqualification'
  #05 len= 17 id=25734 tok='Ġextraterrestrial'
  #06 len= 17 id=15227 tok='Ġresponsibilities'
  #07 len= 17 id=23429 tok='Ġunconstitutional'
  #08 len= 16 id=25484 tok='ĠCharacteristics'
  #09 len= 16 id=23399 tok='ĠGloucestershire'
  #10 len= 16 id=11190 tok='ĠRepresentatives'
  #11 len= 16 id=19476 tok='Ġaccomplishments'
  #12 len= 16 id= 7258 tok='Ġcharacteristics'
  #13 len= 16 id=23276 tok='Ġcinematographer'
  #14 len= 16 id=24037 tok='Ġclassifications'
  #15 len= 16 id=26563 tok='Ġdissatisfaction'
  #16 len= 16 id=21151 tok='Ġelectromagnetic'
  #17 len= 16 id=22722 tok='Ġexperimentation'
  #18 len= 16 id=13036 tok='Ġinstrumentation'
  #19 len= 16 id=17370 tok='Ġintensification'
  #20 len= 16 id=12616 tok='Ġinternationally'

[load] loading tokenizers ...
[load] done in 0.22s  len_rust_active=True


[done]
lines        = 1165029
chars        = 534799177
len_tokens   = 100310413  tpc=0.187567  chars/token=5.331
bpe_tokens   = 113327064  tpc=0.211906  chars/token=4.719
elapsed      = 75.7s
